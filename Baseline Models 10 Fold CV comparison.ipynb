{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc84e7b3-8515-40e5-9f99-371b90f16911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading DREAMER dataset from: C:\\Users\\akash\\Downloads\\DREAMER.mat\n",
      "ðŸ‘¤ Subject 01: 18 trials\n",
      "ðŸ‘¤ Subject 02: 18 trials\n",
      "ðŸ‘¤ Subject 03: 18 trials\n",
      "ðŸ‘¤ Subject 04: 18 trials\n",
      "ðŸ‘¤ Subject 05: 18 trials\n",
      "ðŸ‘¤ Subject 06: 18 trials\n",
      "ðŸ‘¤ Subject 07: 18 trials\n",
      "ðŸ‘¤ Subject 08: 18 trials\n",
      "ðŸ‘¤ Subject 09: 18 trials\n",
      "ðŸ‘¤ Subject 10: 18 trials\n",
      "ðŸ‘¤ Subject 11: 18 trials\n",
      "ðŸ‘¤ Subject 12: 18 trials\n",
      "ðŸ‘¤ Subject 13: 18 trials\n",
      "ðŸ‘¤ Subject 14: 18 trials\n",
      "ðŸ‘¤ Subject 15: 18 trials\n",
      "ðŸ‘¤ Subject 16: 18 trials\n",
      "ðŸ‘¤ Subject 17: 18 trials\n",
      "ðŸ‘¤ Subject 18: 18 trials\n",
      "ðŸ‘¤ Subject 19: 18 trials\n",
      "ðŸ‘¤ Subject 20: 18 trials\n",
      "ðŸ‘¤ Subject 21: 18 trials\n",
      "ðŸ‘¤ Subject 22: 18 trials\n",
      "ðŸ‘¤ Subject 23: 18 trials\n",
      "\n",
      "âœ… Built spectrogram dataset: (414, 14, 36, 32)\n",
      "   â€¢ Valence labels: (414,)\n",
      "   â€¢ Arousal labels: (414,)\n",
      "   â€¢ Subjects: 23\n",
      "\n",
      "âœ… Dataset summary:\n",
      "EEG spectrograms: (414, 14, 36, 32)\n",
      "Valence labels: [4. 3. 5. 4. 4.]\n",
      "Arousal labels: [3. 3. 4. 3. 4.]\n",
      "Unique subjects: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.signal import stft, resample\n",
    "\n",
    "# Default parameters\n",
    "FS = 128            # Sampling frequency (Hz)\n",
    "NPERSEG = 256       # STFT window length\n",
    "NOVERLAP = 128      # Overlap between segments\n",
    "FREQ_MAX = 45       # Upper frequency limit (Hz)\n",
    "TIME_BINS = 32      # Time bins for spectrogram resizing\n",
    "FREQ_BINS = 36      # Frequency bins for spectrogram resizing\n",
    "\n",
    "# Path to DREAMER .mat file (update this)\n",
    "DREAMER_PATH = r\"C:\\Users\\akash\\Downloads\\DREAMER.mat\"  # <-- change this to your actual .mat path\n",
    "\n",
    "def load_dreamer_and_build_spectrograms(mat_path, fs=FS, nperseg=NPERSEG, noverlap=NOVERLAP,\n",
    "                                        freq_max=FREQ_MAX, time_bins=TIME_BINS, freq_bins=FREQ_BINS):\n",
    "    \"\"\"Universal DREAMER loader â€” works for MATLAB structs or dicts\"\"\"\n",
    "    print(f\"ðŸ“‚ Loading DREAMER dataset from: {mat_path}\")\n",
    "    mat = sio.loadmat(mat_path, squeeze_me=True, struct_as_record=False)\n",
    "\n",
    "    # Handle struct or dict\n",
    "    dreamer = mat.get('DREAMER', None)\n",
    "    if dreamer is None:\n",
    "        raise ValueError(\"âŒ 'DREAMER' key not found in .mat file!\")\n",
    "\n",
    "    if hasattr(dreamer, 'Data'):\n",
    "        data = dreamer.Data\n",
    "    elif isinstance(dreamer, dict) and 'Data' in dreamer:\n",
    "        data = dreamer['Data']\n",
    "    else:\n",
    "        raise ValueError(\"âŒ Unknown DREAMER structure type\")\n",
    "\n",
    "    channels = ['AF3','F7','F3','FC5','T7','P7','O1','O2','P8','T8','FC6','F4','F8','AF4']\n",
    "    X_list, y_val, y_aro, meta = [], [], [], []\n",
    "\n",
    "    # Helper for flattening score arrays\n",
    "    def flatten_scores(s):\n",
    "        if isinstance(s, (list, tuple)):\n",
    "            return np.array([float(np.ravel(x)[0]) for x in s], dtype=float)\n",
    "        elif isinstance(s, np.ndarray) and s.dtype == 'O':  # object (MATLAB cell)\n",
    "            return np.array([float(np.ravel(x)[0]) for x in s], dtype=float)\n",
    "        else:\n",
    "            return np.ravel(s).astype(float)\n",
    "\n",
    "    # Loop subjects\n",
    "    for subj_idx in range(len(data)):\n",
    "        subj = data[subj_idx]\n",
    "\n",
    "        eeg = subj.EEG if hasattr(subj, 'EEG') else subj['EEG']\n",
    "        baseline = eeg.baseline if hasattr(eeg, 'baseline') else eeg['baseline']\n",
    "        stimuli = eeg.stimuli if hasattr(eeg, 'stimuli') else eeg['stimuli']\n",
    "\n",
    "        sv = subj.ScoreValence if hasattr(subj, 'ScoreValence') else subj['ScoreValence']\n",
    "        sa = subj.ScoreArousal if hasattr(subj, 'ScoreArousal') else subj['ScoreArousal']\n",
    "\n",
    "        scores_val = flatten_scores(sv)\n",
    "        scores_aro = flatten_scores(sa)\n",
    "\n",
    "        n_trials = min(18, len(scores_val), len(scores_aro))\n",
    "        print(f\"ðŸ‘¤ Subject {subj_idx+1:02d}: {n_trials} trials\")\n",
    "\n",
    "        # Build spectrograms per trial\n",
    "        for t in range(n_trials):\n",
    "            base = np.array(baseline[t])\n",
    "            stim = np.array(stimuli[t])\n",
    "            if base.ndim < 2 or stim.ndim < 2:\n",
    "                continue\n",
    "\n",
    "            ch_imgs = []\n",
    "            for ch in range(stim.shape[1]):\n",
    "                sig_b, sig_s = base[:, ch], stim[:, ch]\n",
    "\n",
    "                # Compute STFT for baseline and stimulus\n",
    "                f_b, t_b, Zb = stft(sig_b, fs=fs, nperseg=nperseg, noverlap=noverlap, boundary=None)\n",
    "                f_s, t_s, Zs = stft(sig_s, fs=fs, nperseg=nperseg, noverlap=noverlap, boundary=None)\n",
    "                Pb, Ps = np.abs(Zb)**2, np.abs(Zs)**2\n",
    "\n",
    "                freq_mask = (f_s <= freq_max)\n",
    "                Ps, Pb = Ps[freq_mask, :], Pb[freq_mask, :]\n",
    "\n",
    "                # âœ… Fix shape mismatch in time dimension\n",
    "                min_T = min(Ps.shape[1], Pb.shape[1])\n",
    "                if Ps.shape[1] != min_T:\n",
    "                    Ps = Ps[:, :min_T]\n",
    "                if Pb.shape[1] != min_T:\n",
    "                    Pb = Pb[:, :min_T]\n",
    "\n",
    "                # Compute log ratio spectrogram\n",
    "                ratio = np.log((Ps + 1e-12) / (Pb + 1e-12))\n",
    "\n",
    "                # Resize to fixed grid\n",
    "                if ratio.shape[1] != time_bins:\n",
    "                    ratio = resample(ratio, time_bins, axis=1)\n",
    "                if ratio.shape[0] != freq_bins:\n",
    "                    ratio = resample(ratio, freq_bins, axis=0)\n",
    "                ch_imgs.append(ratio)\n",
    "\n",
    "            img = np.stack(ch_imgs, axis=0).astype(np.float32)\n",
    "            X_list.append(img)\n",
    "            y_val.append(scores_val[t])\n",
    "            y_aro.append(scores_aro[t])\n",
    "            meta.append((subj_idx, t))\n",
    "\n",
    "    # Convert to arrays\n",
    "    X = np.array(X_list)\n",
    "    y_val = np.array(y_val).astype(float)\n",
    "    y_aro = np.array(y_aro).astype(float)\n",
    "    groups = np.array([m[0] for m in meta], dtype=int)\n",
    "\n",
    "    print(f\"\\nâœ… Built spectrogram dataset: {X.shape}\")\n",
    "    print(f\"   â€¢ Valence labels: {y_val.shape}\")\n",
    "    print(f\"   â€¢ Arousal labels: {y_aro.shape}\")\n",
    "    print(f\"   â€¢ Subjects: {len(np.unique(groups))}\\n\")\n",
    "    return X, y_val, y_aro, groups, channels\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Run this cell to test loading\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    X, y_val, y_aro, groups, channels = load_dreamer_and_build_spectrograms(\n",
    "        DREAMER_PATH, fs=FS, nperseg=NPERSEG, noverlap=NOVERLAP,\n",
    "        freq_max=FREQ_MAX, time_bins=TIME_BINS, freq_bins=FREQ_BINS\n",
    "    )\n",
    "\n",
    "    print(\"âœ… Dataset summary:\")\n",
    "    print(f\"EEG spectrograms: {X.shape}\")\n",
    "    print(f\"Valence labels: {y_val[:5]}\")\n",
    "    print(f\"Arousal labels: {y_aro[:5]}\")\n",
    "    print(f\"Unique subjects: {np.unique(groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c86899-4345-4ecb-b0ea-c3c9030a182e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "âœ… Spectrogram dataset: (414, 14, 36, 32)\n",
      "\n",
      "########## Valence: 10-fold STRATIFIED CV ##########\n",
      "Valence global median (for stratification) = 3.0000\n",
      "Class counts: [161 253]\n",
      "\n",
      "----- Valence Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 1) | epochs=70\n",
      "[Valence (Fold 1)] Ep 01/70 | Loss=0.5395 | Val Acc=64.29% | F1=0.737 | AUC=0.589\n",
      "[Valence (Fold 1)] Ep 02/70 | Loss=0.5321 | Val Acc=61.90% | F1=0.733 | AUC=0.673\n",
      "[Valence (Fold 1)] Ep 03/70 | Loss=0.5175 | Val Acc=64.29% | F1=0.754 | AUC=0.647\n",
      "[Valence (Fold 1)] Ep 04/70 | Loss=0.4794 | Val Acc=57.14% | F1=0.591 | AUC=0.623\n",
      "[Valence (Fold 1)] Ep 05/70 | Loss=0.4239 | Val Acc=57.14% | F1=0.667 | AUC=0.582\n",
      "[Valence (Fold 1)] Ep 06/70 | Loss=0.3600 | Val Acc=64.29% | F1=0.746 | AUC=0.630\n",
      "[Valence (Fold 1)] Ep 07/70 | Loss=0.2609 | Val Acc=57.14% | F1=0.654 | AUC=0.572\n",
      "[Valence (Fold 1)] Ep 08/70 | Loss=0.1634 | Val Acc=61.90% | F1=0.692 | AUC=0.599\n",
      "[Valence (Fold 1)] Ep 09/70 | Loss=0.0903 | Val Acc=59.52% | F1=0.667 | AUC=0.613\n",
      "[Valence (Fold 1)] Ep 10/70 | Loss=0.0661 | Val Acc=54.76% | F1=0.612 | AUC=0.591\n",
      "[Valence (Fold 1)] Ep 11/70 | Loss=0.0400 | Val Acc=50.00% | F1=0.553 | AUC=0.550\n",
      "[Valence (Fold 1)] Ep 12/70 | Loss=0.0241 | Val Acc=54.76% | F1=0.612 | AUC=0.546\n",
      "[Valence (Fold 1)] Ep 13/70 | Loss=0.0152 | Val Acc=54.76% | F1=0.612 | AUC=0.531\n",
      "[Valence (Fold 1)] Ep 14/70 | Loss=0.0183 | Val Acc=57.14% | F1=0.640 | AUC=0.553\n",
      "[Valence (Fold 1)] Ep 15/70 | Loss=0.0143 | Val Acc=57.14% | F1=0.625 | AUC=0.534\n",
      "[Valence (Fold 1)] Ep 16/70 | Loss=0.0103 | Val Acc=54.76% | F1=0.612 | AUC=0.531\n",
      "[Valence (Fold 1)] Ep 17/70 | Loss=0.0079 | Val Acc=54.76% | F1=0.612 | AUC=0.546\n",
      "[Valence (Fold 1)] Ep 18/70 | Loss=0.0075 | Val Acc=54.76% | F1=0.612 | AUC=0.550\n",
      "[Valence (Fold 1)] Ep 19/70 | Loss=0.0070 | Val Acc=54.76% | F1=0.612 | AUC=0.555\n",
      "[Valence (Fold 1)] Ep 20/70 | Loss=0.0074 | Val Acc=52.38% | F1=0.600 | AUC=0.560\n",
      "[Valence (Fold 1)] Ep 21/70 | Loss=0.0323 | Val Acc=59.52% | F1=0.679 | AUC=0.579\n",
      "[Valence (Fold 1)] Ep 22/70 | Loss=0.0443 | Val Acc=59.52% | F1=0.653 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 23/70 | Loss=0.0443 | Val Acc=54.76% | F1=0.655 | AUC=0.579\n",
      "[Valence (Fold 1)] Ep 24/70 | Loss=0.0242 | Val Acc=61.90% | F1=0.714 | AUC=0.531\n",
      "[Valence (Fold 1)] Ep 25/70 | Loss=0.0260 | Val Acc=57.14% | F1=0.667 | AUC=0.630\n",
      "[Valence (Fold 1)] Ep 26/70 | Loss=0.0138 | Val Acc=59.52% | F1=0.691 | AUC=0.589\n",
      "[Valence (Fold 1)] Ep 27/70 | Loss=0.0144 | Val Acc=57.14% | F1=0.654 | AUC=0.579\n",
      "[Valence (Fold 1)] Ep 28/70 | Loss=0.0099 | Val Acc=57.14% | F1=0.654 | AUC=0.543\n",
      "[Valence (Fold 1)] Ep 29/70 | Loss=0.0108 | Val Acc=61.90% | F1=0.714 | AUC=0.536\n",
      "[Valence (Fold 1)] Ep 30/70 | Loss=0.0013 | Val Acc=50.00% | F1=0.571 | AUC=0.502\n",
      "[Valence (Fold 1)] Ep 31/70 | Loss=0.0027 | Val Acc=59.52% | F1=0.638 | AUC=0.572\n",
      "â¹ [Valence (Fold 1)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 1)] TEST (best thr=0.450) | Acc=54.76% | F1=0.642 | AUC=0.536\n",
      "\n",
      "Fold 1 Results: Acc=0.5476, F1=0.6415, AUC=0.5361\n",
      "\n",
      "----- Valence Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 2) | epochs=70\n",
      "[Valence (Fold 2)] Ep 01/70 | Loss=0.5351 | Val Acc=57.14% | F1=0.679 | AUC=0.529\n",
      "[Valence (Fold 2)] Ep 02/70 | Loss=0.5151 | Val Acc=50.00% | F1=0.512 | AUC=0.567\n",
      "[Valence (Fold 2)] Ep 03/70 | Loss=0.4781 | Val Acc=50.00% | F1=0.400 | AUC=0.536\n",
      "[Valence (Fold 2)] Ep 04/70 | Loss=0.4174 | Val Acc=38.10% | F1=0.316 | AUC=0.445\n",
      "[Valence (Fold 2)] Ep 05/70 | Loss=0.3709 | Val Acc=45.24% | F1=0.410 | AUC=0.519\n",
      "[Valence (Fold 2)] Ep 06/70 | Loss=0.2826 | Val Acc=52.38% | F1=0.500 | AUC=0.512\n",
      "[Valence (Fold 2)] Ep 07/70 | Loss=0.1928 | Val Acc=54.76% | F1=0.513 | AUC=0.474\n",
      "[Valence (Fold 2)] Ep 08/70 | Loss=0.1289 | Val Acc=52.38% | F1=0.655 | AUC=0.401\n",
      "[Valence (Fold 2)] Ep 09/70 | Loss=0.0989 | Val Acc=45.24% | F1=0.582 | AUC=0.435\n",
      "[Valence (Fold 2)] Ep 10/70 | Loss=0.0611 | Val Acc=54.76% | F1=0.537 | AUC=0.450\n",
      "[Valence (Fold 2)] Ep 11/70 | Loss=0.0375 | Val Acc=52.38% | F1=0.524 | AUC=0.498\n",
      "[Valence (Fold 2)] Ep 12/70 | Loss=0.0234 | Val Acc=42.86% | F1=0.500 | AUC=0.466\n",
      "[Valence (Fold 2)] Ep 13/70 | Loss=0.0196 | Val Acc=42.86% | F1=0.556 | AUC=0.445\n",
      "[Valence (Fold 2)] Ep 14/70 | Loss=0.0131 | Val Acc=45.24% | F1=0.596 | AUC=0.476\n",
      "[Valence (Fold 2)] Ep 15/70 | Loss=0.0116 | Val Acc=47.62% | F1=0.560 | AUC=0.493\n",
      "[Valence (Fold 2)] Ep 16/70 | Loss=0.0093 | Val Acc=47.62% | F1=0.560 | AUC=0.471\n",
      "[Valence (Fold 2)] Ep 17/70 | Loss=0.0132 | Val Acc=47.62% | F1=0.577 | AUC=0.445\n",
      "[Valence (Fold 2)] Ep 18/70 | Loss=0.0083 | Val Acc=47.62% | F1=0.577 | AUC=0.462\n",
      "[Valence (Fold 2)] Ep 19/70 | Loss=0.0063 | Val Acc=45.24% | F1=0.549 | AUC=0.457\n",
      "[Valence (Fold 2)] Ep 20/70 | Loss=0.0083 | Val Acc=45.24% | F1=0.549 | AUC=0.452\n",
      "[Valence (Fold 2)] Ep 21/70 | Loss=0.0350 | Val Acc=42.86% | F1=0.368 | AUC=0.445\n",
      "[Valence (Fold 2)] Ep 22/70 | Loss=0.0573 | Val Acc=47.62% | F1=0.593 | AUC=0.437\n",
      "[Valence (Fold 2)] Ep 23/70 | Loss=0.0271 | Val Acc=42.86% | F1=0.478 | AUC=0.450\n",
      "[Valence (Fold 2)] Ep 24/70 | Loss=0.0489 | Val Acc=57.14% | F1=0.700 | AUC=0.416\n",
      "[Valence (Fold 2)] Ep 25/70 | Loss=0.0450 | Val Acc=59.52% | F1=0.667 | AUC=0.505\n",
      "[Valence (Fold 2)] Ep 26/70 | Loss=0.0199 | Val Acc=57.14% | F1=0.571 | AUC=0.498\n",
      "[Valence (Fold 2)] Ep 27/70 | Loss=0.0072 | Val Acc=47.62% | F1=0.560 | AUC=0.466\n",
      "[Valence (Fold 2)] Ep 28/70 | Loss=0.0081 | Val Acc=40.48% | F1=0.510 | AUC=0.454\n",
      "[Valence (Fold 2)] Ep 29/70 | Loss=0.0137 | Val Acc=50.00% | F1=0.632 | AUC=0.450\n",
      "[Valence (Fold 2)] Ep 30/70 | Loss=0.0062 | Val Acc=50.00% | F1=0.588 | AUC=0.476\n",
      "[Valence (Fold 2)] Ep 31/70 | Loss=0.0045 | Val Acc=52.38% | F1=0.524 | AUC=0.418\n",
      "[Valence (Fold 2)] Ep 32/70 | Loss=0.0037 | Val Acc=45.24% | F1=0.511 | AUC=0.476\n",
      "[Valence (Fold 2)] Ep 33/70 | Loss=0.0010 | Val Acc=47.62% | F1=0.542 | AUC=0.445\n",
      "[Valence (Fold 2)] Ep 34/70 | Loss=0.0020 | Val Acc=47.62% | F1=0.560 | AUC=0.466\n",
      "[Valence (Fold 2)] Ep 35/70 | Loss=0.0006 | Val Acc=50.00% | F1=0.604 | AUC=0.471\n",
      "[Valence (Fold 2)] Ep 36/70 | Loss=0.0006 | Val Acc=50.00% | F1=0.571 | AUC=0.454\n",
      "[Valence (Fold 2)] Ep 37/70 | Loss=0.0006 | Val Acc=50.00% | F1=0.571 | AUC=0.452\n",
      "[Valence (Fold 2)] Ep 38/70 | Loss=0.0008 | Val Acc=50.00% | F1=0.571 | AUC=0.447\n",
      "[Valence (Fold 2)] Ep 39/70 | Loss=0.0005 | Val Acc=47.62% | F1=0.542 | AUC=0.450\n",
      "[Valence (Fold 2)] Ep 40/70 | Loss=0.0006 | Val Acc=50.00% | F1=0.571 | AUC=0.450\n",
      "[Valence (Fold 2)] Ep 41/70 | Loss=0.0008 | Val Acc=45.24% | F1=0.549 | AUC=0.464\n",
      "[Valence (Fold 2)] Ep 42/70 | Loss=0.0130 | Val Acc=47.62% | F1=0.421 | AUC=0.502\n",
      "[Valence (Fold 2)] Ep 43/70 | Loss=0.1021 | Val Acc=52.38% | F1=0.677 | AUC=0.508\n",
      "â¹ [Valence (Fold 2)] Early stopping at epoch 43\n",
      "\n",
      "ðŸ”š [Valence (Fold 2)] TEST (best thr=0.150) | Acc=64.29% | F1=0.769 | AUC=0.601\n",
      "\n",
      "Fold 2 Results: Acc=0.6429, F1=0.7692, AUC=0.6010\n",
      "\n",
      "----- Valence Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 3) | epochs=70\n",
      "[Valence (Fold 3)] Ep 01/70 | Loss=0.5362 | Val Acc=54.76% | F1=0.678 | AUC=0.430\n",
      "[Valence (Fold 3)] Ep 02/70 | Loss=0.5092 | Val Acc=54.76% | F1=0.642 | AUC=0.505\n",
      "[Valence (Fold 3)] Ep 03/70 | Loss=0.4662 | Val Acc=59.52% | F1=0.653 | AUC=0.531\n",
      "[Valence (Fold 3)] Ep 04/70 | Loss=0.3962 | Val Acc=64.29% | F1=0.754 | AUC=0.471\n",
      "[Valence (Fold 3)] Ep 05/70 | Loss=0.3858 | Val Acc=50.00% | F1=0.604 | AUC=0.459\n",
      "[Valence (Fold 3)] Ep 06/70 | Loss=0.2940 | Val Acc=61.90% | F1=0.742 | AUC=0.442\n",
      "[Valence (Fold 3)] Ep 07/70 | Loss=0.2180 | Val Acc=47.62% | F1=0.421 | AUC=0.490\n",
      "[Valence (Fold 3)] Ep 08/70 | Loss=0.1442 | Val Acc=54.76% | F1=0.655 | AUC=0.462\n",
      "[Valence (Fold 3)] Ep 09/70 | Loss=0.0794 | Val Acc=38.10% | F1=0.409 | AUC=0.428\n",
      "[Valence (Fold 3)] Ep 10/70 | Loss=0.0700 | Val Acc=42.86% | F1=0.520 | AUC=0.440\n",
      "[Valence (Fold 3)] Ep 11/70 | Loss=0.0382 | Val Acc=45.24% | F1=0.410 | AUC=0.474\n",
      "[Valence (Fold 3)] Ep 12/70 | Loss=0.0288 | Val Acc=45.24% | F1=0.531 | AUC=0.462\n",
      "[Valence (Fold 3)] Ep 13/70 | Loss=0.0300 | Val Acc=54.76% | F1=0.655 | AUC=0.442\n",
      "[Valence (Fold 3)] Ep 14/70 | Loss=0.0173 | Val Acc=54.76% | F1=0.667 | AUC=0.462\n",
      "[Valence (Fold 3)] Ep 15/70 | Loss=0.0101 | Val Acc=52.38% | F1=0.643 | AUC=0.452\n",
      "[Valence (Fold 3)] Ep 16/70 | Loss=0.0102 | Val Acc=54.76% | F1=0.627 | AUC=0.454\n",
      "[Valence (Fold 3)] Ep 17/70 | Loss=0.0139 | Val Acc=52.38% | F1=0.643 | AUC=0.440\n",
      "[Valence (Fold 3)] Ep 18/70 | Loss=0.0094 | Val Acc=54.76% | F1=0.667 | AUC=0.438\n",
      "[Valence (Fold 3)] Ep 19/70 | Loss=0.0113 | Val Acc=52.38% | F1=0.643 | AUC=0.442\n",
      "[Valence (Fold 3)] Ep 20/70 | Loss=0.0078 | Val Acc=52.38% | F1=0.643 | AUC=0.445\n",
      "[Valence (Fold 3)] Ep 21/70 | Loss=0.0517 | Val Acc=57.14% | F1=0.700 | AUC=0.454\n",
      "[Valence (Fold 3)] Ep 22/70 | Loss=0.0678 | Val Acc=61.90% | F1=0.733 | AUC=0.454\n",
      "[Valence (Fold 3)] Ep 23/70 | Loss=0.0998 | Val Acc=45.24% | F1=0.549 | AUC=0.474\n",
      "[Valence (Fold 3)] Ep 24/70 | Loss=0.0388 | Val Acc=50.00% | F1=0.644 | AUC=0.399\n",
      "[Valence (Fold 3)] Ep 25/70 | Loss=0.0266 | Val Acc=40.48% | F1=0.324 | AUC=0.462\n",
      "[Valence (Fold 3)] Ep 26/70 | Loss=0.0194 | Val Acc=47.62% | F1=0.560 | AUC=0.464\n",
      "[Valence (Fold 3)] Ep 27/70 | Loss=0.0064 | Val Acc=54.76% | F1=0.689 | AUC=0.401\n",
      "[Valence (Fold 3)] Ep 28/70 | Loss=0.0031 | Val Acc=45.24% | F1=0.549 | AUC=0.394\n",
      "[Valence (Fold 3)] Ep 29/70 | Loss=0.0046 | Val Acc=47.62% | F1=0.577 | AUC=0.416\n",
      "[Valence (Fold 3)] Ep 30/70 | Loss=0.0014 | Val Acc=40.48% | F1=0.468 | AUC=0.389\n",
      "[Valence (Fold 3)] Ep 31/70 | Loss=0.0110 | Val Acc=52.38% | F1=0.643 | AUC=0.404\n",
      "â¹ [Valence (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 3)] TEST (best thr=0.100) | Acc=61.90% | F1=0.714 | AUC=0.553\n",
      "\n",
      "Fold 3 Results: Acc=0.6190, F1=0.7143, AUC=0.5529\n",
      "\n",
      "----- Valence Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [128 202]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [17 25]\n",
      "[Valence (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.634\n",
      "\n",
      "ðŸš€ Training Valence (Fold 4) | epochs=70\n",
      "[Valence (Fold 4)] Ep 01/70 | Loss=0.5318 | Val Acc=57.14% | F1=0.609 | AUC=0.647\n",
      "[Valence (Fold 4)] Ep 02/70 | Loss=0.5128 | Val Acc=38.10% | F1=0.133 | AUC=0.623\n",
      "[Valence (Fold 4)] Ep 03/70 | Loss=0.4753 | Val Acc=40.48% | F1=0.194 | AUC=0.541\n",
      "[Valence (Fold 4)] Ep 04/70 | Loss=0.4554 | Val Acc=57.14% | F1=0.609 | AUC=0.587\n",
      "[Valence (Fold 4)] Ep 05/70 | Loss=0.3770 | Val Acc=42.86% | F1=0.200 | AUC=0.577\n",
      "[Valence (Fold 4)] Ep 06/70 | Loss=0.3153 | Val Acc=59.52% | F1=0.721 | AUC=0.546\n",
      "[Valence (Fold 4)] Ep 07/70 | Loss=0.2541 | Val Acc=64.29% | F1=0.727 | AUC=0.615\n",
      "[Valence (Fold 4)] Ep 08/70 | Loss=0.1760 | Val Acc=50.00% | F1=0.462 | AUC=0.589\n",
      "[Valence (Fold 4)] Ep 09/70 | Loss=0.1208 | Val Acc=66.67% | F1=0.759 | AUC=0.627\n",
      "[Valence (Fold 4)] Ep 10/70 | Loss=0.1051 | Val Acc=59.52% | F1=0.702 | AUC=0.550\n",
      "[Valence (Fold 4)] Ep 11/70 | Loss=0.0709 | Val Acc=59.52% | F1=0.702 | AUC=0.538\n",
      "[Valence (Fold 4)] Ep 12/70 | Loss=0.0480 | Val Acc=52.38% | F1=0.677 | AUC=0.613\n",
      "[Valence (Fold 4)] Ep 13/70 | Loss=0.0455 | Val Acc=57.14% | F1=0.679 | AUC=0.582\n",
      "[Valence (Fold 4)] Ep 14/70 | Loss=0.0356 | Val Acc=59.52% | F1=0.702 | AUC=0.587\n",
      "[Valence (Fold 4)] Ep 15/70 | Loss=0.0281 | Val Acc=61.90% | F1=0.680 | AUC=0.632\n",
      "[Valence (Fold 4)] Ep 16/70 | Loss=0.0152 | Val Acc=66.67% | F1=0.750 | AUC=0.572\n",
      "[Valence (Fold 4)] Ep 17/70 | Loss=0.0121 | Val Acc=57.14% | F1=0.654 | AUC=0.587\n",
      "[Valence (Fold 4)] Ep 18/70 | Loss=0.0105 | Val Acc=61.90% | F1=0.704 | AUC=0.603\n",
      "[Valence (Fold 4)] Ep 19/70 | Loss=0.0117 | Val Acc=69.05% | F1=0.772 | AUC=0.594\n",
      "[Valence (Fold 4)] Ep 20/70 | Loss=0.0117 | Val Acc=64.29% | F1=0.746 | AUC=0.594\n",
      "[Valence (Fold 4)] Ep 21/70 | Loss=0.0494 | Val Acc=59.52% | F1=0.622 | AUC=0.651\n",
      "[Valence (Fold 4)] Ep 22/70 | Loss=0.0596 | Val Acc=45.24% | F1=0.258 | AUC=0.575\n",
      "[Valence (Fold 4)] Ep 23/70 | Loss=0.0489 | Val Acc=52.38% | F1=0.545 | AUC=0.608\n",
      "[Valence (Fold 4)] Ep 24/70 | Loss=0.0980 | Val Acc=42.86% | F1=0.429 | AUC=0.538\n",
      "[Valence (Fold 4)] Ep 25/70 | Loss=0.0566 | Val Acc=50.00% | F1=0.364 | AUC=0.603\n",
      "[Valence (Fold 4)] Ep 26/70 | Loss=0.0258 | Val Acc=50.00% | F1=0.400 | AUC=0.659\n",
      "[Valence (Fold 4)] Ep 27/70 | Loss=0.0481 | Val Acc=64.29% | F1=0.776 | AUC=0.538\n",
      "[Valence (Fold 4)] Ep 28/70 | Loss=0.0158 | Val Acc=59.52% | F1=0.691 | AUC=0.651\n",
      "[Valence (Fold 4)] Ep 29/70 | Loss=0.0096 | Val Acc=66.67% | F1=0.750 | AUC=0.663\n",
      "[Valence (Fold 4)] Ep 30/70 | Loss=0.0130 | Val Acc=61.90% | F1=0.742 | AUC=0.649\n",
      "[Valence (Fold 4)] Ep 31/70 | Loss=0.0148 | Val Acc=66.67% | F1=0.759 | AUC=0.589\n",
      "[Valence (Fold 4)] Ep 32/70 | Loss=0.0021 | Val Acc=64.29% | F1=0.754 | AUC=0.625\n",
      "[Valence (Fold 4)] Ep 33/70 | Loss=0.0008 | Val Acc=66.67% | F1=0.750 | AUC=0.625\n",
      "[Valence (Fold 4)] Ep 34/70 | Loss=0.0025 | Val Acc=71.43% | F1=0.793 | AUC=0.623\n",
      "[Valence (Fold 4)] Ep 35/70 | Loss=0.0008 | Val Acc=69.05% | F1=0.772 | AUC=0.618\n",
      "[Valence (Fold 4)] Ep 36/70 | Loss=0.0018 | Val Acc=61.90% | F1=0.714 | AUC=0.637\n",
      "[Valence (Fold 4)] Ep 37/70 | Loss=0.0007 | Val Acc=59.52% | F1=0.691 | AUC=0.627\n",
      "[Valence (Fold 4)] Ep 38/70 | Loss=0.0023 | Val Acc=64.29% | F1=0.737 | AUC=0.620\n",
      "[Valence (Fold 4)] Ep 39/70 | Loss=0.0008 | Val Acc=64.29% | F1=0.737 | AUC=0.623\n",
      "[Valence (Fold 4)] Ep 40/70 | Loss=0.0006 | Val Acc=64.29% | F1=0.737 | AUC=0.623\n",
      "[Valence (Fold 4)] Ep 41/70 | Loss=0.0021 | Val Acc=59.52% | F1=0.702 | AUC=0.600\n",
      "[Valence (Fold 4)] Ep 42/70 | Loss=0.0506 | Val Acc=64.29% | F1=0.776 | AUC=0.493\n",
      "[Valence (Fold 4)] Ep 43/70 | Loss=0.0694 | Val Acc=71.43% | F1=0.793 | AUC=0.680\n",
      "[Valence (Fold 4)] Ep 44/70 | Loss=0.0202 | Val Acc=69.05% | F1=0.755 | AUC=0.663\n",
      "[Valence (Fold 4)] Ep 45/70 | Loss=0.0270 | Val Acc=57.14% | F1=0.640 | AUC=0.565\n",
      "[Valence (Fold 4)] Ep 46/70 | Loss=0.0242 | Val Acc=50.00% | F1=0.462 | AUC=0.577\n",
      "[Valence (Fold 4)] Ep 47/70 | Loss=0.0152 | Val Acc=50.00% | F1=0.588 | AUC=0.558\n",
      "[Valence (Fold 4)] Ep 48/70 | Loss=0.0088 | Val Acc=61.90% | F1=0.680 | AUC=0.632\n",
      "[Valence (Fold 4)] Ep 49/70 | Loss=0.0129 | Val Acc=61.90% | F1=0.692 | AUC=0.606\n",
      "[Valence (Fold 4)] Ep 50/70 | Loss=0.0055 | Val Acc=61.90% | F1=0.724 | AUC=0.572\n",
      "[Valence (Fold 4)] Ep 51/70 | Loss=0.0017 | Val Acc=59.52% | F1=0.730 | AUC=0.526\n",
      "[Valence (Fold 4)] Ep 52/70 | Loss=0.0019 | Val Acc=54.76% | F1=0.655 | AUC=0.565\n",
      "â¹ [Valence (Fold 4)] Early stopping at epoch 52\n",
      "\n",
      "ðŸ”š [Valence (Fold 4)] TEST (best thr=0.700) | Acc=64.29% | F1=0.667 | AUC=0.713\n",
      "\n",
      "Fold 4 Results: Acc=0.6429, F1=0.6667, AUC=0.7129\n",
      "\n",
      "----- Valence Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 5) | epochs=70\n",
      "[Valence (Fold 5)] Ep 01/70 | Loss=0.5422 | Val Acc=41.46% | F1=0.368 | AUC=0.547\n",
      "[Valence (Fold 5)] Ep 02/70 | Loss=0.5299 | Val Acc=51.22% | F1=0.545 | AUC=0.583\n",
      "[Valence (Fold 5)] Ep 03/70 | Loss=0.4931 | Val Acc=51.22% | F1=0.545 | AUC=0.520\n",
      "[Valence (Fold 5)] Ep 04/70 | Loss=0.4602 | Val Acc=34.15% | F1=0.129 | AUC=0.463\n",
      "[Valence (Fold 5)] Ep 05/70 | Loss=0.4090 | Val Acc=56.10% | F1=0.500 | AUC=0.585\n",
      "[Valence (Fold 5)] Ep 06/70 | Loss=0.3272 | Val Acc=51.22% | F1=0.412 | AUC=0.502\n",
      "[Valence (Fold 5)] Ep 07/70 | Loss=0.2330 | Val Acc=43.90% | F1=0.303 | AUC=0.510\n",
      "[Valence (Fold 5)] Ep 08/70 | Loss=0.1597 | Val Acc=60.98% | F1=0.724 | AUC=0.675\n",
      "[Valence (Fold 5)] Ep 09/70 | Loss=0.1013 | Val Acc=43.90% | F1=0.378 | AUC=0.493\n",
      "[Valence (Fold 5)] Ep 10/70 | Loss=0.0650 | Val Acc=46.34% | F1=0.522 | AUC=0.542\n",
      "[Valence (Fold 5)] Ep 11/70 | Loss=0.0315 | Val Acc=65.85% | F1=0.767 | AUC=0.650\n",
      "[Valence (Fold 5)] Ep 12/70 | Loss=0.0336 | Val Acc=56.10% | F1=0.667 | AUC=0.583\n",
      "[Valence (Fold 5)] Ep 13/70 | Loss=0.0227 | Val Acc=56.10% | F1=0.654 | AUC=0.583\n",
      "[Valence (Fold 5)] Ep 14/70 | Loss=0.0142 | Val Acc=53.66% | F1=0.627 | AUC=0.573\n",
      "[Valence (Fold 5)] Ep 15/70 | Loss=0.0124 | Val Acc=53.66% | F1=0.667 | AUC=0.585\n",
      "[Valence (Fold 5)] Ep 16/70 | Loss=0.0107 | Val Acc=51.22% | F1=0.565 | AUC=0.535\n",
      "[Valence (Fold 5)] Ep 17/70 | Loss=0.0082 | Val Acc=51.22% | F1=0.583 | AUC=0.552\n",
      "[Valence (Fold 5)] Ep 18/70 | Loss=0.0111 | Val Acc=51.22% | F1=0.615 | AUC=0.578\n",
      "[Valence (Fold 5)] Ep 19/70 | Loss=0.0062 | Val Acc=53.66% | F1=0.612 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 20/70 | Loss=0.0065 | Val Acc=51.22% | F1=0.615 | AUC=0.570\n",
      "[Valence (Fold 5)] Ep 21/70 | Loss=0.0594 | Val Acc=39.02% | F1=0.286 | AUC=0.435\n",
      "[Valence (Fold 5)] Ep 22/70 | Loss=0.0275 | Val Acc=48.78% | F1=0.604 | AUC=0.568\n",
      "[Valence (Fold 5)] Ep 23/70 | Loss=0.0641 | Val Acc=41.46% | F1=0.250 | AUC=0.470\n",
      "[Valence (Fold 5)] Ep 24/70 | Loss=0.0592 | Val Acc=43.90% | F1=0.303 | AUC=0.470\n",
      "[Valence (Fold 5)] Ep 25/70 | Loss=0.0237 | Val Acc=60.98% | F1=0.692 | AUC=0.620\n",
      "[Valence (Fold 5)] Ep 26/70 | Loss=0.0197 | Val Acc=65.85% | F1=0.774 | AUC=0.667\n",
      "[Valence (Fold 5)] Ep 27/70 | Loss=0.0129 | Val Acc=56.10% | F1=0.700 | AUC=0.608\n",
      "[Valence (Fold 5)] Ep 28/70 | Loss=0.0074 | Val Acc=43.90% | F1=0.343 | AUC=0.545\n",
      "[Valence (Fold 5)] Ep 29/70 | Loss=0.0086 | Val Acc=51.22% | F1=0.565 | AUC=0.495\n",
      "[Valence (Fold 5)] Ep 30/70 | Loss=0.0030 | Val Acc=48.78% | F1=0.553 | AUC=0.503\n",
      "[Valence (Fold 5)] Ep 31/70 | Loss=0.0031 | Val Acc=53.66% | F1=0.596 | AUC=0.560\n",
      "â¹ [Valence (Fold 5)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 5)] TEST (best thr=0.100) | Acc=65.85% | F1=0.741 | AUC=0.608\n",
      "\n",
      "Fold 5 Results: Acc=0.6585, F1=0.7407, AUC=0.6075\n",
      "\n",
      "----- Valence Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 6) | epochs=70\n",
      "[Valence (Fold 6)] Ep 01/70 | Loss=0.5367 | Val Acc=53.66% | F1=0.678 | AUC=0.448\n",
      "[Valence (Fold 6)] Ep 02/70 | Loss=0.5259 | Val Acc=41.46% | F1=0.077 | AUC=0.540\n",
      "[Valence (Fold 6)] Ep 03/70 | Loss=0.5058 | Val Acc=58.54% | F1=0.738 | AUC=0.568\n",
      "[Valence (Fold 6)] Ep 04/70 | Loss=0.4790 | Val Acc=63.41% | F1=0.769 | AUC=0.635\n",
      "[Valence (Fold 6)] Ep 05/70 | Loss=0.4476 | Val Acc=70.73% | F1=0.778 | AUC=0.637\n",
      "[Valence (Fold 6)] Ep 06/70 | Loss=0.3814 | Val Acc=58.54% | F1=0.514 | AUC=0.800\n",
      "[Valence (Fold 6)] Ep 07/70 | Loss=0.2934 | Val Acc=51.22% | F1=0.500 | AUC=0.635\n",
      "[Valence (Fold 6)] Ep 08/70 | Loss=0.2000 | Val Acc=60.98% | F1=0.680 | AUC=0.680\n",
      "[Valence (Fold 6)] Ep 09/70 | Loss=0.1114 | Val Acc=56.10% | F1=0.667 | AUC=0.700\n",
      "[Valence (Fold 6)] Ep 10/70 | Loss=0.0694 | Val Acc=63.41% | F1=0.762 | AUC=0.660\n",
      "[Valence (Fold 6)] Ep 11/70 | Loss=0.0508 | Val Acc=56.10% | F1=0.526 | AUC=0.715\n",
      "[Valence (Fold 6)] Ep 12/70 | Loss=0.0340 | Val Acc=60.98% | F1=0.714 | AUC=0.695\n",
      "[Valence (Fold 6)] Ep 13/70 | Loss=0.0238 | Val Acc=58.54% | F1=0.667 | AUC=0.617\n",
      "[Valence (Fold 6)] Ep 14/70 | Loss=0.0192 | Val Acc=48.78% | F1=0.432 | AUC=0.597\n",
      "[Valence (Fold 6)] Ep 15/70 | Loss=0.0171 | Val Acc=58.54% | F1=0.691 | AUC=0.633\n",
      "[Valence (Fold 6)] Ep 16/70 | Loss=0.0106 | Val Acc=53.66% | F1=0.655 | AUC=0.578\n",
      "[Valence (Fold 6)] Ep 17/70 | Loss=0.0099 | Val Acc=63.41% | F1=0.746 | AUC=0.540\n",
      "[Valence (Fold 6)] Ep 18/70 | Loss=0.0100 | Val Acc=56.10% | F1=0.679 | AUC=0.598\n",
      "[Valence (Fold 6)] Ep 19/70 | Loss=0.0117 | Val Acc=60.98% | F1=0.714 | AUC=0.610\n",
      "[Valence (Fold 6)] Ep 20/70 | Loss=0.0069 | Val Acc=60.98% | F1=0.714 | AUC=0.605\n",
      "[Valence (Fold 6)] Ep 21/70 | Loss=0.0281 | Val Acc=58.54% | F1=0.653 | AUC=0.618\n",
      "[Valence (Fold 6)] Ep 22/70 | Loss=0.0680 | Val Acc=48.78% | F1=0.571 | AUC=0.458\n",
      "[Valence (Fold 6)] Ep 23/70 | Loss=0.0424 | Val Acc=63.41% | F1=0.769 | AUC=0.490\n",
      "[Valence (Fold 6)] Ep 24/70 | Loss=0.0349 | Val Acc=63.41% | F1=0.717 | AUC=0.628\n",
      "[Valence (Fold 6)] Ep 25/70 | Loss=0.0413 | Val Acc=60.98% | F1=0.636 | AUC=0.698\n",
      "[Valence (Fold 6)] Ep 26/70 | Loss=0.0284 | Val Acc=51.22% | F1=0.565 | AUC=0.587\n",
      "[Valence (Fold 6)] Ep 27/70 | Loss=0.0248 | Val Acc=63.41% | F1=0.706 | AUC=0.653\n",
      "[Valence (Fold 6)] Ep 28/70 | Loss=0.0216 | Val Acc=58.54% | F1=0.585 | AUC=0.728\n",
      "[Valence (Fold 6)] Ep 29/70 | Loss=0.0118 | Val Acc=58.54% | F1=0.730 | AUC=0.510\n",
      "[Valence (Fold 6)] Ep 30/70 | Loss=0.0024 | Val Acc=58.54% | F1=0.691 | AUC=0.608\n",
      "[Valence (Fold 6)] Ep 31/70 | Loss=0.0048 | Val Acc=60.98% | F1=0.724 | AUC=0.682\n",
      "â¹ [Valence (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 6)] TEST (best thr=0.850) | Acc=65.85% | F1=0.731 | AUC=0.715\n",
      "\n",
      "Fold 6 Results: Acc=0.6585, F1=0.7308, AUC=0.7150\n",
      "\n",
      "----- Valence Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 7) | epochs=70\n",
      "[Valence (Fold 7)] Ep 01/70 | Loss=0.5376 | Val Acc=68.29% | F1=0.755 | AUC=0.710\n",
      "[Valence (Fold 7)] Ep 02/70 | Loss=0.5081 | Val Acc=60.98% | F1=0.652 | AUC=0.740\n",
      "[Valence (Fold 7)] Ep 03/70 | Loss=0.4800 | Val Acc=60.98% | F1=0.652 | AUC=0.720\n",
      "[Valence (Fold 7)] Ep 04/70 | Loss=0.4306 | Val Acc=60.98% | F1=0.724 | AUC=0.670\n",
      "[Valence (Fold 7)] Ep 05/70 | Loss=0.3798 | Val Acc=63.41% | F1=0.746 | AUC=0.650\n",
      "[Valence (Fold 7)] Ep 06/70 | Loss=0.2904 | Val Acc=63.41% | F1=0.706 | AUC=0.647\n",
      "[Valence (Fold 7)] Ep 07/70 | Loss=0.2156 | Val Acc=63.41% | F1=0.762 | AUC=0.665\n",
      "[Valence (Fold 7)] Ep 08/70 | Loss=0.1453 | Val Acc=58.54% | F1=0.622 | AUC=0.635\n",
      "[Valence (Fold 7)] Ep 09/70 | Loss=0.1023 | Val Acc=63.41% | F1=0.737 | AUC=0.650\n",
      "[Valence (Fold 7)] Ep 10/70 | Loss=0.0645 | Val Acc=60.98% | F1=0.619 | AUC=0.640\n",
      "[Valence (Fold 7)] Ep 11/70 | Loss=0.0394 | Val Acc=60.98% | F1=0.652 | AUC=0.617\n",
      "[Valence (Fold 7)] Ep 12/70 | Loss=0.0227 | Val Acc=56.10% | F1=0.640 | AUC=0.623\n",
      "[Valence (Fold 7)] Ep 13/70 | Loss=0.0514 | Val Acc=65.85% | F1=0.682 | AUC=0.628\n",
      "[Valence (Fold 7)] Ep 14/70 | Loss=0.0216 | Val Acc=60.98% | F1=0.714 | AUC=0.605\n",
      "[Valence (Fold 7)] Ep 15/70 | Loss=0.0209 | Val Acc=58.54% | F1=0.653 | AUC=0.580\n",
      "[Valence (Fold 7)] Ep 16/70 | Loss=0.0127 | Val Acc=63.41% | F1=0.717 | AUC=0.597\n",
      "[Valence (Fold 7)] Ep 17/70 | Loss=0.0095 | Val Acc=65.85% | F1=0.759 | AUC=0.588\n",
      "[Valence (Fold 7)] Ep 18/70 | Loss=0.0124 | Val Acc=65.85% | F1=0.759 | AUC=0.607\n",
      "[Valence (Fold 7)] Ep 19/70 | Loss=0.0158 | Val Acc=65.85% | F1=0.759 | AUC=0.620\n",
      "[Valence (Fold 7)] Ep 20/70 | Loss=0.0091 | Val Acc=63.41% | F1=0.737 | AUC=0.605\n",
      "[Valence (Fold 7)] Ep 21/70 | Loss=0.0488 | Val Acc=60.98% | F1=0.758 | AUC=0.555\n",
      "[Valence (Fold 7)] Ep 22/70 | Loss=0.1635 | Val Acc=58.54% | F1=0.605 | AUC=0.632\n",
      "[Valence (Fold 7)] Ep 23/70 | Loss=0.0427 | Val Acc=48.78% | F1=0.323 | AUC=0.608\n",
      "[Valence (Fold 7)] Ep 24/70 | Loss=0.0341 | Val Acc=65.85% | F1=0.767 | AUC=0.603\n",
      "[Valence (Fold 7)] Ep 25/70 | Loss=0.0320 | Val Acc=48.78% | F1=0.553 | AUC=0.645\n",
      "[Valence (Fold 7)] Ep 26/70 | Loss=0.0668 | Val Acc=60.98% | F1=0.652 | AUC=0.627\n",
      "[Valence (Fold 7)] Ep 27/70 | Loss=0.0284 | Val Acc=63.41% | F1=0.651 | AUC=0.650\n",
      "[Valence (Fold 7)] Ep 28/70 | Loss=0.0139 | Val Acc=60.98% | F1=0.692 | AUC=0.545\n",
      "[Valence (Fold 7)] Ep 29/70 | Loss=0.0121 | Val Acc=48.78% | F1=0.364 | AUC=0.558\n",
      "[Valence (Fold 7)] Ep 30/70 | Loss=0.0047 | Val Acc=56.10% | F1=0.667 | AUC=0.640\n",
      "[Valence (Fold 7)] Ep 31/70 | Loss=0.0038 | Val Acc=65.85% | F1=0.759 | AUC=0.600\n",
      "â¹ [Valence (Fold 7)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 7)] TEST (best thr=0.100) | Acc=46.34% | F1=0.577 | AUC=0.487\n",
      "\n",
      "Fold 7 Results: Acc=0.4634, F1=0.5769, AUC=0.4875\n",
      "\n",
      "----- Valence Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 8) | epochs=70\n",
      "[Valence (Fold 8)] Ep 01/70 | Loss=0.5338 | Val Acc=65.85% | F1=0.759 | AUC=0.458\n",
      "[Valence (Fold 8)] Ep 02/70 | Loss=0.5213 | Val Acc=46.34% | F1=0.522 | AUC=0.450\n",
      "[Valence (Fold 8)] Ep 03/70 | Loss=0.4766 | Val Acc=58.54% | F1=0.679 | AUC=0.510\n",
      "[Valence (Fold 8)] Ep 04/70 | Loss=0.4380 | Val Acc=53.66% | F1=0.678 | AUC=0.510\n",
      "[Valence (Fold 8)] Ep 05/70 | Loss=0.3783 | Val Acc=36.59% | F1=0.381 | AUC=0.460\n",
      "[Valence (Fold 8)] Ep 06/70 | Loss=0.2975 | Val Acc=48.78% | F1=0.553 | AUC=0.468\n",
      "[Valence (Fold 8)] Ep 07/70 | Loss=0.2306 | Val Acc=51.22% | F1=0.583 | AUC=0.458\n",
      "[Valence (Fold 8)] Ep 08/70 | Loss=0.1794 | Val Acc=51.22% | F1=0.667 | AUC=0.483\n",
      "[Valence (Fold 8)] Ep 09/70 | Loss=0.1178 | Val Acc=43.90% | F1=0.549 | AUC=0.483\n",
      "[Valence (Fold 8)] Ep 10/70 | Loss=0.0797 | Val Acc=51.22% | F1=0.565 | AUC=0.545\n",
      "[Valence (Fold 8)] Ep 11/70 | Loss=0.0597 | Val Acc=63.41% | F1=0.727 | AUC=0.610\n",
      "[Valence (Fold 8)] Ep 12/70 | Loss=0.0459 | Val Acc=51.22% | F1=0.630 | AUC=0.542\n",
      "[Valence (Fold 8)] Ep 13/70 | Loss=0.0347 | Val Acc=56.10% | F1=0.690 | AUC=0.443\n",
      "[Valence (Fold 8)] Ep 14/70 | Loss=0.0281 | Val Acc=43.90% | F1=0.549 | AUC=0.497\n",
      "[Valence (Fold 8)] Ep 15/70 | Loss=0.0230 | Val Acc=53.66% | F1=0.537 | AUC=0.527\n",
      "[Valence (Fold 8)] Ep 16/70 | Loss=0.0156 | Val Acc=56.10% | F1=0.690 | AUC=0.450\n",
      "[Valence (Fold 8)] Ep 17/70 | Loss=0.0175 | Val Acc=46.34% | F1=0.577 | AUC=0.522\n",
      "[Valence (Fold 8)] Ep 18/70 | Loss=0.0097 | Val Acc=58.54% | F1=0.702 | AUC=0.527\n",
      "[Valence (Fold 8)] Ep 19/70 | Loss=0.0078 | Val Acc=48.78% | F1=0.571 | AUC=0.535\n",
      "[Valence (Fold 8)] Ep 20/70 | Loss=0.0115 | Val Acc=46.34% | F1=0.577 | AUC=0.540\n",
      "[Valence (Fold 8)] Ep 21/70 | Loss=0.0554 | Val Acc=41.46% | F1=0.077 | AUC=0.450\n",
      "[Valence (Fold 8)] Ep 22/70 | Loss=0.0498 | Val Acc=58.54% | F1=0.702 | AUC=0.565\n",
      "[Valence (Fold 8)] Ep 23/70 | Loss=0.0526 | Val Acc=53.66% | F1=0.689 | AUC=0.508\n",
      "[Valence (Fold 8)] Ep 24/70 | Loss=0.0335 | Val Acc=48.78% | F1=0.553 | AUC=0.510\n",
      "[Valence (Fold 8)] Ep 25/70 | Loss=0.0239 | Val Acc=53.66% | F1=0.642 | AUC=0.497\n",
      "[Valence (Fold 8)] Ep 26/70 | Loss=0.0301 | Val Acc=51.22% | F1=0.677 | AUC=0.378\n",
      "[Valence (Fold 8)] Ep 27/70 | Loss=0.0087 | Val Acc=46.34% | F1=0.577 | AUC=0.427\n",
      "[Valence (Fold 8)] Ep 28/70 | Loss=0.0072 | Val Acc=46.34% | F1=0.500 | AUC=0.560\n",
      "[Valence (Fold 8)] Ep 29/70 | Loss=0.0046 | Val Acc=53.66% | F1=0.642 | AUC=0.525\n",
      "[Valence (Fold 8)] Ep 30/70 | Loss=0.0045 | Val Acc=53.66% | F1=0.642 | AUC=0.465\n",
      "[Valence (Fold 8)] Ep 31/70 | Loss=0.0030 | Val Acc=53.66% | F1=0.698 | AUC=0.385\n",
      "â¹ [Valence (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 8)] TEST (best thr=0.100) | Acc=63.41% | F1=0.754 | AUC=0.560\n",
      "\n",
      "Fold 8 Results: Acc=0.6341, F1=0.7541, AUC=0.5600\n",
      "\n",
      "----- Valence Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 9) | epochs=70\n",
      "[Valence (Fold 9)] Ep 01/70 | Loss=0.5371 | Val Acc=53.66% | F1=0.537 | AUC=0.640\n",
      "[Valence (Fold 9)] Ep 02/70 | Loss=0.5156 | Val Acc=70.73% | F1=0.778 | AUC=0.657\n",
      "[Valence (Fold 9)] Ep 03/70 | Loss=0.4786 | Val Acc=46.34% | F1=0.267 | AUC=0.603\n",
      "[Valence (Fold 9)] Ep 04/70 | Loss=0.4422 | Val Acc=70.73% | F1=0.793 | AUC=0.620\n",
      "[Valence (Fold 9)] Ep 05/70 | Loss=0.4218 | Val Acc=56.10% | F1=0.640 | AUC=0.613\n",
      "[Valence (Fold 9)] Ep 06/70 | Loss=0.3981 | Val Acc=60.98% | F1=0.636 | AUC=0.630\n",
      "[Valence (Fold 9)] Ep 07/70 | Loss=0.2936 | Val Acc=53.66% | F1=0.558 | AUC=0.643\n",
      "[Valence (Fold 9)] Ep 08/70 | Loss=0.1860 | Val Acc=56.10% | F1=0.591 | AUC=0.627\n",
      "[Valence (Fold 9)] Ep 09/70 | Loss=0.1343 | Val Acc=68.29% | F1=0.794 | AUC=0.613\n",
      "[Valence (Fold 9)] Ep 10/70 | Loss=0.0708 | Val Acc=65.85% | F1=0.759 | AUC=0.653\n",
      "[Valence (Fold 9)] Ep 11/70 | Loss=0.0530 | Val Acc=53.66% | F1=0.627 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 12/70 | Loss=0.0309 | Val Acc=63.41% | F1=0.727 | AUC=0.610\n",
      "[Valence (Fold 9)] Ep 13/70 | Loss=0.0291 | Val Acc=68.29% | F1=0.764 | AUC=0.632\n",
      "[Valence (Fold 9)] Ep 14/70 | Loss=0.0325 | Val Acc=56.10% | F1=0.640 | AUC=0.640\n",
      "[Valence (Fold 9)] Ep 15/70 | Loss=0.0205 | Val Acc=63.41% | F1=0.727 | AUC=0.630\n",
      "[Valence (Fold 9)] Ep 16/70 | Loss=0.0118 | Val Acc=63.41% | F1=0.727 | AUC=0.623\n",
      "[Valence (Fold 9)] Ep 17/70 | Loss=0.0170 | Val Acc=56.10% | F1=0.640 | AUC=0.627\n",
      "[Valence (Fold 9)] Ep 18/70 | Loss=0.0119 | Val Acc=63.41% | F1=0.727 | AUC=0.625\n",
      "[Valence (Fold 9)] Ep 19/70 | Loss=0.0108 | Val Acc=65.85% | F1=0.741 | AUC=0.637\n",
      "[Valence (Fold 9)] Ep 20/70 | Loss=0.0096 | Val Acc=56.10% | F1=0.640 | AUC=0.637\n",
      "[Valence (Fold 9)] Ep 21/70 | Loss=0.0553 | Val Acc=63.41% | F1=0.727 | AUC=0.623\n",
      "[Valence (Fold 9)] Ep 22/70 | Loss=0.0715 | Val Acc=46.34% | F1=0.421 | AUC=0.570\n",
      "[Valence (Fold 9)] Ep 23/70 | Loss=0.0445 | Val Acc=60.98% | F1=0.714 | AUC=0.585\n",
      "[Valence (Fold 9)] Ep 24/70 | Loss=0.0260 | Val Acc=65.85% | F1=0.759 | AUC=0.620\n",
      "[Valence (Fold 9)] Ep 25/70 | Loss=0.0290 | Val Acc=68.29% | F1=0.787 | AUC=0.620\n",
      "[Valence (Fold 9)] Ep 26/70 | Loss=0.0282 | Val Acc=60.98% | F1=0.733 | AUC=0.595\n",
      "[Valence (Fold 9)] Ep 27/70 | Loss=0.0140 | Val Acc=65.85% | F1=0.750 | AUC=0.590\n",
      "[Valence (Fold 9)] Ep 28/70 | Loss=0.0075 | Val Acc=53.66% | F1=0.612 | AUC=0.580\n",
      "[Valence (Fold 9)] Ep 29/70 | Loss=0.0058 | Val Acc=58.54% | F1=0.653 | AUC=0.595\n",
      "[Valence (Fold 9)] Ep 30/70 | Loss=0.0071 | Val Acc=56.10% | F1=0.625 | AUC=0.595\n",
      "[Valence (Fold 9)] Ep 31/70 | Loss=0.0013 | Val Acc=60.98% | F1=0.704 | AUC=0.557\n",
      "â¹ [Valence (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 9)] TEST (best thr=0.100) | Acc=58.54% | F1=0.730 | AUC=0.265\n",
      "\n",
      "Fold 9 Results: Acc=0.5854, F1=0.7302, AUC=0.2650\n",
      "\n",
      "----- Valence Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 10) | epochs=70\n",
      "[Valence (Fold 10)] Ep 01/70 | Loss=0.5399 | Val Acc=41.46% | F1=0.400 | AUC=0.395\n",
      "[Valence (Fold 10)] Ep 02/70 | Loss=0.5148 | Val Acc=48.78% | F1=0.618 | AUC=0.380\n",
      "[Valence (Fold 10)] Ep 03/70 | Loss=0.4833 | Val Acc=51.22% | F1=0.474 | AUC=0.453\n",
      "[Valence (Fold 10)] Ep 04/70 | Loss=0.4399 | Val Acc=53.66% | F1=0.678 | AUC=0.378\n",
      "[Valence (Fold 10)] Ep 05/70 | Loss=0.3611 | Val Acc=43.90% | F1=0.511 | AUC=0.440\n",
      "[Valence (Fold 10)] Ep 06/70 | Loss=0.2777 | Val Acc=48.78% | F1=0.632 | AUC=0.467\n",
      "[Valence (Fold 10)] Ep 07/70 | Loss=0.1874 | Val Acc=41.46% | F1=0.294 | AUC=0.477\n",
      "[Valence (Fold 10)] Ep 08/70 | Loss=0.1374 | Val Acc=56.10% | F1=0.679 | AUC=0.430\n",
      "[Valence (Fold 10)] Ep 09/70 | Loss=0.0521 | Val Acc=46.34% | F1=0.560 | AUC=0.417\n",
      "[Valence (Fold 10)] Ep 10/70 | Loss=0.0553 | Val Acc=46.34% | F1=0.593 | AUC=0.432\n",
      "[Valence (Fold 10)] Ep 11/70 | Loss=0.0308 | Val Acc=51.22% | F1=0.655 | AUC=0.375\n",
      "[Valence (Fold 10)] Ep 12/70 | Loss=0.0175 | Val Acc=41.46% | F1=0.520 | AUC=0.430\n",
      "[Valence (Fold 10)] Ep 13/70 | Loss=0.0133 | Val Acc=56.10% | F1=0.667 | AUC=0.427\n",
      "[Valence (Fold 10)] Ep 14/70 | Loss=0.0119 | Val Acc=53.66% | F1=0.655 | AUC=0.425\n",
      "[Valence (Fold 10)] Ep 15/70 | Loss=0.0076 | Val Acc=48.78% | F1=0.571 | AUC=0.438\n",
      "[Valence (Fold 10)] Ep 16/70 | Loss=0.0076 | Val Acc=48.78% | F1=0.588 | AUC=0.445\n",
      "[Valence (Fold 10)] Ep 17/70 | Loss=0.0094 | Val Acc=53.66% | F1=0.642 | AUC=0.425\n",
      "[Valence (Fold 10)] Ep 18/70 | Loss=0.0090 | Val Acc=53.66% | F1=0.642 | AUC=0.435\n",
      "[Valence (Fold 10)] Ep 19/70 | Loss=0.0059 | Val Acc=48.78% | F1=0.588 | AUC=0.415\n",
      "[Valence (Fold 10)] Ep 20/70 | Loss=0.0060 | Val Acc=51.22% | F1=0.615 | AUC=0.422\n",
      "[Valence (Fold 10)] Ep 21/70 | Loss=0.0346 | Val Acc=53.66% | F1=0.655 | AUC=0.522\n",
      "[Valence (Fold 10)] Ep 22/70 | Loss=0.0519 | Val Acc=56.10% | F1=0.700 | AUC=0.443\n",
      "[Valence (Fold 10)] Ep 23/70 | Loss=0.0488 | Val Acc=48.78% | F1=0.571 | AUC=0.362\n",
      "[Valence (Fold 10)] Ep 24/70 | Loss=0.0163 | Val Acc=34.15% | F1=0.270 | AUC=0.362\n",
      "[Valence (Fold 10)] Ep 25/70 | Loss=0.0239 | Val Acc=48.78% | F1=0.604 | AUC=0.393\n",
      "[Valence (Fold 10)] Ep 26/70 | Loss=0.0135 | Val Acc=56.10% | F1=0.667 | AUC=0.505\n",
      "[Valence (Fold 10)] Ep 27/70 | Loss=0.0283 | Val Acc=41.46% | F1=0.500 | AUC=0.417\n",
      "[Valence (Fold 10)] Ep 28/70 | Loss=0.0110 | Val Acc=46.34% | F1=0.476 | AUC=0.410\n",
      "[Valence (Fold 10)] Ep 29/70 | Loss=0.0084 | Val Acc=51.22% | F1=0.655 | AUC=0.353\n",
      "[Valence (Fold 10)] Ep 30/70 | Loss=0.0101 | Val Acc=51.22% | F1=0.643 | AUC=0.405\n",
      "[Valence (Fold 10)] Ep 31/70 | Loss=0.0011 | Val Acc=46.34% | F1=0.560 | AUC=0.417\n",
      "â¹ [Valence (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 10)] TEST (best thr=0.750) | Acc=65.85% | F1=0.720 | AUC=0.633\n",
      "\n",
      "Fold 10 Results: Acc=0.6585, F1=0.7200, AUC=0.6325\n",
      "\n",
      "===== FINAL Valence 10-FOLD RESULTS =====\n",
      "Accuracy: 0.6111 Â± 0.0599\n",
      "F1-score: 0.7044 Â± 0.0560\n",
      "AUC:      0.5670 Â± 0.1221\n",
      "\n",
      "########## Arousal: 10-fold STRATIFIED CV ##########\n",
      "Arousal global median (for stratification) = 3.0000\n",
      "Class counts: [114 300]\n",
      "\n",
      "----- Arousal Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 1) | epochs=70\n",
      "[Arousal (Fold 1)] Ep 01/70 | Loss=0.3777 | Val Acc=30.95% | F1=0.065 | AUC=0.589\n",
      "[Arousal (Fold 1)] Ep 02/70 | Loss=0.3722 | Val Acc=33.33% | F1=0.125 | AUC=0.617\n",
      "[Arousal (Fold 1)] Ep 03/70 | Loss=0.3545 | Val Acc=52.38% | F1=0.565 | AUC=0.653\n",
      "[Arousal (Fold 1)] Ep 04/70 | Loss=0.3341 | Val Acc=52.38% | F1=0.630 | AUC=0.564\n",
      "[Arousal (Fold 1)] Ep 05/70 | Loss=0.2876 | Val Acc=38.10% | F1=0.278 | AUC=0.528\n",
      "[Arousal (Fold 1)] Ep 06/70 | Loss=0.2463 | Val Acc=59.52% | F1=0.730 | AUC=0.561\n",
      "[Arousal (Fold 1)] Ep 07/70 | Loss=0.1862 | Val Acc=66.67% | F1=0.800 | AUC=0.483\n",
      "[Arousal (Fold 1)] Ep 08/70 | Loss=0.1072 | Val Acc=61.90% | F1=0.733 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 09/70 | Loss=0.0614 | Val Acc=54.76% | F1=0.642 | AUC=0.492\n",
      "[Arousal (Fold 1)] Ep 10/70 | Loss=0.0336 | Val Acc=35.71% | F1=0.426 | AUC=0.369\n",
      "[Arousal (Fold 1)] Ep 11/70 | Loss=0.0270 | Val Acc=69.05% | F1=0.806 | AUC=0.594\n",
      "[Arousal (Fold 1)] Ep 12/70 | Loss=0.0186 | Val Acc=57.14% | F1=0.700 | AUC=0.444\n",
      "[Arousal (Fold 1)] Ep 13/70 | Loss=0.0095 | Val Acc=61.90% | F1=0.765 | AUC=0.458\n",
      "[Arousal (Fold 1)] Ep 14/70 | Loss=0.0098 | Val Acc=64.29% | F1=0.754 | AUC=0.514\n",
      "[Arousal (Fold 1)] Ep 15/70 | Loss=0.0116 | Val Acc=61.90% | F1=0.742 | AUC=0.522\n",
      "[Arousal (Fold 1)] Ep 16/70 | Loss=0.0073 | Val Acc=59.52% | F1=0.746 | AUC=0.486\n",
      "[Arousal (Fold 1)] Ep 17/70 | Loss=0.0059 | Val Acc=61.90% | F1=0.758 | AUC=0.503\n",
      "[Arousal (Fold 1)] Ep 18/70 | Loss=0.0062 | Val Acc=59.52% | F1=0.746 | AUC=0.514\n",
      "[Arousal (Fold 1)] Ep 19/70 | Loss=0.0056 | Val Acc=59.52% | F1=0.746 | AUC=0.531\n",
      "[Arousal (Fold 1)] Ep 20/70 | Loss=0.0047 | Val Acc=59.52% | F1=0.746 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 21/70 | Loss=0.0144 | Val Acc=69.05% | F1=0.812 | AUC=0.478\n",
      "[Arousal (Fold 1)] Ep 22/70 | Loss=0.0358 | Val Acc=64.29% | F1=0.769 | AUC=0.528\n",
      "[Arousal (Fold 1)] Ep 23/70 | Loss=0.0322 | Val Acc=69.05% | F1=0.812 | AUC=0.550\n",
      "[Arousal (Fold 1)] Ep 24/70 | Loss=0.0250 | Val Acc=71.43% | F1=0.829 | AUC=0.419\n",
      "[Arousal (Fold 1)] Ep 25/70 | Loss=0.0268 | Val Acc=59.52% | F1=0.746 | AUC=0.439\n",
      "[Arousal (Fold 1)] Ep 26/70 | Loss=0.0111 | Val Acc=59.52% | F1=0.738 | AUC=0.447\n",
      "[Arousal (Fold 1)] Ep 27/70 | Loss=0.0058 | Val Acc=66.67% | F1=0.794 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 28/70 | Loss=0.0031 | Val Acc=64.29% | F1=0.783 | AUC=0.431\n",
      "[Arousal (Fold 1)] Ep 29/70 | Loss=0.0031 | Val Acc=57.14% | F1=0.679 | AUC=0.525\n",
      "[Arousal (Fold 1)] Ep 30/70 | Loss=0.0054 | Val Acc=47.62% | F1=0.633 | AUC=0.325\n",
      "[Arousal (Fold 1)] Ep 31/70 | Loss=0.0011 | Val Acc=66.67% | F1=0.794 | AUC=0.375\n",
      "[Arousal (Fold 1)] Ep 32/70 | Loss=0.0007 | Val Acc=64.29% | F1=0.776 | AUC=0.339\n",
      "[Arousal (Fold 1)] Ep 33/70 | Loss=0.0005 | Val Acc=64.29% | F1=0.776 | AUC=0.378\n",
      "[Arousal (Fold 1)] Ep 34/70 | Loss=0.0006 | Val Acc=64.29% | F1=0.776 | AUC=0.383\n",
      "[Arousal (Fold 1)] Ep 35/70 | Loss=0.0011 | Val Acc=64.29% | F1=0.776 | AUC=0.381\n",
      "[Arousal (Fold 1)] Ep 36/70 | Loss=0.0006 | Val Acc=61.90% | F1=0.758 | AUC=0.367\n",
      "[Arousal (Fold 1)] Ep 37/70 | Loss=0.0005 | Val Acc=61.90% | F1=0.758 | AUC=0.367\n",
      "[Arousal (Fold 1)] Ep 38/70 | Loss=0.0009 | Val Acc=61.90% | F1=0.758 | AUC=0.389\n",
      "[Arousal (Fold 1)] Ep 39/70 | Loss=0.0008 | Val Acc=64.29% | F1=0.776 | AUC=0.397\n",
      "[Arousal (Fold 1)] Ep 40/70 | Loss=0.0011 | Val Acc=61.90% | F1=0.758 | AUC=0.400\n",
      "[Arousal (Fold 1)] Ep 41/70 | Loss=0.0004 | Val Acc=64.29% | F1=0.783 | AUC=0.469\n",
      "[Arousal (Fold 1)] Ep 42/70 | Loss=0.0006 | Val Acc=64.29% | F1=0.769 | AUC=0.461\n",
      "â¹ [Arousal (Fold 1)] Early stopping at epoch 42\n",
      "\n",
      "ðŸ”š [Arousal (Fold 1)] TEST (best thr=0.100) | Acc=66.67% | F1=0.774 | AUC=0.567\n",
      "\n",
      "Fold 1 Results: Acc=0.6667, F1=0.7742, AUC=0.5667\n",
      "\n",
      "----- Arousal Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 2) | epochs=70\n",
      "[Arousal (Fold 2)] Ep 01/70 | Loss=0.3781 | Val Acc=64.29% | F1=0.783 | AUC=0.458\n",
      "[Arousal (Fold 2)] Ep 02/70 | Loss=0.3614 | Val Acc=45.24% | F1=0.549 | AUC=0.425\n",
      "[Arousal (Fold 2)] Ep 03/70 | Loss=0.3353 | Val Acc=50.00% | F1=0.588 | AUC=0.469\n",
      "[Arousal (Fold 2)] Ep 04/70 | Loss=0.2976 | Val Acc=64.29% | F1=0.783 | AUC=0.400\n",
      "[Arousal (Fold 2)] Ep 05/70 | Loss=0.2317 | Val Acc=47.62% | F1=0.522 | AUC=0.458\n",
      "[Arousal (Fold 2)] Ep 06/70 | Loss=0.1662 | Val Acc=64.29% | F1=0.776 | AUC=0.378\n",
      "[Arousal (Fold 2)] Ep 07/70 | Loss=0.0848 | Val Acc=59.52% | F1=0.746 | AUC=0.458\n",
      "[Arousal (Fold 2)] Ep 08/70 | Loss=0.0664 | Val Acc=66.67% | F1=0.794 | AUC=0.517\n",
      "[Arousal (Fold 2)] Ep 09/70 | Loss=0.0369 | Val Acc=64.29% | F1=0.776 | AUC=0.431\n",
      "[Arousal (Fold 2)] Ep 10/70 | Loss=0.0335 | Val Acc=64.29% | F1=0.783 | AUC=0.311\n",
      "[Arousal (Fold 2)] Ep 11/70 | Loss=0.0185 | Val Acc=57.14% | F1=0.700 | AUC=0.456\n",
      "[Arousal (Fold 2)] Ep 12/70 | Loss=0.0133 | Val Acc=59.52% | F1=0.746 | AUC=0.436\n",
      "[Arousal (Fold 2)] Ep 13/70 | Loss=0.0078 | Val Acc=59.52% | F1=0.738 | AUC=0.419\n",
      "[Arousal (Fold 2)] Ep 14/70 | Loss=0.0075 | Val Acc=59.52% | F1=0.738 | AUC=0.428\n",
      "[Arousal (Fold 2)] Ep 15/70 | Loss=0.0052 | Val Acc=66.67% | F1=0.800 | AUC=0.375\n",
      "[Arousal (Fold 2)] Ep 16/70 | Loss=0.0045 | Val Acc=61.90% | F1=0.765 | AUC=0.450\n",
      "[Arousal (Fold 2)] Ep 17/70 | Loss=0.0049 | Val Acc=59.52% | F1=0.746 | AUC=0.439\n",
      "[Arousal (Fold 2)] Ep 18/70 | Loss=0.0037 | Val Acc=61.90% | F1=0.765 | AUC=0.417\n",
      "[Arousal (Fold 2)] Ep 19/70 | Loss=0.0051 | Val Acc=61.90% | F1=0.765 | AUC=0.419\n",
      "[Arousal (Fold 2)] Ep 20/70 | Loss=0.0042 | Val Acc=61.90% | F1=0.765 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 21/70 | Loss=0.0228 | Val Acc=61.90% | F1=0.758 | AUC=0.397\n",
      "[Arousal (Fold 2)] Ep 22/70 | Loss=0.0229 | Val Acc=69.05% | F1=0.817 | AUC=0.442\n",
      "[Arousal (Fold 2)] Ep 23/70 | Loss=0.0524 | Val Acc=71.43% | F1=0.833 | AUC=0.444\n",
      "[Arousal (Fold 2)] Ep 24/70 | Loss=0.0327 | Val Acc=66.67% | F1=0.800 | AUC=0.494\n",
      "[Arousal (Fold 2)] Ep 25/70 | Loss=0.0158 | Val Acc=66.67% | F1=0.794 | AUC=0.294\n",
      "[Arousal (Fold 2)] Ep 26/70 | Loss=0.0136 | Val Acc=52.38% | F1=0.655 | AUC=0.433\n",
      "[Arousal (Fold 2)] Ep 27/70 | Loss=0.0129 | Val Acc=59.52% | F1=0.738 | AUC=0.417\n",
      "[Arousal (Fold 2)] Ep 28/70 | Loss=0.0049 | Val Acc=54.76% | F1=0.667 | AUC=0.411\n",
      "[Arousal (Fold 2)] Ep 29/70 | Loss=0.0039 | Val Acc=64.29% | F1=0.776 | AUC=0.375\n",
      "[Arousal (Fold 2)] Ep 30/70 | Loss=0.0020 | Val Acc=66.67% | F1=0.800 | AUC=0.344\n",
      "[Arousal (Fold 2)] Ep 31/70 | Loss=0.0005 | Val Acc=61.90% | F1=0.765 | AUC=0.317\n",
      "[Arousal (Fold 2)] Ep 32/70 | Loss=0.0008 | Val Acc=64.29% | F1=0.783 | AUC=0.308\n",
      "[Arousal (Fold 2)] Ep 33/70 | Loss=0.0007 | Val Acc=64.29% | F1=0.783 | AUC=0.358\n",
      "[Arousal (Fold 2)] Ep 34/70 | Loss=0.0012 | Val Acc=64.29% | F1=0.769 | AUC=0.375\n",
      "[Arousal (Fold 2)] Ep 35/70 | Loss=0.0004 | Val Acc=64.29% | F1=0.776 | AUC=0.372\n",
      "[Arousal (Fold 2)] Ep 36/70 | Loss=0.0003 | Val Acc=64.29% | F1=0.776 | AUC=0.378\n",
      "[Arousal (Fold 2)] Ep 37/70 | Loss=0.0006 | Val Acc=66.67% | F1=0.794 | AUC=0.367\n",
      "[Arousal (Fold 2)] Ep 38/70 | Loss=0.0004 | Val Acc=66.67% | F1=0.794 | AUC=0.353\n",
      "[Arousal (Fold 2)] Ep 39/70 | Loss=0.0004 | Val Acc=64.29% | F1=0.783 | AUC=0.353\n",
      "[Arousal (Fold 2)] Ep 40/70 | Loss=0.0004 | Val Acc=64.29% | F1=0.783 | AUC=0.372\n",
      "[Arousal (Fold 2)] Ep 41/70 | Loss=0.0091 | Val Acc=61.90% | F1=0.733 | AUC=0.467\n",
      "â¹ [Arousal (Fold 2)] Early stopping at epoch 41\n",
      "\n",
      "ðŸ”š [Arousal (Fold 2)] TEST (best thr=0.100) | Acc=59.52% | F1=0.702 | AUC=0.519\n",
      "\n",
      "Fold 2 Results: Acc=0.5952, F1=0.7018, AUC=0.5194\n",
      "\n",
      "----- Arousal Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 3) | epochs=70\n",
      "[Arousal (Fold 3)] Ep 01/70 | Loss=0.3796 | Val Acc=57.14% | F1=0.591 | AUC=0.669\n",
      "[Arousal (Fold 3)] Ep 02/70 | Loss=0.3571 | Val Acc=54.76% | F1=0.578 | AUC=0.675\n",
      "[Arousal (Fold 3)] Ep 03/70 | Loss=0.3379 | Val Acc=45.24% | F1=0.489 | AUC=0.583\n",
      "[Arousal (Fold 3)] Ep 04/70 | Loss=0.2916 | Val Acc=57.14% | F1=0.640 | AUC=0.597\n",
      "[Arousal (Fold 3)] Ep 05/70 | Loss=0.2232 | Val Acc=47.62% | F1=0.542 | AUC=0.583\n",
      "[Arousal (Fold 3)] Ep 06/70 | Loss=0.1399 | Val Acc=69.05% | F1=0.772 | AUC=0.642\n",
      "[Arousal (Fold 3)] Ep 07/70 | Loss=0.0806 | Val Acc=71.43% | F1=0.829 | AUC=0.417\n",
      "[Arousal (Fold 3)] Ep 08/70 | Loss=0.0639 | Val Acc=52.38% | F1=0.600 | AUC=0.542\n",
      "[Arousal (Fold 3)] Ep 09/70 | Loss=0.0354 | Val Acc=61.90% | F1=0.704 | AUC=0.650\n",
      "[Arousal (Fold 3)] Ep 10/70 | Loss=0.0205 | Val Acc=64.29% | F1=0.776 | AUC=0.472\n",
      "[Arousal (Fold 3)] Ep 11/70 | Loss=0.0232 | Val Acc=66.67% | F1=0.794 | AUC=0.508\n",
      "[Arousal (Fold 3)] Ep 12/70 | Loss=0.0094 | Val Acc=64.29% | F1=0.776 | AUC=0.497\n",
      "[Arousal (Fold 3)] Ep 13/70 | Loss=0.0071 | Val Acc=66.67% | F1=0.788 | AUC=0.542\n",
      "[Arousal (Fold 3)] Ep 14/70 | Loss=0.0064 | Val Acc=64.29% | F1=0.776 | AUC=0.481\n",
      "[Arousal (Fold 3)] Ep 15/70 | Loss=0.0056 | Val Acc=64.29% | F1=0.769 | AUC=0.564\n",
      "[Arousal (Fold 3)] Ep 16/70 | Loss=0.0043 | Val Acc=66.67% | F1=0.788 | AUC=0.561\n",
      "[Arousal (Fold 3)] Ep 17/70 | Loss=0.0036 | Val Acc=64.29% | F1=0.776 | AUC=0.544\n",
      "[Arousal (Fold 3)] Ep 18/70 | Loss=0.0039 | Val Acc=66.67% | F1=0.788 | AUC=0.553\n",
      "[Arousal (Fold 3)] Ep 19/70 | Loss=0.0034 | Val Acc=64.29% | F1=0.769 | AUC=0.556\n",
      "[Arousal (Fold 3)] Ep 20/70 | Loss=0.0035 | Val Acc=64.29% | F1=0.769 | AUC=0.556\n",
      "[Arousal (Fold 3)] Ep 21/70 | Loss=0.0069 | Val Acc=64.29% | F1=0.776 | AUC=0.506\n",
      "[Arousal (Fold 3)] Ep 22/70 | Loss=0.0553 | Val Acc=64.29% | F1=0.776 | AUC=0.500\n",
      "[Arousal (Fold 3)] Ep 23/70 | Loss=0.0212 | Val Acc=50.00% | F1=0.644 | AUC=0.497\n",
      "[Arousal (Fold 3)] Ep 24/70 | Loss=0.0262 | Val Acc=42.86% | F1=0.500 | AUC=0.494\n",
      "[Arousal (Fold 3)] Ep 25/70 | Loss=0.0151 | Val Acc=69.05% | F1=0.812 | AUC=0.478\n",
      "[Arousal (Fold 3)] Ep 26/70 | Loss=0.0114 | Val Acc=57.14% | F1=0.719 | AUC=0.506\n",
      "[Arousal (Fold 3)] Ep 27/70 | Loss=0.0054 | Val Acc=64.29% | F1=0.776 | AUC=0.425\n",
      "[Arousal (Fold 3)] Ep 28/70 | Loss=0.0083 | Val Acc=54.76% | F1=0.698 | AUC=0.544\n",
      "[Arousal (Fold 3)] Ep 29/70 | Loss=0.0018 | Val Acc=57.14% | F1=0.719 | AUC=0.467\n",
      "[Arousal (Fold 3)] Ep 30/70 | Loss=0.0011 | Val Acc=57.14% | F1=0.700 | AUC=0.492\n",
      "[Arousal (Fold 3)] Ep 31/70 | Loss=0.0021 | Val Acc=64.29% | F1=0.776 | AUC=0.486\n",
      "â¹ [Arousal (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 3)] TEST (best thr=0.300) | Acc=73.81% | F1=0.845 | AUC=0.525\n",
      "\n",
      "Fold 3 Results: Acc=0.7381, F1=0.8451, AUC=0.5250\n",
      "\n",
      "----- Arousal Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 4) | epochs=70\n",
      "[Arousal (Fold 4)] Ep 01/70 | Loss=0.3717 | Val Acc=33.33% | F1=0.300 | AUC=0.408\n",
      "[Arousal (Fold 4)] Ep 02/70 | Loss=0.3524 | Val Acc=42.86% | F1=0.500 | AUC=0.403\n",
      "[Arousal (Fold 4)] Ep 03/70 | Loss=0.3260 | Val Acc=54.76% | F1=0.678 | AUC=0.422\n",
      "[Arousal (Fold 4)] Ep 04/70 | Loss=0.2871 | Val Acc=40.48% | F1=0.510 | AUC=0.397\n",
      "[Arousal (Fold 4)] Ep 05/70 | Loss=0.2450 | Val Acc=33.33% | F1=0.222 | AUC=0.536\n",
      "[Arousal (Fold 4)] Ep 06/70 | Loss=0.1892 | Val Acc=64.29% | F1=0.762 | AUC=0.461\n",
      "[Arousal (Fold 4)] Ep 07/70 | Loss=0.1027 | Val Acc=71.43% | F1=0.818 | AUC=0.556\n",
      "[Arousal (Fold 4)] Ep 08/70 | Loss=0.0701 | Val Acc=64.29% | F1=0.769 | AUC=0.483\n",
      "[Arousal (Fold 4)] Ep 09/70 | Loss=0.0480 | Val Acc=69.05% | F1=0.806 | AUC=0.531\n",
      "[Arousal (Fold 4)] Ep 10/70 | Loss=0.0309 | Val Acc=52.38% | F1=0.630 | AUC=0.550\n",
      "[Arousal (Fold 4)] Ep 11/70 | Loss=0.0170 | Val Acc=64.29% | F1=0.769 | AUC=0.511\n",
      "[Arousal (Fold 4)] Ep 12/70 | Loss=0.0144 | Val Acc=69.05% | F1=0.817 | AUC=0.558\n",
      "[Arousal (Fold 4)] Ep 13/70 | Loss=0.0100 | Val Acc=57.14% | F1=0.700 | AUC=0.472\n",
      "[Arousal (Fold 4)] Ep 14/70 | Loss=0.0063 | Val Acc=71.43% | F1=0.824 | AUC=0.481\n",
      "[Arousal (Fold 4)] Ep 15/70 | Loss=0.0066 | Val Acc=69.05% | F1=0.806 | AUC=0.497\n",
      "[Arousal (Fold 4)] Ep 16/70 | Loss=0.0052 | Val Acc=66.67% | F1=0.788 | AUC=0.478\n",
      "[Arousal (Fold 4)] Ep 17/70 | Loss=0.0047 | Val Acc=71.43% | F1=0.824 | AUC=0.464\n",
      "[Arousal (Fold 4)] Ep 18/70 | Loss=0.0063 | Val Acc=69.05% | F1=0.806 | AUC=0.486\n",
      "[Arousal (Fold 4)] Ep 19/70 | Loss=0.0057 | Val Acc=71.43% | F1=0.824 | AUC=0.481\n",
      "[Arousal (Fold 4)] Ep 20/70 | Loss=0.0060 | Val Acc=71.43% | F1=0.824 | AUC=0.472\n",
      "[Arousal (Fold 4)] Ep 21/70 | Loss=0.0165 | Val Acc=61.90% | F1=0.742 | AUC=0.533\n",
      "[Arousal (Fold 4)] Ep 22/70 | Loss=0.0227 | Val Acc=42.86% | F1=0.538 | AUC=0.406\n",
      "[Arousal (Fold 4)] Ep 23/70 | Loss=0.0293 | Val Acc=66.67% | F1=0.800 | AUC=0.500\n",
      "[Arousal (Fold 4)] Ep 24/70 | Loss=0.0170 | Val Acc=64.29% | F1=0.783 | AUC=0.508\n",
      "[Arousal (Fold 4)] Ep 25/70 | Loss=0.0259 | Val Acc=59.52% | F1=0.721 | AUC=0.542\n",
      "[Arousal (Fold 4)] Ep 26/70 | Loss=0.0091 | Val Acc=54.76% | F1=0.642 | AUC=0.525\n",
      "[Arousal (Fold 4)] Ep 27/70 | Loss=0.0024 | Val Acc=69.05% | F1=0.806 | AUC=0.478\n",
      "[Arousal (Fold 4)] Ep 28/70 | Loss=0.0023 | Val Acc=57.14% | F1=0.679 | AUC=0.497\n",
      "[Arousal (Fold 4)] Ep 29/70 | Loss=0.0046 | Val Acc=61.90% | F1=0.750 | AUC=0.519\n",
      "[Arousal (Fold 4)] Ep 30/70 | Loss=0.0063 | Val Acc=69.05% | F1=0.812 | AUC=0.483\n",
      "[Arousal (Fold 4)] Ep 31/70 | Loss=0.0007 | Val Acc=66.67% | F1=0.788 | AUC=0.511\n",
      "â¹ [Arousal (Fold 4)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 4)] TEST (best thr=0.100) | Acc=61.90% | F1=0.758 | AUC=0.361\n",
      "\n",
      "Fold 4 Results: Acc=0.6190, F1=0.7576, AUC=0.3611\n",
      "\n",
      "----- Arousal Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 5) | epochs=70\n",
      "[Arousal (Fold 5)] Ep 01/70 | Loss=0.3842 | Val Acc=36.59% | F1=0.235 | AUC=0.633\n",
      "[Arousal (Fold 5)] Ep 02/70 | Loss=0.3694 | Val Acc=43.90% | F1=0.378 | AUC=0.682\n",
      "[Arousal (Fold 5)] Ep 03/70 | Loss=0.3556 | Val Acc=58.54% | F1=0.712 | AUC=0.582\n",
      "[Arousal (Fold 5)] Ep 04/70 | Loss=0.3199 | Val Acc=63.41% | F1=0.762 | AUC=0.573\n",
      "[Arousal (Fold 5)] Ep 05/70 | Loss=0.2706 | Val Acc=60.98% | F1=0.742 | AUC=0.582\n",
      "[Arousal (Fold 5)] Ep 06/70 | Loss=0.2117 | Val Acc=56.10% | F1=0.654 | AUC=0.606\n",
      "[Arousal (Fold 5)] Ep 07/70 | Loss=0.1281 | Val Acc=65.85% | F1=0.788 | AUC=0.533\n",
      "[Arousal (Fold 5)] Ep 08/70 | Loss=0.0782 | Val Acc=58.54% | F1=0.721 | AUC=0.524\n",
      "[Arousal (Fold 5)] Ep 09/70 | Loss=0.0358 | Val Acc=65.85% | F1=0.794 | AUC=0.373\n",
      "[Arousal (Fold 5)] Ep 10/70 | Loss=0.0309 | Val Acc=58.54% | F1=0.721 | AUC=0.548\n",
      "[Arousal (Fold 5)] Ep 11/70 | Loss=0.0181 | Val Acc=60.98% | F1=0.733 | AUC=0.430\n",
      "[Arousal (Fold 5)] Ep 12/70 | Loss=0.0122 | Val Acc=65.85% | F1=0.774 | AUC=0.488\n",
      "[Arousal (Fold 5)] Ep 13/70 | Loss=0.0064 | Val Acc=60.98% | F1=0.750 | AUC=0.445\n",
      "[Arousal (Fold 5)] Ep 14/70 | Loss=0.0063 | Val Acc=56.10% | F1=0.679 | AUC=0.461\n",
      "[Arousal (Fold 5)] Ep 15/70 | Loss=0.0058 | Val Acc=63.41% | F1=0.762 | AUC=0.448\n",
      "[Arousal (Fold 5)] Ep 16/70 | Loss=0.0040 | Val Acc=63.41% | F1=0.762 | AUC=0.458\n",
      "[Arousal (Fold 5)] Ep 17/70 | Loss=0.0047 | Val Acc=65.85% | F1=0.788 | AUC=0.461\n",
      "[Arousal (Fold 5)] Ep 18/70 | Loss=0.0041 | Val Acc=65.85% | F1=0.788 | AUC=0.467\n",
      "[Arousal (Fold 5)] Ep 19/70 | Loss=0.0043 | Val Acc=63.41% | F1=0.762 | AUC=0.458\n",
      "[Arousal (Fold 5)] Ep 20/70 | Loss=0.0043 | Val Acc=63.41% | F1=0.762 | AUC=0.455\n",
      "[Arousal (Fold 5)] Ep 21/70 | Loss=0.0400 | Val Acc=56.10% | F1=0.679 | AUC=0.491\n",
      "[Arousal (Fold 5)] Ep 22/70 | Loss=0.0408 | Val Acc=51.22% | F1=0.667 | AUC=0.445\n",
      "[Arousal (Fold 5)] Ep 23/70 | Loss=0.0293 | Val Acc=56.10% | F1=0.625 | AUC=0.576\n",
      "[Arousal (Fold 5)] Ep 24/70 | Loss=0.0163 | Val Acc=63.41% | F1=0.776 | AUC=0.373\n",
      "[Arousal (Fold 5)] Ep 25/70 | Loss=0.0096 | Val Acc=65.85% | F1=0.794 | AUC=0.476\n",
      "[Arousal (Fold 5)] Ep 26/70 | Loss=0.0054 | Val Acc=70.73% | F1=0.829 | AUC=0.400\n",
      "[Arousal (Fold 5)] Ep 27/70 | Loss=0.0146 | Val Acc=48.78% | F1=0.604 | AUC=0.445\n",
      "[Arousal (Fold 5)] Ep 28/70 | Loss=0.0087 | Val Acc=53.66% | F1=0.667 | AUC=0.433\n",
      "[Arousal (Fold 5)] Ep 29/70 | Loss=0.0059 | Val Acc=63.41% | F1=0.762 | AUC=0.433\n",
      "[Arousal (Fold 5)] Ep 30/70 | Loss=0.0027 | Val Acc=51.22% | F1=0.615 | AUC=0.524\n",
      "[Arousal (Fold 5)] Ep 31/70 | Loss=0.0008 | Val Acc=60.98% | F1=0.758 | AUC=0.336\n",
      "[Arousal (Fold 5)] Ep 32/70 | Loss=0.0011 | Val Acc=63.41% | F1=0.762 | AUC=0.376\n",
      "[Arousal (Fold 5)] Ep 33/70 | Loss=0.0023 | Val Acc=58.54% | F1=0.738 | AUC=0.417\n",
      "[Arousal (Fold 5)] Ep 34/70 | Loss=0.0005 | Val Acc=60.98% | F1=0.758 | AUC=0.418\n",
      "[Arousal (Fold 5)] Ep 35/70 | Loss=0.0005 | Val Acc=63.41% | F1=0.762 | AUC=0.430\n",
      "[Arousal (Fold 5)] Ep 36/70 | Loss=0.0008 | Val Acc=60.98% | F1=0.758 | AUC=0.452\n",
      "[Arousal (Fold 5)] Ep 37/70 | Loss=0.0003 | Val Acc=65.85% | F1=0.781 | AUC=0.455\n",
      "[Arousal (Fold 5)] Ep 38/70 | Loss=0.0007 | Val Acc=60.98% | F1=0.758 | AUC=0.448\n",
      "[Arousal (Fold 5)] Ep 39/70 | Loss=0.0003 | Val Acc=60.98% | F1=0.758 | AUC=0.436\n",
      "[Arousal (Fold 5)] Ep 40/70 | Loss=0.0004 | Val Acc=60.98% | F1=0.758 | AUC=0.445\n",
      "[Arousal (Fold 5)] Ep 41/70 | Loss=0.0002 | Val Acc=63.41% | F1=0.769 | AUC=0.430\n",
      "[Arousal (Fold 5)] Ep 42/70 | Loss=0.0019 | Val Acc=68.29% | F1=0.812 | AUC=0.395\n",
      "[Arousal (Fold 5)] Ep 43/70 | Loss=0.0288 | Val Acc=51.22% | F1=0.630 | AUC=0.461\n",
      "[Arousal (Fold 5)] Ep 44/70 | Loss=0.0363 | Val Acc=56.10% | F1=0.654 | AUC=0.455\n",
      "â¹ [Arousal (Fold 5)] Early stopping at epoch 44\n",
      "\n",
      "ðŸ”š [Arousal (Fold 5)] TEST (best thr=0.100) | Acc=51.22% | F1=0.615 | AUC=0.467\n",
      "\n",
      "Fold 5 Results: Acc=0.5122, F1=0.6154, AUC=0.4667\n",
      "\n",
      "----- Arousal Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 6) | epochs=70\n",
      "[Arousal (Fold 6)] Ep 01/70 | Loss=0.3840 | Val Acc=68.29% | F1=0.755 | AUC=0.791\n",
      "[Arousal (Fold 6)] Ep 02/70 | Loss=0.3752 | Val Acc=41.46% | F1=0.333 | AUC=0.764\n",
      "[Arousal (Fold 6)] Ep 03/70 | Loss=0.3497 | Val Acc=68.29% | F1=0.787 | AUC=0.694\n",
      "[Arousal (Fold 6)] Ep 04/70 | Loss=0.3276 | Val Acc=68.29% | F1=0.772 | AUC=0.682\n",
      "[Arousal (Fold 6)] Ep 05/70 | Loss=0.2857 | Val Acc=70.73% | F1=0.824 | AUC=0.603\n",
      "[Arousal (Fold 6)] Ep 06/70 | Loss=0.2054 | Val Acc=58.54% | F1=0.702 | AUC=0.588\n",
      "[Arousal (Fold 6)] Ep 07/70 | Loss=0.1475 | Val Acc=70.73% | F1=0.824 | AUC=0.558\n",
      "[Arousal (Fold 6)] Ep 08/70 | Loss=0.0971 | Val Acc=56.10% | F1=0.700 | AUC=0.548\n",
      "[Arousal (Fold 6)] Ep 09/70 | Loss=0.0724 | Val Acc=58.54% | F1=0.679 | AUC=0.512\n",
      "[Arousal (Fold 6)] Ep 10/70 | Loss=0.0439 | Val Acc=53.66% | F1=0.667 | AUC=0.591\n",
      "[Arousal (Fold 6)] Ep 11/70 | Loss=0.0293 | Val Acc=60.98% | F1=0.724 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 12/70 | Loss=0.0243 | Val Acc=58.54% | F1=0.702 | AUC=0.582\n",
      "[Arousal (Fold 6)] Ep 13/70 | Loss=0.0143 | Val Acc=60.98% | F1=0.742 | AUC=0.545\n",
      "[Arousal (Fold 6)] Ep 14/70 | Loss=0.0131 | Val Acc=65.85% | F1=0.788 | AUC=0.524\n",
      "[Arousal (Fold 6)] Ep 15/70 | Loss=0.0101 | Val Acc=68.29% | F1=0.806 | AUC=0.518\n",
      "[Arousal (Fold 6)] Ep 16/70 | Loss=0.0097 | Val Acc=68.29% | F1=0.806 | AUC=0.542\n",
      "[Arousal (Fold 6)] Ep 17/70 | Loss=0.0065 | Val Acc=65.85% | F1=0.781 | AUC=0.545\n",
      "[Arousal (Fold 6)] Ep 18/70 | Loss=0.0065 | Val Acc=65.85% | F1=0.781 | AUC=0.527\n",
      "[Arousal (Fold 6)] Ep 19/70 | Loss=0.0065 | Val Acc=63.41% | F1=0.762 | AUC=0.548\n",
      "[Arousal (Fold 6)] Ep 20/70 | Loss=0.0063 | Val Acc=63.41% | F1=0.762 | AUC=0.530\n",
      "[Arousal (Fold 6)] Ep 21/70 | Loss=0.0213 | Val Acc=65.85% | F1=0.750 | AUC=0.558\n",
      "[Arousal (Fold 6)] Ep 22/70 | Loss=0.0700 | Val Acc=73.17% | F1=0.845 | AUC=0.555\n",
      "[Arousal (Fold 6)] Ep 23/70 | Loss=0.0210 | Val Acc=70.73% | F1=0.812 | AUC=0.642\n",
      "[Arousal (Fold 6)] Ep 24/70 | Loss=0.0211 | Val Acc=63.41% | F1=0.762 | AUC=0.561\n",
      "[Arousal (Fold 6)] Ep 25/70 | Loss=0.0375 | Val Acc=60.98% | F1=0.714 | AUC=0.636\n",
      "[Arousal (Fold 6)] Ep 26/70 | Loss=0.0105 | Val Acc=63.41% | F1=0.762 | AUC=0.542\n",
      "[Arousal (Fold 6)] Ep 27/70 | Loss=0.0063 | Val Acc=65.85% | F1=0.781 | AUC=0.624\n",
      "[Arousal (Fold 6)] Ep 28/70 | Loss=0.0098 | Val Acc=63.41% | F1=0.754 | AUC=0.636\n",
      "[Arousal (Fold 6)] Ep 29/70 | Loss=0.0063 | Val Acc=70.73% | F1=0.818 | AUC=0.652\n",
      "[Arousal (Fold 6)] Ep 30/70 | Loss=0.0074 | Val Acc=70.73% | F1=0.824 | AUC=0.521\n",
      "[Arousal (Fold 6)] Ep 31/70 | Loss=0.0015 | Val Acc=70.73% | F1=0.824 | AUC=0.458\n",
      "[Arousal (Fold 6)] Ep 32/70 | Loss=0.0008 | Val Acc=60.98% | F1=0.750 | AUC=0.539\n",
      "[Arousal (Fold 6)] Ep 33/70 | Loss=0.0020 | Val Acc=68.29% | F1=0.806 | AUC=0.433\n",
      "[Arousal (Fold 6)] Ep 34/70 | Loss=0.0007 | Val Acc=63.41% | F1=0.762 | AUC=0.445\n",
      "[Arousal (Fold 6)] Ep 35/70 | Loss=0.0009 | Val Acc=60.98% | F1=0.750 | AUC=0.461\n",
      "[Arousal (Fold 6)] Ep 36/70 | Loss=0.0007 | Val Acc=65.85% | F1=0.788 | AUC=0.503\n",
      "[Arousal (Fold 6)] Ep 37/70 | Loss=0.0006 | Val Acc=65.85% | F1=0.781 | AUC=0.527\n",
      "[Arousal (Fold 6)] Ep 38/70 | Loss=0.0005 | Val Acc=68.29% | F1=0.800 | AUC=0.533\n",
      "[Arousal (Fold 6)] Ep 39/70 | Loss=0.0005 | Val Acc=68.29% | F1=0.800 | AUC=0.530\n",
      "[Arousal (Fold 6)] Ep 40/70 | Loss=0.0007 | Val Acc=68.29% | F1=0.800 | AUC=0.527\n",
      "â¹ [Arousal (Fold 6)] Early stopping at epoch 40\n",
      "\n",
      "ðŸ”š [Arousal (Fold 6)] TEST (best thr=0.100) | Acc=73.17% | F1=0.825 | AUC=0.682\n",
      "\n",
      "Fold 6 Results: Acc=0.7317, F1=0.8254, AUC=0.6818\n",
      "\n",
      "----- Arousal Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 7) | epochs=70\n",
      "[Arousal (Fold 7)] Ep 01/70 | Loss=0.3863 | Val Acc=73.17% | F1=0.831 | AUC=0.667\n",
      "[Arousal (Fold 7)] Ep 02/70 | Loss=0.3712 | Val Acc=56.10% | F1=0.571 | AUC=0.642\n",
      "[Arousal (Fold 7)] Ep 03/70 | Loss=0.3540 | Val Acc=60.98% | F1=0.692 | AUC=0.597\n",
      "[Arousal (Fold 7)] Ep 04/70 | Loss=0.3272 | Val Acc=41.46% | F1=0.429 | AUC=0.570\n",
      "[Arousal (Fold 7)] Ep 05/70 | Loss=0.2734 | Val Acc=46.34% | F1=0.476 | AUC=0.558\n",
      "[Arousal (Fold 7)] Ep 06/70 | Loss=0.1962 | Val Acc=56.10% | F1=0.654 | AUC=0.645\n",
      "[Arousal (Fold 7)] Ep 07/70 | Loss=0.1102 | Val Acc=73.17% | F1=0.831 | AUC=0.697\n",
      "[Arousal (Fold 7)] Ep 08/70 | Loss=0.0659 | Val Acc=68.29% | F1=0.755 | AUC=0.688\n",
      "[Arousal (Fold 7)] Ep 09/70 | Loss=0.0505 | Val Acc=60.98% | F1=0.680 | AUC=0.630\n",
      "[Arousal (Fold 7)] Ep 10/70 | Loss=0.0278 | Val Acc=73.17% | F1=0.836 | AUC=0.685\n",
      "[Arousal (Fold 7)] Ep 11/70 | Loss=0.0110 | Val Acc=73.17% | F1=0.820 | AUC=0.718\n",
      "[Arousal (Fold 7)] Ep 12/70 | Loss=0.0103 | Val Acc=73.17% | F1=0.836 | AUC=0.773\n",
      "[Arousal (Fold 7)] Ep 13/70 | Loss=0.0093 | Val Acc=78.05% | F1=0.857 | AUC=0.724\n",
      "[Arousal (Fold 7)] Ep 14/70 | Loss=0.0066 | Val Acc=78.05% | F1=0.862 | AUC=0.727\n",
      "[Arousal (Fold 7)] Ep 15/70 | Loss=0.0050 | Val Acc=78.05% | F1=0.852 | AUC=0.697\n",
      "[Arousal (Fold 7)] Ep 16/70 | Loss=0.0040 | Val Acc=75.61% | F1=0.844 | AUC=0.712\n",
      "[Arousal (Fold 7)] Ep 17/70 | Loss=0.0034 | Val Acc=80.49% | F1=0.875 | AUC=0.736\n",
      "[Arousal (Fold 7)] Ep 18/70 | Loss=0.0033 | Val Acc=78.05% | F1=0.862 | AUC=0.736\n",
      "[Arousal (Fold 7)] Ep 19/70 | Loss=0.0034 | Val Acc=75.61% | F1=0.848 | AUC=0.724\n",
      "[Arousal (Fold 7)] Ep 20/70 | Loss=0.0037 | Val Acc=75.61% | F1=0.848 | AUC=0.724\n",
      "[Arousal (Fold 7)] Ep 21/70 | Loss=0.0153 | Val Acc=68.29% | F1=0.806 | AUC=0.715\n",
      "[Arousal (Fold 7)] Ep 22/70 | Loss=0.0415 | Val Acc=73.17% | F1=0.836 | AUC=0.694\n",
      "[Arousal (Fold 7)] Ep 23/70 | Loss=0.0412 | Val Acc=63.41% | F1=0.737 | AUC=0.624\n",
      "[Arousal (Fold 7)] Ep 24/70 | Loss=0.0358 | Val Acc=60.98% | F1=0.714 | AUC=0.603\n",
      "[Arousal (Fold 7)] Ep 25/70 | Loss=0.0318 | Val Acc=75.61% | F1=0.853 | AUC=0.603\n",
      "[Arousal (Fold 7)] Ep 26/70 | Loss=0.0126 | Val Acc=68.29% | F1=0.812 | AUC=0.612\n",
      "[Arousal (Fold 7)] Ep 27/70 | Loss=0.0042 | Val Acc=75.61% | F1=0.848 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 28/70 | Loss=0.0027 | Val Acc=68.29% | F1=0.800 | AUC=0.636\n",
      "[Arousal (Fold 7)] Ep 29/70 | Loss=0.0024 | Val Acc=75.61% | F1=0.848 | AUC=0.670\n",
      "[Arousal (Fold 7)] Ep 30/70 | Loss=0.0011 | Val Acc=70.73% | F1=0.818 | AUC=0.697\n",
      "[Arousal (Fold 7)] Ep 31/70 | Loss=0.0017 | Val Acc=78.05% | F1=0.866 | AUC=0.624\n",
      "[Arousal (Fold 7)] Ep 32/70 | Loss=0.0014 | Val Acc=75.61% | F1=0.848 | AUC=0.694\n",
      "[Arousal (Fold 7)] Ep 33/70 | Loss=0.0010 | Val Acc=73.17% | F1=0.841 | AUC=0.667\n",
      "[Arousal (Fold 7)] Ep 34/70 | Loss=0.0021 | Val Acc=73.17% | F1=0.836 | AUC=0.709\n",
      "[Arousal (Fold 7)] Ep 35/70 | Loss=0.0008 | Val Acc=70.73% | F1=0.818 | AUC=0.691\n",
      "â¹ [Arousal (Fold 7)] Early stopping at epoch 35\n",
      "\n",
      "ðŸ”š [Arousal (Fold 7)] TEST (best thr=0.100) | Acc=63.41% | F1=0.776 | AUC=0.439\n",
      "\n",
      "Fold 7 Results: Acc=0.6341, F1=0.7761, AUC=0.4394\n",
      "\n",
      "----- Arousal Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 8) | epochs=70\n",
      "[Arousal (Fold 8)] Ep 01/70 | Loss=0.3833 | Val Acc=26.83% | F1=0.118 | AUC=0.482\n",
      "[Arousal (Fold 8)] Ep 02/70 | Loss=0.3717 | Val Acc=34.15% | F1=0.308 | AUC=0.521\n",
      "[Arousal (Fold 8)] Ep 03/70 | Loss=0.3504 | Val Acc=36.59% | F1=0.278 | AUC=0.497\n",
      "[Arousal (Fold 8)] Ep 04/70 | Loss=0.3274 | Val Acc=43.90% | F1=0.489 | AUC=0.488\n",
      "[Arousal (Fold 8)] Ep 05/70 | Loss=0.2791 | Val Acc=46.34% | F1=0.476 | AUC=0.518\n",
      "[Arousal (Fold 8)] Ep 06/70 | Loss=0.2111 | Val Acc=73.17% | F1=0.841 | AUC=0.606\n",
      "[Arousal (Fold 8)] Ep 07/70 | Loss=0.1395 | Val Acc=58.54% | F1=0.653 | AUC=0.609\n",
      "[Arousal (Fold 8)] Ep 08/70 | Loss=0.0759 | Val Acc=58.54% | F1=0.653 | AUC=0.606\n",
      "[Arousal (Fold 8)] Ep 09/70 | Loss=0.0469 | Val Acc=68.29% | F1=0.787 | AUC=0.688\n",
      "[Arousal (Fold 8)] Ep 10/70 | Loss=0.0306 | Val Acc=58.54% | F1=0.638 | AUC=0.685\n",
      "[Arousal (Fold 8)] Ep 11/70 | Loss=0.0193 | Val Acc=65.85% | F1=0.759 | AUC=0.706\n",
      "[Arousal (Fold 8)] Ep 12/70 | Loss=0.0129 | Val Acc=63.41% | F1=0.737 | AUC=0.606\n",
      "[Arousal (Fold 8)] Ep 13/70 | Loss=0.0093 | Val Acc=63.41% | F1=0.737 | AUC=0.664\n",
      "[Arousal (Fold 8)] Ep 14/70 | Loss=0.0064 | Val Acc=65.85% | F1=0.759 | AUC=0.591\n",
      "[Arousal (Fold 8)] Ep 15/70 | Loss=0.0054 | Val Acc=65.85% | F1=0.767 | AUC=0.658\n",
      "[Arousal (Fold 8)] Ep 16/70 | Loss=0.0045 | Val Acc=68.29% | F1=0.787 | AUC=0.679\n",
      "[Arousal (Fold 8)] Ep 17/70 | Loss=0.0039 | Val Acc=63.41% | F1=0.746 | AUC=0.670\n",
      "[Arousal (Fold 8)] Ep 18/70 | Loss=0.0042 | Val Acc=63.41% | F1=0.746 | AUC=0.676\n",
      "[Arousal (Fold 8)] Ep 19/70 | Loss=0.0064 | Val Acc=65.85% | F1=0.767 | AUC=0.664\n",
      "[Arousal (Fold 8)] Ep 20/70 | Loss=0.0048 | Val Acc=63.41% | F1=0.746 | AUC=0.655\n",
      "[Arousal (Fold 8)] Ep 21/70 | Loss=0.0132 | Val Acc=39.02% | F1=0.324 | AUC=0.576\n",
      "[Arousal (Fold 8)] Ep 22/70 | Loss=0.0425 | Val Acc=53.66% | F1=0.612 | AUC=0.573\n",
      "[Arousal (Fold 8)] Ep 23/70 | Loss=0.0295 | Val Acc=51.22% | F1=0.630 | AUC=0.518\n",
      "[Arousal (Fold 8)] Ep 24/70 | Loss=0.0314 | Val Acc=58.54% | F1=0.712 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 25/70 | Loss=0.0131 | Val Acc=70.73% | F1=0.818 | AUC=0.697\n",
      "[Arousal (Fold 8)] Ep 26/70 | Loss=0.0108 | Val Acc=65.85% | F1=0.774 | AUC=0.636\n",
      "[Arousal (Fold 8)] Ep 27/70 | Loss=0.0087 | Val Acc=60.98% | F1=0.714 | AUC=0.539\n",
      "[Arousal (Fold 8)] Ep 28/70 | Loss=0.0096 | Val Acc=70.73% | F1=0.800 | AUC=0.600\n",
      "[Arousal (Fold 8)] Ep 29/70 | Loss=0.0037 | Val Acc=78.05% | F1=0.862 | AUC=0.688\n",
      "[Arousal (Fold 8)] Ep 30/70 | Loss=0.0007 | Val Acc=73.17% | F1=0.825 | AUC=0.636\n",
      "[Arousal (Fold 8)] Ep 31/70 | Loss=0.0007 | Val Acc=68.29% | F1=0.772 | AUC=0.642\n",
      "[Arousal (Fold 8)] Ep 32/70 | Loss=0.0005 | Val Acc=73.17% | F1=0.825 | AUC=0.642\n",
      "[Arousal (Fold 8)] Ep 33/70 | Loss=0.0003 | Val Acc=65.85% | F1=0.767 | AUC=0.627\n",
      "[Arousal (Fold 8)] Ep 34/70 | Loss=0.0007 | Val Acc=65.85% | F1=0.759 | AUC=0.630\n",
      "[Arousal (Fold 8)] Ep 35/70 | Loss=0.0005 | Val Acc=70.73% | F1=0.800 | AUC=0.630\n",
      "[Arousal (Fold 8)] Ep 36/70 | Loss=0.0013 | Val Acc=75.61% | F1=0.839 | AUC=0.615\n",
      "[Arousal (Fold 8)] Ep 37/70 | Loss=0.0005 | Val Acc=70.73% | F1=0.800 | AUC=0.645\n",
      "[Arousal (Fold 8)] Ep 38/70 | Loss=0.0005 | Val Acc=73.17% | F1=0.820 | AUC=0.642\n",
      "[Arousal (Fold 8)] Ep 39/70 | Loss=0.0004 | Val Acc=70.73% | F1=0.800 | AUC=0.655\n",
      "[Arousal (Fold 8)] Ep 40/70 | Loss=0.0004 | Val Acc=70.73% | F1=0.800 | AUC=0.633\n",
      "[Arousal (Fold 8)] Ep 41/70 | Loss=0.0003 | Val Acc=73.17% | F1=0.820 | AUC=0.614\n",
      "[Arousal (Fold 8)] Ep 42/70 | Loss=0.0009 | Val Acc=68.29% | F1=0.780 | AUC=0.645\n",
      "[Arousal (Fold 8)] Ep 43/70 | Loss=0.0069 | Val Acc=73.17% | F1=0.825 | AUC=0.600\n",
      "[Arousal (Fold 8)] Ep 44/70 | Loss=0.0138 | Val Acc=68.29% | F1=0.794 | AUC=0.652\n",
      "[Arousal (Fold 8)] Ep 45/70 | Loss=0.0572 | Val Acc=70.73% | F1=0.806 | AUC=0.576\n",
      "[Arousal (Fold 8)] Ep 46/70 | Loss=0.0170 | Val Acc=60.98% | F1=0.733 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 47/70 | Loss=0.0049 | Val Acc=63.41% | F1=0.737 | AUC=0.573\n",
      "â¹ [Arousal (Fold 8)] Early stopping at epoch 47\n",
      "\n",
      "ðŸ”š [Arousal (Fold 8)] TEST (best thr=0.100) | Acc=68.29% | F1=0.794 | AUC=0.585\n",
      "\n",
      "Fold 8 Results: Acc=0.6829, F1=0.7937, AUC=0.5848\n",
      "\n",
      "----- Arousal Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 9) | epochs=70\n",
      "[Arousal (Fold 9)] Ep 01/70 | Loss=0.3825 | Val Acc=70.73% | F1=0.800 | AUC=0.615\n",
      "[Arousal (Fold 9)] Ep 02/70 | Loss=0.3645 | Val Acc=56.10% | F1=0.625 | AUC=0.633\n",
      "[Arousal (Fold 9)] Ep 03/70 | Loss=0.3448 | Val Acc=43.90% | F1=0.378 | AUC=0.630\n",
      "[Arousal (Fold 9)] Ep 04/70 | Loss=0.3070 | Val Acc=70.73% | F1=0.812 | AUC=0.639\n",
      "[Arousal (Fold 9)] Ep 05/70 | Loss=0.2468 | Val Acc=68.29% | F1=0.780 | AUC=0.573\n",
      "[Arousal (Fold 9)] Ep 06/70 | Loss=0.1623 | Val Acc=53.66% | F1=0.642 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 07/70 | Loss=0.1011 | Val Acc=51.22% | F1=0.524 | AUC=0.570\n",
      "[Arousal (Fold 9)] Ep 08/70 | Loss=0.0516 | Val Acc=60.98% | F1=0.724 | AUC=0.636\n",
      "[Arousal (Fold 9)] Ep 09/70 | Loss=0.0606 | Val Acc=65.85% | F1=0.781 | AUC=0.530\n",
      "[Arousal (Fold 9)] Ep 10/70 | Loss=0.0213 | Val Acc=68.29% | F1=0.812 | AUC=0.591\n",
      "[Arousal (Fold 9)] Ep 11/70 | Loss=0.0177 | Val Acc=63.41% | F1=0.769 | AUC=0.527\n",
      "[Arousal (Fold 9)] Ep 12/70 | Loss=0.0107 | Val Acc=68.29% | F1=0.812 | AUC=0.561\n",
      "[Arousal (Fold 9)] Ep 13/70 | Loss=0.0062 | Val Acc=70.73% | F1=0.824 | AUC=0.552\n",
      "[Arousal (Fold 9)] Ep 14/70 | Loss=0.0081 | Val Acc=75.61% | F1=0.848 | AUC=0.585\n",
      "[Arousal (Fold 9)] Ep 15/70 | Loss=0.0060 | Val Acc=73.17% | F1=0.831 | AUC=0.579\n",
      "[Arousal (Fold 9)] Ep 16/70 | Loss=0.0049 | Val Acc=73.17% | F1=0.836 | AUC=0.545\n",
      "[Arousal (Fold 9)] Ep 17/70 | Loss=0.0036 | Val Acc=73.17% | F1=0.836 | AUC=0.539\n",
      "[Arousal (Fold 9)] Ep 18/70 | Loss=0.0036 | Val Acc=73.17% | F1=0.836 | AUC=0.552\n",
      "[Arousal (Fold 9)] Ep 19/70 | Loss=0.0041 | Val Acc=70.73% | F1=0.824 | AUC=0.552\n",
      "[Arousal (Fold 9)] Ep 20/70 | Loss=0.0036 | Val Acc=73.17% | F1=0.836 | AUC=0.542\n",
      "[Arousal (Fold 9)] Ep 21/70 | Loss=0.0036 | Val Acc=48.78% | F1=0.553 | AUC=0.561\n",
      "[Arousal (Fold 9)] Ep 22/70 | Loss=0.0494 | Val Acc=39.02% | F1=0.359 | AUC=0.503\n",
      "[Arousal (Fold 9)] Ep 23/70 | Loss=0.0383 | Val Acc=70.73% | F1=0.829 | AUC=0.452\n",
      "[Arousal (Fold 9)] Ep 24/70 | Loss=0.0138 | Val Acc=65.85% | F1=0.794 | AUC=0.424\n",
      "[Arousal (Fold 9)] Ep 25/70 | Loss=0.0147 | Val Acc=68.29% | F1=0.800 | AUC=0.561\n",
      "[Arousal (Fold 9)] Ep 26/70 | Loss=0.0242 | Val Acc=73.17% | F1=0.841 | AUC=0.567\n",
      "[Arousal (Fold 9)] Ep 27/70 | Loss=0.0073 | Val Acc=68.29% | F1=0.812 | AUC=0.452\n",
      "[Arousal (Fold 9)] Ep 28/70 | Loss=0.0032 | Val Acc=70.73% | F1=0.812 | AUC=0.558\n",
      "[Arousal (Fold 9)] Ep 29/70 | Loss=0.0024 | Val Acc=65.85% | F1=0.781 | AUC=0.470\n",
      "[Arousal (Fold 9)] Ep 30/70 | Loss=0.0024 | Val Acc=63.41% | F1=0.769 | AUC=0.506\n",
      "[Arousal (Fold 9)] Ep 31/70 | Loss=0.0006 | Val Acc=63.41% | F1=0.769 | AUC=0.461\n",
      "[Arousal (Fold 9)] Ep 32/70 | Loss=0.0009 | Val Acc=68.29% | F1=0.812 | AUC=0.412\n",
      "â¹ [Arousal (Fold 9)] Early stopping at epoch 32\n",
      "\n",
      "ðŸ”š [Arousal (Fold 9)] TEST (best thr=0.100) | Acc=65.85% | F1=0.794 | AUC=0.576\n",
      "\n",
      "Fold 9 Results: Acc=0.6585, F1=0.7941, AUC=0.5758\n",
      "\n",
      "----- Arousal Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 10) | epochs=70\n",
      "[Arousal (Fold 10)] Ep 01/70 | Loss=0.3802 | Val Acc=60.98% | F1=0.724 | AUC=0.645\n",
      "[Arousal (Fold 10)] Ep 02/70 | Loss=0.3617 | Val Acc=58.54% | F1=0.679 | AUC=0.642\n",
      "[Arousal (Fold 10)] Ep 03/70 | Loss=0.3358 | Val Acc=68.29% | F1=0.806 | AUC=0.633\n",
      "[Arousal (Fold 10)] Ep 04/70 | Loss=0.3015 | Val Acc=58.54% | F1=0.667 | AUC=0.603\n",
      "[Arousal (Fold 10)] Ep 05/70 | Loss=0.2385 | Val Acc=60.98% | F1=0.692 | AUC=0.627\n",
      "[Arousal (Fold 10)] Ep 06/70 | Loss=0.1688 | Val Acc=46.34% | F1=0.450 | AUC=0.652\n",
      "[Arousal (Fold 10)] Ep 07/70 | Loss=0.1112 | Val Acc=65.85% | F1=0.750 | AUC=0.594\n",
      "[Arousal (Fold 10)] Ep 08/70 | Loss=0.0773 | Val Acc=73.17% | F1=0.841 | AUC=0.588\n",
      "[Arousal (Fold 10)] Ep 09/70 | Loss=0.0347 | Val Acc=65.85% | F1=0.781 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 10/70 | Loss=0.0222 | Val Acc=70.73% | F1=0.818 | AUC=0.618\n",
      "[Arousal (Fold 10)] Ep 11/70 | Loss=0.0149 | Val Acc=73.17% | F1=0.841 | AUC=0.561\n",
      "[Arousal (Fold 10)] Ep 12/70 | Loss=0.0149 | Val Acc=60.98% | F1=0.742 | AUC=0.497\n",
      "[Arousal (Fold 10)] Ep 13/70 | Loss=0.0140 | Val Acc=68.29% | F1=0.800 | AUC=0.594\n",
      "[Arousal (Fold 10)] Ep 14/70 | Loss=0.0055 | Val Acc=60.98% | F1=0.733 | AUC=0.542\n",
      "[Arousal (Fold 10)] Ep 15/70 | Loss=0.0045 | Val Acc=63.41% | F1=0.762 | AUC=0.512\n",
      "[Arousal (Fold 10)] Ep 16/70 | Loss=0.0045 | Val Acc=63.41% | F1=0.762 | AUC=0.552\n",
      "[Arousal (Fold 10)] Ep 17/70 | Loss=0.0039 | Val Acc=63.41% | F1=0.762 | AUC=0.548\n",
      "[Arousal (Fold 10)] Ep 18/70 | Loss=0.0043 | Val Acc=60.98% | F1=0.742 | AUC=0.548\n",
      "[Arousal (Fold 10)] Ep 19/70 | Loss=0.0039 | Val Acc=63.41% | F1=0.762 | AUC=0.552\n",
      "[Arousal (Fold 10)] Ep 20/70 | Loss=0.0033 | Val Acc=65.85% | F1=0.781 | AUC=0.555\n",
      "[Arousal (Fold 10)] Ep 21/70 | Loss=0.0251 | Val Acc=65.85% | F1=0.794 | AUC=0.600\n",
      "[Arousal (Fold 10)] Ep 22/70 | Loss=0.0454 | Val Acc=65.85% | F1=0.781 | AUC=0.600\n",
      "[Arousal (Fold 10)] Ep 23/70 | Loss=0.0257 | Val Acc=63.41% | F1=0.746 | AUC=0.576\n",
      "[Arousal (Fold 10)] Ep 24/70 | Loss=0.0148 | Val Acc=60.98% | F1=0.742 | AUC=0.530\n",
      "[Arousal (Fold 10)] Ep 25/70 | Loss=0.0234 | Val Acc=58.54% | F1=0.712 | AUC=0.445\n",
      "[Arousal (Fold 10)] Ep 26/70 | Loss=0.0089 | Val Acc=65.85% | F1=0.774 | AUC=0.527\n",
      "[Arousal (Fold 10)] Ep 27/70 | Loss=0.0068 | Val Acc=63.41% | F1=0.762 | AUC=0.539\n",
      "[Arousal (Fold 10)] Ep 28/70 | Loss=0.0087 | Val Acc=60.98% | F1=0.742 | AUC=0.536\n",
      "[Arousal (Fold 10)] Ep 29/70 | Loss=0.0046 | Val Acc=58.54% | F1=0.712 | AUC=0.558\n",
      "[Arousal (Fold 10)] Ep 30/70 | Loss=0.0034 | Val Acc=60.98% | F1=0.733 | AUC=0.485\n",
      "[Arousal (Fold 10)] Ep 31/70 | Loss=0.0005 | Val Acc=63.41% | F1=0.762 | AUC=0.536\n",
      "â¹ [Arousal (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 10)] TEST (best thr=0.100) | Acc=70.73% | F1=0.818 | AUC=0.576\n",
      "\n",
      "Fold 10 Results: Acc=0.7073, F1=0.8182, AUC=0.5758\n",
      "\n",
      "===== FINAL Arousal 10-FOLD RESULTS =====\n",
      "Accuracy: 0.6546 Â± 0.0649\n",
      "F1-score: 0.7701 Â± 0.0640\n",
      "AUC:      0.5296 Â± 0.0851\n",
      "\n",
      "===== OVERALL SUMMARY =====\n",
      "Valence Acc: mean=0.6111, std=0.0599\n",
      "Arousal Acc: mean=0.6546, std=0.0649\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# CNN + BiLSTM \n",
    "\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------- DEVICE -------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Dataset wrapper (X already = spectrograms)\n",
    "# -------------------------------------------------\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.asarray(X, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]).float(),\n",
    "            torch.tensor(self.y[idx]).float(),\n",
    "        )\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Augmentations: SpecAugment + noise\n",
    "# -------------------------------------------------\n",
    "def spec_augment(\n",
    "    X,\n",
    "    time_mask_width=4,\n",
    "    n_time_masks=2,\n",
    "    freq_mask_width=6,\n",
    "    n_freq_masks=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    X: [N, C, F, T] spectrograms\n",
    "    Applies random time & frequency masks (SpecAugment style).\n",
    "    \"\"\"\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "\n",
    "    for i in range(N):\n",
    "        # Time masks\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T:\n",
    "                continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "\n",
    "        # Frequency masks\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F:\n",
    "                continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. CNN + BiLSTM model (time-aware)\n",
    "# -------------------------------------------------\n",
    "class EEG_CNN_BiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN extracts [B, 128, F', T'] features.\n",
    "    We then average over frequency â†’ [B, 128, T'] and\n",
    "    feed [B, T', 128] into a BiLSTM (temporal sequence).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=14, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),   # 36Ã—32 â†’ 18Ã—16\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,      # features per time step\n",
    "            hidden_size=64,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(2 * 64),\n",
    "            nn.Linear(2 * 64, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, F, T]\n",
    "        x = self.cnn(x)             # [B, 128, F', T']\n",
    "        # average over frequency â†’ pure temporal tokens\n",
    "        x = x.mean(dim=2)           # [B, 128, T']\n",
    "        x = x.permute(0, 2, 1)      # [B, T', 128]\n",
    "\n",
    "        # BiLSTM over time dimension\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # last forward + last backward hidden\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)  # [B, 2*hidden_size]\n",
    "        out = self.fc(h_cat).squeeze(-1)             # [B]\n",
    "        return out\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Train ONE fold for one emotion\n",
    "# -------------------------------------------------\n",
    "def train_one_fold_emotion(\n",
    "    X,\n",
    "    y_cont,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    emotion_name=\"Valence\",\n",
    "    epochs=70,\n",
    "    base_seed=42,\n",
    "    batch_size=16,\n",
    "):\n",
    "    # 1) Binarize labels w.r.t. TRAIN median\n",
    "    y_train_cont = y_cont[train_idx]\n",
    "    thr = np.median(y_train_cont)\n",
    "\n",
    "    y_train_bin = (y_train_cont >= thr).astype(float)\n",
    "    y_val_bin   = (y_cont[val_idx]  >= thr).astype(float)\n",
    "    y_test_bin  = (y_cont[test_idx] >= thr).astype(float)\n",
    "\n",
    "    print(f\"\\n[{emotion_name}] TRAIN median threshold = {thr:.4f}\")\n",
    "    print(\"  Train class counts:\", np.bincount(y_train_bin.astype(int)))\n",
    "    print(\"  Val   class counts:\", np.bincount(y_val_bin.astype(int)))\n",
    "    print(\"  Test  class counts:\", np.bincount(y_test_bin.astype(int)))\n",
    "\n",
    "    # 2) Standardize features using TRAIN only\n",
    "    X = np.nan_to_num(X)\n",
    "    X_train = X[train_idx]\n",
    "    X_val   = X[val_idx]\n",
    "    X_test  = X[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(len(train_idx), -1)\n",
    "    scaler.fit(X_train_flat)\n",
    "\n",
    "    def transform_block(X_block):\n",
    "        flat = X_block.reshape(X_block.shape[0], -1)\n",
    "        flat_scaled = scaler.transform(flat)\n",
    "        return flat_scaled.reshape(X_block.shape)\n",
    "\n",
    "    X_train_scaled = transform_block(X_train)\n",
    "    X_val_scaled   = transform_block(X_val)\n",
    "    X_test_scaled  = transform_block(X_test)\n",
    "\n",
    "    # 3) Strong augmentation: SpecAugment + noise\n",
    "    X_train_sa    = spec_augment(X_train_scaled)\n",
    "    X_train_noise = add_noise(X_train_scaled, std=0.03)\n",
    "\n",
    "    X_train_aug = np.concatenate(\n",
    "        [X_train_scaled, X_train_sa, X_train_noise],\n",
    "        axis=0,\n",
    "    )\n",
    "    y_train_aug = np.concatenate(\n",
    "        [y_train_bin, y_train_bin, y_train_bin],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # 4) DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        EEGDataset(X_train_aug, y_train_aug),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        EEGDataset(X_val_scaled, y_val_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        EEGDataset(X_test_scaled, y_test_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # 5) Model + optimizer + CLASS-WEIGHTED loss\n",
    "    torch.manual_seed(base_seed)\n",
    "    random.seed(base_seed)\n",
    "    np.random.seed(base_seed)\n",
    "\n",
    "    n_channels = X.shape[1]\n",
    "    model = EEG_CNN_BiLSTM(n_channels=n_channels, hidden_size=64).to(DEVICE)\n",
    "\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts) > 0 else 1\n",
    "    pos = class_counts[1] if len(class_counts) > 1 else 1\n",
    "    pos_weight_val = float(neg) / float(pos) if pos > 0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    print(f\"[{emotion_name}] pos_weight for BCEWithLogitsLoss = {pos_weight_val:.3f}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "    patience = 18\n",
    "    counter = 0\n",
    "\n",
    "    print(f\"\\nðŸš€ Training {emotion_name} | epochs={epochs}\")\n",
    "    for ep in range(1, epochs + 1):\n",
    "        # ---- train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            n_samples += xb.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss = total_loss / max(1, n_samples)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        y_true_val, y_prob_val = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy())\n",
    "                y_prob_val.extend(probs)\n",
    "\n",
    "        y_true_val = np.array(y_true_val)\n",
    "        y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        val_f1  = f1_score(y_true_val, y_pred_val)\n",
    "        try:\n",
    "            val_auc = roc_auc_score(y_true_val, y_prob_val)\n",
    "        except:\n",
    "            val_auc = float(\"nan\")\n",
    "\n",
    "        print(\n",
    "            f\"[{emotion_name}] Ep {ep:02d}/{epochs} | \"\n",
    "            f\"Loss={train_loss:.4f} | Val Acc={val_acc*100:.2f}% | \"\n",
    "            f\"F1={val_f1:.3f} | AUC={val_auc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # early stopping on val accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience:\n",
    "                print(f\"â¹ [{emotion_name}] Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    # ---- load best & evaluate on TEST (with best threshold) ----\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    y_true_test, y_prob_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy())\n",
    "            y_prob_test.extend(probs)\n",
    "\n",
    "    y_true_test = np.array(y_true_test)\n",
    "    y_prob_test = np.array(y_prob_test)\n",
    "\n",
    "    # search threshold for best TEST accuracy\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for thr_ in np.linspace(0.1, 0.9, 17):\n",
    "        yp = (y_prob_test >= thr_).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc = acc_thr\n",
    "            best_thr = thr_\n",
    "\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1  = f1_score(y_true_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nðŸ”š [{emotion_name}] TEST (best thr={best_thr:.3f}) \"\n",
    "        f\"| Acc={test_acc*100:.2f}% | F1={test_f1:.3f} | AUC={test_auc:.3f}\\n\"\n",
    "    )\n",
    "\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. 10-fold CV driver\n",
    "# -------------------------------------------------\n",
    "def run_10fold_cv_emotion(X, y_cont, emotion_name=\"Valence\", epochs=70, base_seed=42):\n",
    "    \"\"\"\n",
    "    10-fold stratified CV:\n",
    "      - Outer fold chooses TEST.\n",
    "      - Remaining data is split into TRAIN / VAL\n",
    "        so that VAL size â‰ˆ TEST size.\n",
    "    \"\"\"\n",
    "    print(f\"\\n########## {emotion_name}: 10-fold STRATIFIED CV ##########\")\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "\n",
    "    # For stratification: global median threshold\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    print(f\"{emotion_name} global median (for stratification) = {global_thr:.4f}\")\n",
    "    print(\"Class counts:\", np.bincount(y_bin_global.astype(int)))\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=10,\n",
    "        shuffle=True,\n",
    "        random_state=base_seed,\n",
    "    )\n",
    "\n",
    "    fold_accs, fold_f1s, fold_aucs = [], [], []\n",
    "\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(\n",
    "        skf.split(np.arange(len(y_cont)), y_bin_global),\n",
    "        start=1\n",
    "    ):\n",
    "        # Split trainval into train & val with val size ~= test size\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "\n",
    "        tv_train_idx, tv_val_idx = train_test_split(\n",
    "            trainval_idx,\n",
    "            test_size=val_frac,\n",
    "            random_state=base_seed + fold,\n",
    "            shuffle=True,\n",
    "            stratify=y_trainval_bin,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n----- {emotion_name} Fold {fold}/10 -----\")\n",
    "        print(\n",
    "            f\"Train size={len(tv_train_idx)}, \"\n",
    "            f\"Val size={len(tv_val_idx)}, \"\n",
    "            f\"Test size={len(test_idx)}\"\n",
    "        )\n",
    "\n",
    "        acc, f1, auc = train_one_fold_emotion(\n",
    "            X,\n",
    "            y_cont,\n",
    "            tv_train_idx,\n",
    "            tv_val_idx,\n",
    "            test_idx,\n",
    "            emotion_name=f\"{emotion_name} (Fold {fold})\",\n",
    "            epochs=epochs,\n",
    "            base_seed=base_seed + fold,\n",
    "            batch_size=16,\n",
    "        )\n",
    "\n",
    "        fold_accs.append(acc)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_aucs.append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold} Results: \"\n",
    "            f\"Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\"\n",
    "        )\n",
    "\n",
    "    fold_accs = np.array(fold_accs)\n",
    "    fold_f1s  = np.array(fold_f1s)\n",
    "    fold_aucs = np.array(fold_aucs)\n",
    "\n",
    "    print(f\"\\n===== FINAL {emotion_name} 10-FOLD RESULTS =====\")\n",
    "    print(f\"Accuracy: {fold_accs.mean():.4f} Â± {fold_accs.std():.4f}\")\n",
    "    print(f\"F1-score: {fold_f1s.mean():.4f} Â± {fold_f1s.std():.4f}\")\n",
    "    print(f\"AUC:      {fold_aucs.mean():.4f} Â± {fold_aucs.std():.4f}\")\n",
    "\n",
    "    return fold_accs, fold_f1s, fold_aucs\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. MAIN (assumes X, y_val, y_aro already created)\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure you've already run the cell that does:\n",
    "    # X, y_val, y_aro, groups, channels = load_dreamer_and_build_spectrograms(...)\n",
    "    print(\"\\nâœ… Spectrogram dataset:\", X.shape)\n",
    "\n",
    "    # Valence\n",
    "    val_accs, val_f1s, val_aucs = run_10fold_cv_emotion(\n",
    "        X, y_val, emotion_name=\"Valence\", epochs=70, base_seed=42\n",
    "    )\n",
    "\n",
    "    # Arousal\n",
    "    aro_accs, aro_f1s, aro_aucs = run_10fold_cv_emotion(\n",
    "        X, y_aro, emotion_name=\"Arousal\", epochs=70, base_seed=142\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== OVERALL SUMMARY =====\")\n",
    "    print(f\"Valence Acc: mean={val_accs.mean():.4f}, std={val_accs.std():.4f}\")\n",
    "    print(f\"Arousal Acc: mean={aro_accs.mean():.4f}, std={aro_accs.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f44b507-552a-4283-aadd-351ea710aa8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "âœ… Spectrogram dataset: (414, 14, 36, 32)\n",
      "\n",
      "########## Valence: 10-fold STRATIFIED CV ##########\n",
      "Valence global median (for stratification) = 3.0000\n",
      "Class counts: [161 253]\n",
      "\n",
      "----- Valence Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 1) | epochs=70\n",
      "[Valence (Fold 1)] Ep 01/70 | Loss=0.5360 | Val Acc=61.90% | F1=0.704 | AUC=0.656\n",
      "[Valence (Fold 1)] Ep 02/70 | Loss=0.5138 | Val Acc=59.52% | F1=0.667 | AUC=0.661\n",
      "[Valence (Fold 1)] Ep 03/70 | Loss=0.4827 | Val Acc=59.52% | F1=0.679 | AUC=0.584\n",
      "[Valence (Fold 1)] Ep 04/70 | Loss=0.4544 | Val Acc=47.62% | F1=0.312 | AUC=0.514\n",
      "[Valence (Fold 1)] Ep 05/70 | Loss=0.3688 | Val Acc=47.62% | F1=0.353 | AUC=0.512\n",
      "[Valence (Fold 1)] Ep 06/70 | Loss=0.3359 | Val Acc=61.90% | F1=0.765 | AUC=0.558\n",
      "[Valence (Fold 1)] Ep 07/70 | Loss=0.2439 | Val Acc=42.86% | F1=0.333 | AUC=0.471\n",
      "[Valence (Fold 1)] Ep 08/70 | Loss=0.1704 | Val Acc=66.67% | F1=0.788 | AUC=0.550\n",
      "[Valence (Fold 1)] Ep 09/70 | Loss=0.0989 | Val Acc=64.29% | F1=0.754 | AUC=0.599\n",
      "[Valence (Fold 1)] Ep 10/70 | Loss=0.0831 | Val Acc=54.76% | F1=0.596 | AUC=0.514\n",
      "[Valence (Fold 1)] Ep 11/70 | Loss=0.0590 | Val Acc=54.76% | F1=0.627 | AUC=0.486\n",
      "[Valence (Fold 1)] Ep 12/70 | Loss=0.0376 | Val Acc=54.76% | F1=0.558 | AUC=0.591\n",
      "[Valence (Fold 1)] Ep 13/70 | Loss=0.0318 | Val Acc=38.10% | F1=0.278 | AUC=0.548\n",
      "[Valence (Fold 1)] Ep 14/70 | Loss=0.0205 | Val Acc=59.52% | F1=0.712 | AUC=0.555\n",
      "[Valence (Fold 1)] Ep 15/70 | Loss=0.0288 | Val Acc=45.24% | F1=0.303 | AUC=0.548\n",
      "[Valence (Fold 1)] Ep 16/70 | Loss=0.0184 | Val Acc=50.00% | F1=0.533 | AUC=0.555\n",
      "[Valence (Fold 1)] Ep 17/70 | Loss=0.0219 | Val Acc=61.90% | F1=0.724 | AUC=0.584\n",
      "[Valence (Fold 1)] Ep 18/70 | Loss=0.0237 | Val Acc=45.24% | F1=0.303 | AUC=0.522\n",
      "[Valence (Fold 1)] Ep 19/70 | Loss=0.0178 | Val Acc=59.52% | F1=0.667 | AUC=0.565\n",
      "[Valence (Fold 1)] Ep 20/70 | Loss=0.0145 | Val Acc=61.90% | F1=0.692 | AUC=0.565\n",
      "[Valence (Fold 1)] Ep 21/70 | Loss=0.0475 | Val Acc=57.14% | F1=0.654 | AUC=0.567\n",
      "[Valence (Fold 1)] Ep 22/70 | Loss=0.0628 | Val Acc=40.48% | F1=0.074 | AUC=0.558\n",
      "[Valence (Fold 1)] Ep 23/70 | Loss=0.1026 | Val Acc=59.52% | F1=0.691 | AUC=0.560\n",
      "[Valence (Fold 1)] Ep 24/70 | Loss=0.0516 | Val Acc=57.14% | F1=0.700 | AUC=0.496\n",
      "[Valence (Fold 1)] Ep 25/70 | Loss=0.0304 | Val Acc=45.24% | F1=0.207 | AUC=0.601\n",
      "[Valence (Fold 1)] Ep 26/70 | Loss=0.0180 | Val Acc=35.71% | F1=0.069 | AUC=0.490\n",
      "[Valence (Fold 1)] Ep 27/70 | Loss=0.0176 | Val Acc=38.10% | F1=0.000 | AUC=0.526\n",
      "[Valence (Fold 1)] Ep 28/70 | Loss=0.0176 | Val Acc=69.05% | F1=0.800 | AUC=0.505\n",
      "[Valence (Fold 1)] Ep 29/70 | Loss=0.0070 | Val Acc=50.00% | F1=0.533 | AUC=0.526\n",
      "[Valence (Fold 1)] Ep 30/70 | Loss=0.0108 | Val Acc=64.29% | F1=0.737 | AUC=0.532\n",
      "[Valence (Fold 1)] Ep 31/70 | Loss=0.0064 | Val Acc=61.90% | F1=0.750 | AUC=0.512\n",
      "[Valence (Fold 1)] Ep 32/70 | Loss=0.0039 | Val Acc=52.38% | F1=0.600 | AUC=0.523\n",
      "[Valence (Fold 1)] Ep 33/70 | Loss=0.0080 | Val Acc=59.52% | F1=0.721 | AUC=0.495\n",
      "[Valence (Fold 1)] Ep 34/70 | Loss=0.0031 | Val Acc=61.90% | F1=0.733 | AUC=0.550\n",
      "[Valence (Fold 1)] Ep 35/70 | Loss=0.0019 | Val Acc=54.76% | F1=0.596 | AUC=0.547\n",
      "[Valence (Fold 1)] Ep 36/70 | Loss=0.0025 | Val Acc=54.76% | F1=0.642 | AUC=0.526\n",
      "[Valence (Fold 1)] Ep 37/70 | Loss=0.0028 | Val Acc=52.38% | F1=0.583 | AUC=0.536\n",
      "[Valence (Fold 1)] Ep 38/70 | Loss=0.0011 | Val Acc=50.00% | F1=0.571 | AUC=0.528\n",
      "[Valence (Fold 1)] Ep 39/70 | Loss=0.0071 | Val Acc=50.00% | F1=0.553 | AUC=0.541\n",
      "[Valence (Fold 1)] Ep 40/70 | Loss=0.0016 | Val Acc=52.38% | F1=0.600 | AUC=0.524\n",
      "[Valence (Fold 1)] Ep 41/70 | Loss=0.0153 | Val Acc=61.90% | F1=0.765 | AUC=0.474\n",
      "[Valence (Fold 1)] Ep 42/70 | Loss=0.0955 | Val Acc=59.52% | F1=0.746 | AUC=0.417\n",
      "[Valence (Fold 1)] Ep 43/70 | Loss=0.0723 | Val Acc=42.86% | F1=0.294 | AUC=0.587\n",
      "[Valence (Fold 1)] Ep 44/70 | Loss=0.0410 | Val Acc=54.76% | F1=0.642 | AUC=0.566\n",
      "[Valence (Fold 1)] Ep 45/70 | Loss=0.0202 | Val Acc=61.90% | F1=0.765 | AUC=0.483\n",
      "[Valence (Fold 1)] Ep 46/70 | Loss=0.0280 | Val Acc=59.52% | F1=0.712 | AUC=0.530\n",
      "â¹ [Valence (Fold 1)] Early stopping at epoch 46\n",
      "\n",
      "ðŸ”š [Valence (Fold 1)] TEST (best thr=0.800) | Acc=61.90% | F1=0.733 | AUC=0.605\n",
      "\n",
      "Fold 1 Results: Acc=0.6190, F1=0.7333, AUC=0.6046\n",
      "\n",
      "----- Valence Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 2) | epochs=70\n",
      "[Valence (Fold 2)] Ep 01/70 | Loss=0.5357 | Val Acc=59.52% | F1=0.721 | AUC=0.543\n",
      "[Valence (Fold 2)] Ep 02/70 | Loss=0.5027 | Val Acc=45.24% | F1=0.410 | AUC=0.418\n",
      "[Valence (Fold 2)] Ep 03/70 | Loss=0.4633 | Val Acc=42.86% | F1=0.200 | AUC=0.418\n",
      "[Valence (Fold 2)] Ep 04/70 | Loss=0.4258 | Val Acc=59.52% | F1=0.738 | AUC=0.639\n",
      "[Valence (Fold 2)] Ep 05/70 | Loss=0.3710 | Val Acc=42.86% | F1=0.143 | AUC=0.346\n",
      "[Valence (Fold 2)] Ep 06/70 | Loss=0.2790 | Val Acc=42.86% | F1=0.143 | AUC=0.380\n",
      "[Valence (Fold 2)] Ep 07/70 | Loss=0.1826 | Val Acc=57.14% | F1=0.719 | AUC=0.526\n",
      "[Valence (Fold 2)] Ep 08/70 | Loss=0.1201 | Val Acc=52.38% | F1=0.615 | AUC=0.428\n",
      "[Valence (Fold 2)] Ep 09/70 | Loss=0.0724 | Val Acc=42.86% | F1=0.143 | AUC=0.510\n",
      "[Valence (Fold 2)] Ep 10/70 | Loss=0.0512 | Val Acc=42.86% | F1=0.250 | AUC=0.406\n",
      "[Valence (Fold 2)] Ep 11/70 | Loss=0.0367 | Val Acc=40.48% | F1=0.074 | AUC=0.394\n",
      "[Valence (Fold 2)] Ep 12/70 | Loss=0.0284 | Val Acc=40.48% | F1=0.444 | AUC=0.416\n",
      "[Valence (Fold 2)] Ep 13/70 | Loss=0.0252 | Val Acc=47.62% | F1=0.389 | AUC=0.445\n",
      "[Valence (Fold 2)] Ep 14/70 | Loss=0.0170 | Val Acc=45.24% | F1=0.343 | AUC=0.425\n",
      "[Valence (Fold 2)] Ep 15/70 | Loss=0.0132 | Val Acc=40.48% | F1=0.359 | AUC=0.416\n",
      "[Valence (Fold 2)] Ep 16/70 | Loss=0.0166 | Val Acc=42.86% | F1=0.429 | AUC=0.413\n",
      "[Valence (Fold 2)] Ep 17/70 | Loss=0.0124 | Val Acc=38.10% | F1=0.409 | AUC=0.404\n",
      "[Valence (Fold 2)] Ep 18/70 | Loss=0.0117 | Val Acc=40.48% | F1=0.444 | AUC=0.413\n",
      "[Valence (Fold 2)] Ep 19/70 | Loss=0.0110 | Val Acc=47.62% | F1=0.593 | AUC=0.422\n",
      "[Valence (Fold 2)] Ep 20/70 | Loss=0.0129 | Val Acc=40.48% | F1=0.490 | AUC=0.411\n",
      "[Valence (Fold 2)] Ep 21/70 | Loss=0.0988 | Val Acc=61.90% | F1=0.742 | AUC=0.522\n",
      "[Valence (Fold 2)] Ep 22/70 | Loss=0.0964 | Val Acc=54.76% | F1=0.689 | AUC=0.416\n",
      "[Valence (Fold 2)] Ep 23/70 | Loss=0.0504 | Val Acc=42.86% | F1=0.538 | AUC=0.409\n",
      "[Valence (Fold 2)] Ep 24/70 | Loss=0.0388 | Val Acc=61.90% | F1=0.765 | AUC=0.446\n",
      "[Valence (Fold 2)] Ep 25/70 | Loss=0.0409 | Val Acc=40.48% | F1=0.138 | AUC=0.476\n",
      "[Valence (Fold 2)] Ep 26/70 | Loss=0.0421 | Val Acc=61.90% | F1=0.765 | AUC=0.445\n",
      "[Valence (Fold 2)] Ep 27/70 | Loss=0.0220 | Val Acc=45.24% | F1=0.343 | AUC=0.428\n",
      "[Valence (Fold 2)] Ep 28/70 | Loss=0.0281 | Val Acc=40.48% | F1=0.074 | AUC=0.502\n",
      "[Valence (Fold 2)] Ep 29/70 | Loss=0.0134 | Val Acc=47.62% | F1=0.389 | AUC=0.469\n",
      "[Valence (Fold 2)] Ep 30/70 | Loss=0.0181 | Val Acc=42.86% | F1=0.143 | AUC=0.505\n",
      "[Valence (Fold 2)] Ep 31/70 | Loss=0.0141 | Val Acc=59.52% | F1=0.738 | AUC=0.520\n",
      "[Valence (Fold 2)] Ep 32/70 | Loss=0.0045 | Val Acc=42.86% | F1=0.333 | AUC=0.433\n",
      "[Valence (Fold 2)] Ep 33/70 | Loss=0.0030 | Val Acc=45.24% | F1=0.410 | AUC=0.406\n",
      "[Valence (Fold 2)] Ep 34/70 | Loss=0.0062 | Val Acc=42.86% | F1=0.455 | AUC=0.472\n",
      "[Valence (Fold 2)] Ep 35/70 | Loss=0.0045 | Val Acc=45.24% | F1=0.343 | AUC=0.459\n",
      "[Valence (Fold 2)] Ep 36/70 | Loss=0.0040 | Val Acc=52.38% | F1=0.643 | AUC=0.450\n",
      "[Valence (Fold 2)] Ep 37/70 | Loss=0.0016 | Val Acc=52.38% | F1=0.630 | AUC=0.441\n",
      "[Valence (Fold 2)] Ep 38/70 | Loss=0.0029 | Val Acc=50.00% | F1=0.618 | AUC=0.457\n",
      "[Valence (Fold 2)] Ep 39/70 | Loss=0.0014 | Val Acc=47.62% | F1=0.542 | AUC=0.438\n",
      "â¹ [Valence (Fold 2)] Early stopping at epoch 39\n",
      "\n",
      "ðŸ”š [Valence (Fold 2)] TEST (best thr=0.100) | Acc=59.52% | F1=0.638 | AUC=0.534\n",
      "\n",
      "Fold 2 Results: Acc=0.5952, F1=0.6383, AUC=0.5337\n",
      "\n",
      "----- Valence Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 3) | epochs=70\n",
      "[Valence (Fold 3)] Ep 01/70 | Loss=0.5339 | Val Acc=45.24% | F1=0.511 | AUC=0.442\n",
      "[Valence (Fold 3)] Ep 02/70 | Loss=0.5065 | Val Acc=59.52% | F1=0.746 | AUC=0.512\n",
      "[Valence (Fold 3)] Ep 03/70 | Loss=0.4787 | Val Acc=52.38% | F1=0.630 | AUC=0.514\n",
      "[Valence (Fold 3)] Ep 04/70 | Loss=0.4332 | Val Acc=33.33% | F1=0.125 | AUC=0.387\n",
      "[Valence (Fold 3)] Ep 05/70 | Loss=0.3968 | Val Acc=38.10% | F1=0.133 | AUC=0.425\n",
      "[Valence (Fold 3)] Ep 06/70 | Loss=0.2969 | Val Acc=38.10% | F1=0.000 | AUC=0.351\n",
      "[Valence (Fold 3)] Ep 07/70 | Loss=0.2084 | Val Acc=52.38% | F1=0.643 | AUC=0.435\n",
      "[Valence (Fold 3)] Ep 08/70 | Loss=0.1468 | Val Acc=35.71% | F1=0.000 | AUC=0.332\n",
      "[Valence (Fold 3)] Ep 09/70 | Loss=0.0933 | Val Acc=64.29% | F1=0.776 | AUC=0.584\n",
      "[Valence (Fold 3)] Ep 10/70 | Loss=0.0649 | Val Acc=35.71% | F1=0.000 | AUC=0.339\n",
      "[Valence (Fold 3)] Ep 11/70 | Loss=0.0428 | Val Acc=38.10% | F1=0.350 | AUC=0.375\n",
      "[Valence (Fold 3)] Ep 12/70 | Loss=0.0328 | Val Acc=59.52% | F1=0.712 | AUC=0.425\n",
      "[Valence (Fold 3)] Ep 13/70 | Loss=0.0364 | Val Acc=33.33% | F1=0.222 | AUC=0.385\n",
      "[Valence (Fold 3)] Ep 14/70 | Loss=0.0355 | Val Acc=57.14% | F1=0.679 | AUC=0.413\n",
      "[Valence (Fold 3)] Ep 15/70 | Loss=0.0271 | Val Acc=35.71% | F1=0.229 | AUC=0.387\n",
      "[Valence (Fold 3)] Ep 16/70 | Loss=0.0153 | Val Acc=30.95% | F1=0.216 | AUC=0.401\n",
      "[Valence (Fold 3)] Ep 17/70 | Loss=0.0133 | Val Acc=33.33% | F1=0.333 | AUC=0.380\n",
      "[Valence (Fold 3)] Ep 18/70 | Loss=0.0211 | Val Acc=50.00% | F1=0.588 | AUC=0.397\n",
      "[Valence (Fold 3)] Ep 19/70 | Loss=0.0175 | Val Acc=42.86% | F1=0.478 | AUC=0.387\n",
      "[Valence (Fold 3)] Ep 20/70 | Loss=0.0176 | Val Acc=47.62% | F1=0.560 | AUC=0.394\n",
      "[Valence (Fold 3)] Ep 21/70 | Loss=0.1321 | Val Acc=40.48% | F1=0.194 | AUC=0.406\n",
      "[Valence (Fold 3)] Ep 22/70 | Loss=0.1106 | Val Acc=38.10% | F1=0.000 | AUC=0.442\n",
      "[Valence (Fold 3)] Ep 23/70 | Loss=0.0421 | Val Acc=57.14% | F1=0.700 | AUC=0.459\n",
      "[Valence (Fold 3)] Ep 24/70 | Loss=0.0344 | Val Acc=30.95% | F1=0.065 | AUC=0.397\n",
      "[Valence (Fold 3)] Ep 25/70 | Loss=0.0417 | Val Acc=45.24% | F1=0.511 | AUC=0.411\n",
      "[Valence (Fold 3)] Ep 26/70 | Loss=0.0229 | Val Acc=38.10% | F1=0.071 | AUC=0.399\n",
      "[Valence (Fold 3)] Ep 27/70 | Loss=0.0377 | Val Acc=38.10% | F1=0.071 | AUC=0.435\n",
      "[Valence (Fold 3)] Ep 28/70 | Loss=0.0094 | Val Acc=59.52% | F1=0.702 | AUC=0.465\n",
      "[Valence (Fold 3)] Ep 29/70 | Loss=0.0081 | Val Acc=64.29% | F1=0.762 | AUC=0.564\n",
      "[Valence (Fold 3)] Ep 30/70 | Loss=0.0118 | Val Acc=50.00% | F1=0.604 | AUC=0.419\n",
      "[Valence (Fold 3)] Ep 31/70 | Loss=0.0168 | Val Acc=54.76% | F1=0.655 | AUC=0.410\n",
      "â¹ [Valence (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 3)] TEST (best thr=0.150) | Acc=59.52% | F1=0.712 | AUC=0.579\n",
      "\n",
      "Fold 3 Results: Acc=0.5952, F1=0.7119, AUC=0.5793\n",
      "\n",
      "----- Valence Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [128 202]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [17 25]\n",
      "[Valence (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.634\n",
      "\n",
      "ðŸš€ Training Valence (Fold 4) | epochs=70\n",
      "[Valence (Fold 4)] Ep 01/70 | Loss=0.5371 | Val Acc=59.52% | F1=0.738 | AUC=0.594\n",
      "[Valence (Fold 4)] Ep 02/70 | Loss=0.5057 | Val Acc=61.90% | F1=0.742 | AUC=0.615\n",
      "[Valence (Fold 4)] Ep 03/70 | Loss=0.4804 | Val Acc=64.29% | F1=0.776 | AUC=0.680\n",
      "[Valence (Fold 4)] Ep 04/70 | Loss=0.4517 | Val Acc=64.29% | F1=0.776 | AUC=0.543\n",
      "[Valence (Fold 4)] Ep 05/70 | Loss=0.4084 | Val Acc=61.90% | F1=0.758 | AUC=0.570\n",
      "[Valence (Fold 4)] Ep 06/70 | Loss=0.3332 | Val Acc=47.62% | F1=0.450 | AUC=0.565\n",
      "[Valence (Fold 4)] Ep 07/70 | Loss=0.2284 | Val Acc=57.14% | F1=0.719 | AUC=0.608\n",
      "[Valence (Fold 4)] Ep 08/70 | Loss=0.1469 | Val Acc=57.14% | F1=0.710 | AUC=0.589\n",
      "[Valence (Fold 4)] Ep 09/70 | Loss=0.1072 | Val Acc=40.48% | F1=0.390 | AUC=0.543\n",
      "[Valence (Fold 4)] Ep 10/70 | Loss=0.0752 | Val Acc=64.29% | F1=0.776 | AUC=0.603\n",
      "[Valence (Fold 4)] Ep 11/70 | Loss=0.0675 | Val Acc=61.90% | F1=0.758 | AUC=0.558\n",
      "[Valence (Fold 4)] Ep 12/70 | Loss=0.0555 | Val Acc=59.52% | F1=0.738 | AUC=0.550\n",
      "[Valence (Fold 4)] Ep 13/70 | Loss=0.0356 | Val Acc=57.14% | F1=0.679 | AUC=0.498\n",
      "[Valence (Fold 4)] Ep 14/70 | Loss=0.0314 | Val Acc=59.52% | F1=0.730 | AUC=0.603\n",
      "[Valence (Fold 4)] Ep 15/70 | Loss=0.0250 | Val Acc=50.00% | F1=0.571 | AUC=0.483\n",
      "[Valence (Fold 4)] Ep 16/70 | Loss=0.0194 | Val Acc=40.48% | F1=0.324 | AUC=0.454\n",
      "[Valence (Fold 4)] Ep 17/70 | Loss=0.0120 | Val Acc=57.14% | F1=0.679 | AUC=0.522\n",
      "[Valence (Fold 4)] Ep 18/70 | Loss=0.0153 | Val Acc=54.76% | F1=0.642 | AUC=0.495\n",
      "[Valence (Fold 4)] Ep 19/70 | Loss=0.0151 | Val Acc=57.14% | F1=0.679 | AUC=0.524\n",
      "[Valence (Fold 4)] Ep 20/70 | Loss=0.0160 | Val Acc=59.52% | F1=0.702 | AUC=0.502\n",
      "[Valence (Fold 4)] Ep 21/70 | Loss=0.0544 | Val Acc=57.14% | F1=0.710 | AUC=0.589\n",
      "[Valence (Fold 4)] Ep 22/70 | Loss=0.0947 | Val Acc=50.00% | F1=0.462 | AUC=0.534\n",
      "[Valence (Fold 4)] Ep 23/70 | Loss=0.0511 | Val Acc=42.86% | F1=0.368 | AUC=0.478\n",
      "[Valence (Fold 4)] Ep 24/70 | Loss=0.0744 | Val Acc=38.10% | F1=0.000 | AUC=0.514\n",
      "[Valence (Fold 4)] Ep 25/70 | Loss=0.0716 | Val Acc=57.14% | F1=0.640 | AUC=0.560\n",
      "[Valence (Fold 4)] Ep 26/70 | Loss=0.0798 | Val Acc=59.52% | F1=0.691 | AUC=0.534\n",
      "[Valence (Fold 4)] Ep 27/70 | Loss=0.0229 | Val Acc=57.14% | F1=0.667 | AUC=0.511\n",
      "[Valence (Fold 4)] Ep 28/70 | Loss=0.0102 | Val Acc=59.52% | F1=0.712 | AUC=0.558\n",
      "[Valence (Fold 4)] Ep 29/70 | Loss=0.0062 | Val Acc=52.38% | F1=0.655 | AUC=0.514\n",
      "[Valence (Fold 4)] Ep 30/70 | Loss=0.0089 | Val Acc=57.14% | F1=0.700 | AUC=0.573\n",
      "[Valence (Fold 4)] Ep 31/70 | Loss=0.0068 | Val Acc=59.52% | F1=0.721 | AUC=0.584\n",
      "â¹ [Valence (Fold 4)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 4)] TEST (best thr=0.100) | Acc=54.76% | F1=0.698 | AUC=0.425\n",
      "\n",
      "Fold 4 Results: Acc=0.5476, F1=0.6984, AUC=0.4247\n",
      "\n",
      "----- Valence Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 5) | epochs=70\n",
      "[Valence (Fold 5)] Ep 01/70 | Loss=0.5298 | Val Acc=58.54% | F1=0.653 | AUC=0.570\n",
      "[Valence (Fold 5)] Ep 02/70 | Loss=0.4915 | Val Acc=36.59% | F1=0.000 | AUC=0.437\n",
      "[Valence (Fold 5)] Ep 03/70 | Loss=0.4589 | Val Acc=51.22% | F1=0.600 | AUC=0.545\n",
      "[Valence (Fold 5)] Ep 04/70 | Loss=0.4215 | Val Acc=36.59% | F1=0.071 | AUC=0.343\n",
      "[Valence (Fold 5)] Ep 05/70 | Loss=0.3476 | Val Acc=58.54% | F1=0.738 | AUC=0.605\n",
      "[Valence (Fold 5)] Ep 06/70 | Loss=0.2554 | Val Acc=60.98% | F1=0.758 | AUC=0.650\n",
      "[Valence (Fold 5)] Ep 07/70 | Loss=0.1773 | Val Acc=36.59% | F1=0.071 | AUC=0.367\n",
      "[Valence (Fold 5)] Ep 08/70 | Loss=0.1132 | Val Acc=41.46% | F1=0.200 | AUC=0.525\n",
      "[Valence (Fold 5)] Ep 09/70 | Loss=0.0710 | Val Acc=43.90% | F1=0.343 | AUC=0.490\n",
      "[Valence (Fold 5)] Ep 10/70 | Loss=0.0634 | Val Acc=58.54% | F1=0.721 | AUC=0.575\n",
      "[Valence (Fold 5)] Ep 11/70 | Loss=0.0314 | Val Acc=51.22% | F1=0.524 | AUC=0.588\n",
      "[Valence (Fold 5)] Ep 12/70 | Loss=0.0221 | Val Acc=51.22% | F1=0.545 | AUC=0.542\n",
      "[Valence (Fold 5)] Ep 13/70 | Loss=0.0204 | Val Acc=58.54% | F1=0.712 | AUC=0.618\n",
      "[Valence (Fold 5)] Ep 14/70 | Loss=0.0267 | Val Acc=43.90% | F1=0.303 | AUC=0.495\n",
      "[Valence (Fold 5)] Ep 15/70 | Loss=0.0137 | Val Acc=53.66% | F1=0.558 | AUC=0.573\n",
      "[Valence (Fold 5)] Ep 16/70 | Loss=0.0169 | Val Acc=43.90% | F1=0.343 | AUC=0.542\n",
      "[Valence (Fold 5)] Ep 17/70 | Loss=0.0145 | Val Acc=51.22% | F1=0.524 | AUC=0.578\n",
      "[Valence (Fold 5)] Ep 18/70 | Loss=0.0123 | Val Acc=58.54% | F1=0.667 | AUC=0.610\n",
      "[Valence (Fold 5)] Ep 19/70 | Loss=0.0136 | Val Acc=53.66% | F1=0.558 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 20/70 | Loss=0.0160 | Val Acc=60.98% | F1=0.692 | AUC=0.605\n",
      "[Valence (Fold 5)] Ep 21/70 | Loss=0.0604 | Val Acc=39.02% | F1=0.138 | AUC=0.432\n",
      "[Valence (Fold 5)] Ep 22/70 | Loss=0.0634 | Val Acc=60.98% | F1=0.758 | AUC=0.556\n",
      "[Valence (Fold 5)] Ep 23/70 | Loss=0.0621 | Val Acc=58.54% | F1=0.721 | AUC=0.641\n",
      "[Valence (Fold 5)] Ep 24/70 | Loss=0.0471 | Val Acc=58.54% | F1=0.638 | AUC=0.562\n",
      "[Valence (Fold 5)] Ep 25/70 | Loss=0.0483 | Val Acc=39.02% | F1=0.138 | AUC=0.335\n",
      "[Valence (Fold 5)] Ep 26/70 | Loss=0.0308 | Val Acc=41.46% | F1=0.250 | AUC=0.477\n",
      "[Valence (Fold 5)] Ep 27/70 | Loss=0.0106 | Val Acc=46.34% | F1=0.476 | AUC=0.495\n",
      "[Valence (Fold 5)] Ep 28/70 | Loss=0.0222 | Val Acc=36.59% | F1=0.133 | AUC=0.340\n",
      "[Valence (Fold 5)] Ep 29/70 | Loss=0.0159 | Val Acc=56.10% | F1=0.700 | AUC=0.555\n",
      "[Valence (Fold 5)] Ep 30/70 | Loss=0.0065 | Val Acc=51.22% | F1=0.655 | AUC=0.639\n",
      "[Valence (Fold 5)] Ep 31/70 | Loss=0.0059 | Val Acc=51.22% | F1=0.545 | AUC=0.552\n",
      "â¹ [Valence (Fold 5)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 5)] TEST (best thr=0.100) | Acc=63.41% | F1=0.634 | AUC=0.650\n",
      "\n",
      "Fold 5 Results: Acc=0.6341, F1=0.6341, AUC=0.6500\n",
      "\n",
      "----- Valence Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 6) | epochs=70\n",
      "[Valence (Fold 6)] Ep 01/70 | Loss=0.5388 | Val Acc=63.41% | F1=0.727 | AUC=0.645\n",
      "[Valence (Fold 6)] Ep 02/70 | Loss=0.5130 | Val Acc=58.54% | F1=0.605 | AUC=0.717\n",
      "[Valence (Fold 6)] Ep 03/70 | Loss=0.4889 | Val Acc=60.98% | F1=0.704 | AUC=0.628\n",
      "[Valence (Fold 6)] Ep 04/70 | Loss=0.4649 | Val Acc=58.54% | F1=0.653 | AUC=0.613\n",
      "[Valence (Fold 6)] Ep 05/70 | Loss=0.4065 | Val Acc=68.29% | F1=0.755 | AUC=0.615\n",
      "[Valence (Fold 6)] Ep 06/70 | Loss=0.3263 | Val Acc=39.02% | F1=0.000 | AUC=0.608\n",
      "[Valence (Fold 6)] Ep 07/70 | Loss=0.2138 | Val Acc=63.41% | F1=0.769 | AUC=0.415\n",
      "[Valence (Fold 6)] Ep 08/70 | Loss=0.1670 | Val Acc=56.10% | F1=0.719 | AUC=0.455\n",
      "[Valence (Fold 6)] Ep 09/70 | Loss=0.1185 | Val Acc=51.22% | F1=0.444 | AUC=0.618\n",
      "[Valence (Fold 6)] Ep 10/70 | Loss=0.0824 | Val Acc=63.41% | F1=0.762 | AUC=0.498\n",
      "[Valence (Fold 6)] Ep 11/70 | Loss=0.0692 | Val Acc=53.66% | F1=0.642 | AUC=0.497\n",
      "[Valence (Fold 6)] Ep 12/70 | Loss=0.0556 | Val Acc=63.41% | F1=0.754 | AUC=0.578\n",
      "[Valence (Fold 6)] Ep 13/70 | Loss=0.0440 | Val Acc=46.34% | F1=0.312 | AUC=0.632\n",
      "[Valence (Fold 6)] Ep 14/70 | Loss=0.0246 | Val Acc=58.54% | F1=0.638 | AUC=0.585\n",
      "[Valence (Fold 6)] Ep 15/70 | Loss=0.0272 | Val Acc=56.10% | F1=0.710 | AUC=0.477\n",
      "[Valence (Fold 6)] Ep 16/70 | Loss=0.0172 | Val Acc=65.85% | F1=0.741 | AUC=0.590\n",
      "[Valence (Fold 6)] Ep 17/70 | Loss=0.0143 | Val Acc=56.10% | F1=0.500 | AUC=0.640\n",
      "[Valence (Fold 6)] Ep 18/70 | Loss=0.0147 | Val Acc=63.41% | F1=0.717 | AUC=0.587\n",
      "[Valence (Fold 6)] Ep 19/70 | Loss=0.0159 | Val Acc=60.98% | F1=0.680 | AUC=0.603\n",
      "[Valence (Fold 6)] Ep 20/70 | Loss=0.0156 | Val Acc=63.41% | F1=0.737 | AUC=0.575\n",
      "[Valence (Fold 6)] Ep 21/70 | Loss=0.0405 | Val Acc=41.46% | F1=0.200 | AUC=0.603\n",
      "[Valence (Fold 6)] Ep 22/70 | Loss=0.0614 | Val Acc=58.54% | F1=0.730 | AUC=0.443\n",
      "[Valence (Fold 6)] Ep 23/70 | Loss=0.0488 | Val Acc=48.78% | F1=0.400 | AUC=0.575\n",
      "[Valence (Fold 6)] Ep 24/70 | Loss=0.0496 | Val Acc=60.98% | F1=0.652 | AUC=0.670\n",
      "[Valence (Fold 6)] Ep 25/70 | Loss=0.0838 | Val Acc=58.54% | F1=0.691 | AUC=0.562\n",
      "[Valence (Fold 6)] Ep 26/70 | Loss=0.0290 | Val Acc=63.41% | F1=0.762 | AUC=0.374\n",
      "[Valence (Fold 6)] Ep 27/70 | Loss=0.0627 | Val Acc=53.66% | F1=0.578 | AUC=0.560\n",
      "[Valence (Fold 6)] Ep 28/70 | Loss=0.0235 | Val Acc=36.59% | F1=0.000 | AUC=0.650\n",
      "[Valence (Fold 6)] Ep 29/70 | Loss=0.0135 | Val Acc=60.98% | F1=0.750 | AUC=0.435\n",
      "[Valence (Fold 6)] Ep 30/70 | Loss=0.0085 | Val Acc=65.85% | F1=0.774 | AUC=0.539\n",
      "[Valence (Fold 6)] Ep 31/70 | Loss=0.0079 | Val Acc=65.85% | F1=0.767 | AUC=0.501\n",
      "â¹ [Valence (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 6)] TEST (best thr=0.700) | Acc=65.85% | F1=0.759 | AUC=0.613\n",
      "\n",
      "Fold 6 Results: Acc=0.6585, F1=0.7586, AUC=0.6125\n",
      "\n",
      "----- Valence Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 7) | epochs=70\n",
      "[Valence (Fold 7)] Ep 01/70 | Loss=0.5253 | Val Acc=68.29% | F1=0.683 | AUC=0.775\n",
      "[Valence (Fold 7)] Ep 02/70 | Loss=0.4885 | Val Acc=65.85% | F1=0.667 | AUC=0.743\n",
      "[Valence (Fold 7)] Ep 03/70 | Loss=0.4786 | Val Acc=65.85% | F1=0.682 | AUC=0.735\n",
      "[Valence (Fold 7)] Ep 04/70 | Loss=0.4292 | Val Acc=63.41% | F1=0.769 | AUC=0.680\n",
      "[Valence (Fold 7)] Ep 05/70 | Loss=0.3810 | Val Acc=73.17% | F1=0.820 | AUC=0.657\n",
      "[Valence (Fold 7)] Ep 06/70 | Loss=0.3092 | Val Acc=39.02% | F1=0.000 | AUC=0.603\n",
      "[Valence (Fold 7)] Ep 07/70 | Loss=0.2059 | Val Acc=70.73% | F1=0.793 | AUC=0.690\n",
      "[Valence (Fold 7)] Ep 08/70 | Loss=0.1249 | Val Acc=60.98% | F1=0.758 | AUC=0.568\n",
      "[Valence (Fold 7)] Ep 09/70 | Loss=0.0947 | Val Acc=65.85% | F1=0.781 | AUC=0.610\n",
      "[Valence (Fold 7)] Ep 10/70 | Loss=0.0692 | Val Acc=68.29% | F1=0.794 | AUC=0.650\n",
      "[Valence (Fold 7)] Ep 11/70 | Loss=0.0412 | Val Acc=73.17% | F1=0.814 | AUC=0.677\n",
      "[Valence (Fold 7)] Ep 12/70 | Loss=0.0421 | Val Acc=58.54% | F1=0.679 | AUC=0.680\n",
      "[Valence (Fold 7)] Ep 13/70 | Loss=0.0225 | Val Acc=68.29% | F1=0.667 | AUC=0.703\n",
      "[Valence (Fold 7)] Ep 14/70 | Loss=0.0190 | Val Acc=65.85% | F1=0.759 | AUC=0.693\n",
      "[Valence (Fold 7)] Ep 15/70 | Loss=0.0215 | Val Acc=60.98% | F1=0.619 | AUC=0.647\n",
      "[Valence (Fold 7)] Ep 16/70 | Loss=0.0169 | Val Acc=60.98% | F1=0.714 | AUC=0.672\n",
      "[Valence (Fold 7)] Ep 17/70 | Loss=0.0144 | Val Acc=53.66% | F1=0.558 | AUC=0.660\n",
      "[Valence (Fold 7)] Ep 18/70 | Loss=0.0116 | Val Acc=63.41% | F1=0.737 | AUC=0.652\n",
      "[Valence (Fold 7)] Ep 19/70 | Loss=0.0164 | Val Acc=60.98% | F1=0.704 | AUC=0.675\n",
      "[Valence (Fold 7)] Ep 20/70 | Loss=0.0130 | Val Acc=63.41% | F1=0.727 | AUC=0.675\n",
      "[Valence (Fold 7)] Ep 21/70 | Loss=0.0796 | Val Acc=39.02% | F1=0.000 | AUC=0.335\n",
      "[Valence (Fold 7)] Ep 22/70 | Loss=0.1224 | Val Acc=56.10% | F1=0.550 | AUC=0.660\n",
      "[Valence (Fold 7)] Ep 23/70 | Loss=0.0821 | Val Acc=60.98% | F1=0.758 | AUC=0.554\n",
      "[Valence (Fold 7)] Ep 24/70 | Loss=0.0564 | Val Acc=60.98% | F1=0.758 | AUC=0.669\n",
      "[Valence (Fold 7)] Ep 25/70 | Loss=0.0314 | Val Acc=65.85% | F1=0.720 | AUC=0.742\n",
      "[Valence (Fold 7)] Ep 26/70 | Loss=0.0320 | Val Acc=68.29% | F1=0.649 | AUC=0.710\n",
      "[Valence (Fold 7)] Ep 27/70 | Loss=0.0257 | Val Acc=39.02% | F1=0.000 | AUC=0.422\n",
      "[Valence (Fold 7)] Ep 28/70 | Loss=0.0113 | Val Acc=63.41% | F1=0.706 | AUC=0.708\n",
      "[Valence (Fold 7)] Ep 29/70 | Loss=0.0123 | Val Acc=70.73% | F1=0.806 | AUC=0.714\n",
      "[Valence (Fold 7)] Ep 30/70 | Loss=0.0099 | Val Acc=53.66% | F1=0.387 | AUC=0.618\n",
      "[Valence (Fold 7)] Ep 31/70 | Loss=0.0101 | Val Acc=70.73% | F1=0.806 | AUC=0.729\n",
      "â¹ [Valence (Fold 7)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 7)] TEST (best thr=0.400) | Acc=58.54% | F1=0.702 | AUC=0.493\n",
      "\n",
      "Fold 7 Results: Acc=0.5854, F1=0.7018, AUC=0.4925\n",
      "\n",
      "----- Valence Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 8) | epochs=70\n",
      "[Valence (Fold 8)] Ep 01/70 | Loss=0.5224 | Val Acc=51.22% | F1=0.524 | AUC=0.598\n",
      "[Valence (Fold 8)] Ep 02/70 | Loss=0.4946 | Val Acc=53.66% | F1=0.627 | AUC=0.568\n",
      "[Valence (Fold 8)] Ep 03/70 | Loss=0.4648 | Val Acc=39.02% | F1=0.138 | AUC=0.465\n",
      "[Valence (Fold 8)] Ep 04/70 | Loss=0.4364 | Val Acc=53.66% | F1=0.513 | AUC=0.565\n",
      "[Valence (Fold 8)] Ep 05/70 | Loss=0.3848 | Val Acc=56.10% | F1=0.690 | AUC=0.555\n",
      "[Valence (Fold 8)] Ep 06/70 | Loss=0.3141 | Val Acc=60.98% | F1=0.714 | AUC=0.647\n",
      "[Valence (Fold 8)] Ep 07/70 | Loss=0.2264 | Val Acc=58.54% | F1=0.691 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 08/70 | Loss=0.1665 | Val Acc=58.54% | F1=0.721 | AUC=0.598\n",
      "[Valence (Fold 8)] Ep 09/70 | Loss=0.1116 | Val Acc=53.66% | F1=0.698 | AUC=0.485\n",
      "[Valence (Fold 8)] Ep 10/70 | Loss=0.0827 | Val Acc=43.90% | F1=0.303 | AUC=0.515\n",
      "[Valence (Fold 8)] Ep 11/70 | Loss=0.0514 | Val Acc=60.98% | F1=0.733 | AUC=0.542\n",
      "[Valence (Fold 8)] Ep 12/70 | Loss=0.0499 | Val Acc=36.59% | F1=0.133 | AUC=0.540\n",
      "[Valence (Fold 8)] Ep 13/70 | Loss=0.0369 | Val Acc=48.78% | F1=0.400 | AUC=0.590\n",
      "[Valence (Fold 8)] Ep 14/70 | Loss=0.0367 | Val Acc=56.10% | F1=0.700 | AUC=0.635\n",
      "[Valence (Fold 8)] Ep 15/70 | Loss=0.0198 | Val Acc=48.78% | F1=0.400 | AUC=0.548\n",
      "[Valence (Fold 8)] Ep 16/70 | Loss=0.0167 | Val Acc=58.54% | F1=0.653 | AUC=0.600\n",
      "[Valence (Fold 8)] Ep 17/70 | Loss=0.0156 | Val Acc=51.22% | F1=0.524 | AUC=0.610\n",
      "[Valence (Fold 8)] Ep 18/70 | Loss=0.0166 | Val Acc=60.98% | F1=0.680 | AUC=0.590\n",
      "[Valence (Fold 8)] Ep 19/70 | Loss=0.0142 | Val Acc=58.54% | F1=0.653 | AUC=0.565\n",
      "[Valence (Fold 8)] Ep 20/70 | Loss=0.0172 | Val Acc=58.54% | F1=0.653 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 21/70 | Loss=0.0371 | Val Acc=56.10% | F1=0.719 | AUC=0.556\n",
      "[Valence (Fold 8)] Ep 22/70 | Loss=0.0537 | Val Acc=39.02% | F1=0.000 | AUC=0.540\n",
      "[Valence (Fold 8)] Ep 23/70 | Loss=0.0501 | Val Acc=36.59% | F1=0.071 | AUC=0.515\n",
      "[Valence (Fold 8)] Ep 24/70 | Loss=0.0333 | Val Acc=60.98% | F1=0.680 | AUC=0.650\n",
      "[Valence (Fold 8)] Ep 25/70 | Loss=0.0369 | Val Acc=56.10% | F1=0.625 | AUC=0.570\n",
      "[Valence (Fold 8)] Ep 26/70 | Loss=0.0453 | Val Acc=43.90% | F1=0.303 | AUC=0.585\n",
      "[Valence (Fold 8)] Ep 27/70 | Loss=0.0182 | Val Acc=46.34% | F1=0.389 | AUC=0.643\n",
      "[Valence (Fold 8)] Ep 28/70 | Loss=0.0375 | Val Acc=53.66% | F1=0.689 | AUC=0.389\n",
      "[Valence (Fold 8)] Ep 29/70 | Loss=0.0173 | Val Acc=53.66% | F1=0.596 | AUC=0.560\n",
      "[Valence (Fold 8)] Ep 30/70 | Loss=0.0132 | Val Acc=56.10% | F1=0.719 | AUC=0.281\n",
      "[Valence (Fold 8)] Ep 31/70 | Loss=0.0122 | Val Acc=58.54% | F1=0.622 | AUC=0.567\n",
      "â¹ [Valence (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 8)] TEST (best thr=0.100) | Acc=60.98% | F1=0.704 | AUC=0.509\n",
      "\n",
      "Fold 8 Results: Acc=0.6098, F1=0.7037, AUC=0.5088\n",
      "\n",
      "----- Valence Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 9) | epochs=70\n",
      "[Valence (Fold 9)] Ep 01/70 | Loss=0.5224 | Val Acc=58.54% | F1=0.653 | AUC=0.593\n",
      "[Valence (Fold 9)] Ep 02/70 | Loss=0.4983 | Val Acc=36.59% | F1=0.000 | AUC=0.472\n",
      "[Valence (Fold 9)] Ep 03/70 | Loss=0.4617 | Val Acc=65.85% | F1=0.720 | AUC=0.657\n",
      "[Valence (Fold 9)] Ep 04/70 | Loss=0.4374 | Val Acc=56.10% | F1=0.609 | AUC=0.637\n",
      "[Valence (Fold 9)] Ep 05/70 | Loss=0.3809 | Val Acc=73.17% | F1=0.814 | AUC=0.610\n",
      "[Valence (Fold 9)] Ep 06/70 | Loss=0.3080 | Val Acc=41.46% | F1=0.077 | AUC=0.415\n",
      "[Valence (Fold 9)] Ep 07/70 | Loss=0.2235 | Val Acc=60.98% | F1=0.758 | AUC=0.598\n",
      "[Valence (Fold 9)] Ep 08/70 | Loss=0.1261 | Val Acc=58.54% | F1=0.605 | AUC=0.620\n",
      "[Valence (Fold 9)] Ep 09/70 | Loss=0.1007 | Val Acc=51.22% | F1=0.474 | AUC=0.547\n",
      "[Valence (Fold 9)] Ep 10/70 | Loss=0.0747 | Val Acc=68.29% | F1=0.764 | AUC=0.602\n",
      "[Valence (Fold 9)] Ep 11/70 | Loss=0.0642 | Val Acc=46.34% | F1=0.353 | AUC=0.473\n",
      "[Valence (Fold 9)] Ep 12/70 | Loss=0.0691 | Val Acc=60.98% | F1=0.742 | AUC=0.610\n",
      "[Valence (Fold 9)] Ep 13/70 | Loss=0.0400 | Val Acc=53.66% | F1=0.513 | AUC=0.538\n",
      "[Valence (Fold 9)] Ep 14/70 | Loss=0.0494 | Val Acc=63.41% | F1=0.737 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 15/70 | Loss=0.0295 | Val Acc=48.78% | F1=0.488 | AUC=0.540\n",
      "[Valence (Fold 9)] Ep 16/70 | Loss=0.0218 | Val Acc=58.54% | F1=0.691 | AUC=0.595\n",
      "[Valence (Fold 9)] Ep 17/70 | Loss=0.0203 | Val Acc=63.41% | F1=0.717 | AUC=0.613\n",
      "[Valence (Fold 9)] Ep 18/70 | Loss=0.0190 | Val Acc=58.54% | F1=0.653 | AUC=0.580\n",
      "[Valence (Fold 9)] Ep 19/70 | Loss=0.0187 | Val Acc=60.98% | F1=0.680 | AUC=0.575\n",
      "[Valence (Fold 9)] Ep 20/70 | Loss=0.0177 | Val Acc=65.85% | F1=0.708 | AUC=0.583\n",
      "[Valence (Fold 9)] Ep 21/70 | Loss=0.0666 | Val Acc=39.02% | F1=0.324 | AUC=0.480\n",
      "[Valence (Fold 9)] Ep 22/70 | Loss=0.1026 | Val Acc=63.41% | F1=0.694 | AUC=0.645\n",
      "[Valence (Fold 9)] Ep 23/70 | Loss=0.0576 | Val Acc=60.98% | F1=0.652 | AUC=0.590\n",
      "[Valence (Fold 9)] Ep 24/70 | Loss=0.0815 | Val Acc=63.41% | F1=0.727 | AUC=0.588\n",
      "[Valence (Fold 9)] Ep 25/70 | Loss=0.0616 | Val Acc=41.46% | F1=0.077 | AUC=0.443\n",
      "[Valence (Fold 9)] Ep 26/70 | Loss=0.0298 | Val Acc=58.54% | F1=0.702 | AUC=0.609\n",
      "[Valence (Fold 9)] Ep 27/70 | Loss=0.0441 | Val Acc=48.78% | F1=0.323 | AUC=0.477\n",
      "[Valence (Fold 9)] Ep 28/70 | Loss=0.0165 | Val Acc=68.29% | F1=0.794 | AUC=0.606\n",
      "[Valence (Fold 9)] Ep 29/70 | Loss=0.0158 | Val Acc=63.41% | F1=0.717 | AUC=0.569\n",
      "[Valence (Fold 9)] Ep 30/70 | Loss=0.0102 | Val Acc=53.66% | F1=0.558 | AUC=0.530\n",
      "[Valence (Fold 9)] Ep 31/70 | Loss=0.0123 | Val Acc=51.22% | F1=0.474 | AUC=0.522\n",
      "â¹ [Valence (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 9)] TEST (best thr=0.100) | Acc=53.66% | F1=0.537 | AUC=0.600\n",
      "\n",
      "Fold 9 Results: Acc=0.5366, F1=0.5366, AUC=0.6000\n",
      "\n",
      "----- Valence Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 10) | epochs=70\n",
      "[Valence (Fold 10)] Ep 01/70 | Loss=0.5341 | Val Acc=60.98% | F1=0.636 | AUC=0.627\n",
      "[Valence (Fold 10)] Ep 02/70 | Loss=0.5038 | Val Acc=41.46% | F1=0.143 | AUC=0.632\n",
      "[Valence (Fold 10)] Ep 03/70 | Loss=0.4861 | Val Acc=56.10% | F1=0.526 | AUC=0.650\n",
      "[Valence (Fold 10)] Ep 04/70 | Loss=0.4547 | Val Acc=53.66% | F1=0.698 | AUC=0.517\n",
      "[Valence (Fold 10)] Ep 05/70 | Loss=0.4071 | Val Acc=56.10% | F1=0.625 | AUC=0.542\n",
      "[Valence (Fold 10)] Ep 06/70 | Loss=0.3266 | Val Acc=36.59% | F1=0.000 | AUC=0.410\n",
      "[Valence (Fold 10)] Ep 07/70 | Loss=0.2395 | Val Acc=48.78% | F1=0.488 | AUC=0.480\n",
      "[Valence (Fold 10)] Ep 08/70 | Loss=0.1628 | Val Acc=58.54% | F1=0.702 | AUC=0.537\n",
      "[Valence (Fold 10)] Ep 09/70 | Loss=0.1376 | Val Acc=58.54% | F1=0.738 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 10/70 | Loss=0.0838 | Val Acc=60.98% | F1=0.758 | AUC=0.500\n",
      "[Valence (Fold 10)] Ep 11/70 | Loss=0.0546 | Val Acc=60.98% | F1=0.733 | AUC=0.493\n",
      "[Valence (Fold 10)] Ep 12/70 | Loss=0.0488 | Val Acc=63.41% | F1=0.737 | AUC=0.500\n",
      "[Valence (Fold 10)] Ep 13/70 | Loss=0.0385 | Val Acc=39.02% | F1=0.286 | AUC=0.502\n",
      "[Valence (Fold 10)] Ep 14/70 | Loss=0.0329 | Val Acc=60.98% | F1=0.750 | AUC=0.453\n",
      "[Valence (Fold 10)] Ep 15/70 | Loss=0.0301 | Val Acc=29.27% | F1=0.121 | AUC=0.455\n",
      "[Valence (Fold 10)] Ep 16/70 | Loss=0.0221 | Val Acc=36.59% | F1=0.278 | AUC=0.475\n",
      "[Valence (Fold 10)] Ep 17/70 | Loss=0.0250 | Val Acc=51.22% | F1=0.524 | AUC=0.543\n",
      "[Valence (Fold 10)] Ep 18/70 | Loss=0.0186 | Val Acc=53.66% | F1=0.627 | AUC=0.540\n",
      "[Valence (Fold 10)] Ep 19/70 | Loss=0.0177 | Val Acc=53.66% | F1=0.612 | AUC=0.517\n",
      "[Valence (Fold 10)] Ep 20/70 | Loss=0.0177 | Val Acc=51.22% | F1=0.545 | AUC=0.518\n",
      "[Valence (Fold 10)] Ep 21/70 | Loss=0.0388 | Val Acc=56.10% | F1=0.719 | AUC=0.497\n",
      "[Valence (Fold 10)] Ep 22/70 | Loss=0.0570 | Val Acc=36.59% | F1=0.000 | AUC=0.487\n",
      "[Valence (Fold 10)] Ep 23/70 | Loss=0.0352 | Val Acc=60.98% | F1=0.667 | AUC=0.600\n",
      "[Valence (Fold 10)] Ep 24/70 | Loss=0.0608 | Val Acc=46.34% | F1=0.593 | AUC=0.393\n",
      "[Valence (Fold 10)] Ep 25/70 | Loss=0.0624 | Val Acc=58.54% | F1=0.738 | AUC=0.448\n",
      "[Valence (Fold 10)] Ep 26/70 | Loss=0.0592 | Val Acc=53.66% | F1=0.678 | AUC=0.472\n",
      "[Valence (Fold 10)] Ep 27/70 | Loss=0.0192 | Val Acc=60.98% | F1=0.750 | AUC=0.400\n",
      "[Valence (Fold 10)] Ep 28/70 | Loss=0.0285 | Val Acc=34.15% | F1=0.069 | AUC=0.453\n",
      "[Valence (Fold 10)] Ep 29/70 | Loss=0.0090 | Val Acc=31.71% | F1=0.000 | AUC=0.443\n",
      "[Valence (Fold 10)] Ep 30/70 | Loss=0.0084 | Val Acc=60.98% | F1=0.750 | AUC=0.417\n",
      "[Valence (Fold 10)] Ep 31/70 | Loss=0.0061 | Val Acc=53.66% | F1=0.596 | AUC=0.469\n",
      "â¹ [Valence (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 10)] TEST (best thr=0.100) | Acc=60.98% | F1=0.636 | AUC=0.546\n",
      "\n",
      "Fold 10 Results: Acc=0.6098, F1=0.6364, AUC=0.5463\n",
      "\n",
      "===== FINAL Valence 10-FOLD RESULTS =====\n",
      "Accuracy: 0.5991 Â± 0.0349\n",
      "F1-score: 0.6753 Â± 0.0612\n",
      "AUC:      0.5552 Â± 0.0640\n",
      "\n",
      "########## Arousal: 10-fold STRATIFIED CV ##########\n",
      "Arousal global median (for stratification) = 3.0000\n",
      "Class counts: [114 300]\n",
      "\n",
      "----- Arousal Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 1) | epochs=70\n",
      "[Arousal (Fold 1)] Ep 01/70 | Loss=0.3702 | Val Acc=50.00% | F1=0.533 | AUC=0.567\n",
      "[Arousal (Fold 1)] Ep 02/70 | Loss=0.3546 | Val Acc=26.19% | F1=0.000 | AUC=0.478\n",
      "[Arousal (Fold 1)] Ep 03/70 | Loss=0.3373 | Val Acc=66.67% | F1=0.788 | AUC=0.561\n",
      "[Arousal (Fold 1)] Ep 04/70 | Loss=0.3123 | Val Acc=73.81% | F1=0.831 | AUC=0.589\n",
      "[Arousal (Fold 1)] Ep 05/70 | Loss=0.2631 | Val Acc=28.57% | F1=0.062 | AUC=0.522\n",
      "[Arousal (Fold 1)] Ep 06/70 | Loss=0.2012 | Val Acc=40.48% | F1=0.390 | AUC=0.619\n",
      "[Arousal (Fold 1)] Ep 07/70 | Loss=0.1512 | Val Acc=28.57% | F1=0.000 | AUC=0.475\n",
      "[Arousal (Fold 1)] Ep 08/70 | Loss=0.1134 | Val Acc=30.95% | F1=0.216 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 09/70 | Loss=0.0855 | Val Acc=40.48% | F1=0.390 | AUC=0.583\n",
      "[Arousal (Fold 1)] Ep 10/70 | Loss=0.0650 | Val Acc=61.90% | F1=0.724 | AUC=0.592\n",
      "[Arousal (Fold 1)] Ep 11/70 | Loss=0.0383 | Val Acc=30.95% | F1=0.065 | AUC=0.603\n",
      "[Arousal (Fold 1)] Ep 12/70 | Loss=0.0466 | Val Acc=61.90% | F1=0.714 | AUC=0.664\n",
      "[Arousal (Fold 1)] Ep 13/70 | Loss=0.0297 | Val Acc=57.14% | F1=0.679 | AUC=0.575\n",
      "[Arousal (Fold 1)] Ep 14/70 | Loss=0.0243 | Val Acc=54.76% | F1=0.642 | AUC=0.636\n",
      "[Arousal (Fold 1)] Ep 15/70 | Loss=0.0203 | Val Acc=66.67% | F1=0.781 | AUC=0.600\n",
      "[Arousal (Fold 1)] Ep 16/70 | Loss=0.0244 | Val Acc=45.24% | F1=0.489 | AUC=0.589\n",
      "[Arousal (Fold 1)] Ep 17/70 | Loss=0.0173 | Val Acc=61.90% | F1=0.742 | AUC=0.594\n",
      "[Arousal (Fold 1)] Ep 18/70 | Loss=0.0176 | Val Acc=64.29% | F1=0.762 | AUC=0.569\n",
      "[Arousal (Fold 1)] Ep 19/70 | Loss=0.0134 | Val Acc=59.52% | F1=0.702 | AUC=0.550\n",
      "[Arousal (Fold 1)] Ep 20/70 | Loss=0.0181 | Val Acc=59.52% | F1=0.702 | AUC=0.561\n",
      "[Arousal (Fold 1)] Ep 21/70 | Loss=0.0305 | Val Acc=38.10% | F1=0.316 | AUC=0.686\n",
      "[Arousal (Fold 1)] Ep 22/70 | Loss=0.0739 | Val Acc=45.24% | F1=0.465 | AUC=0.581\n",
      "[Arousal (Fold 1)] Ep 23/70 | Loss=0.0496 | Val Acc=73.81% | F1=0.841 | AUC=0.654\n",
      "[Arousal (Fold 1)] Ep 24/70 | Loss=0.0218 | Val Acc=52.38% | F1=0.545 | AUC=0.683\n",
      "[Arousal (Fold 1)] Ep 25/70 | Loss=0.0446 | Val Acc=69.05% | F1=0.787 | AUC=0.658\n",
      "[Arousal (Fold 1)] Ep 26/70 | Loss=0.0227 | Val Acc=33.33% | F1=0.125 | AUC=0.506\n",
      "[Arousal (Fold 1)] Ep 27/70 | Loss=0.0182 | Val Acc=28.57% | F1=0.062 | AUC=0.447\n",
      "[Arousal (Fold 1)] Ep 28/70 | Loss=0.0149 | Val Acc=47.62% | F1=0.476 | AUC=0.678\n",
      "[Arousal (Fold 1)] Ep 29/70 | Loss=0.0144 | Val Acc=33.33% | F1=0.125 | AUC=0.517\n",
      "[Arousal (Fold 1)] Ep 30/70 | Loss=0.0143 | Val Acc=64.29% | F1=0.746 | AUC=0.575\n",
      "[Arousal (Fold 1)] Ep 31/70 | Loss=0.0053 | Val Acc=66.67% | F1=0.767 | AUC=0.561\n",
      "â¹ [Arousal (Fold 1)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 1)] TEST (best thr=0.100) | Acc=61.90% | F1=0.758 | AUC=0.428\n",
      "\n",
      "Fold 1 Results: Acc=0.6190, F1=0.7576, AUC=0.4278\n",
      "\n",
      "----- Arousal Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 2) | epochs=70\n",
      "[Arousal (Fold 2)] Ep 01/70 | Loss=0.3784 | Val Acc=61.90% | F1=0.750 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 02/70 | Loss=0.3592 | Val Acc=47.62% | F1=0.542 | AUC=0.444\n",
      "[Arousal (Fold 2)] Ep 03/70 | Loss=0.3366 | Val Acc=42.86% | F1=0.478 | AUC=0.489\n",
      "[Arousal (Fold 2)] Ep 04/70 | Loss=0.3122 | Val Acc=47.62% | F1=0.560 | AUC=0.439\n",
      "[Arousal (Fold 2)] Ep 05/70 | Loss=0.2548 | Val Acc=69.05% | F1=0.817 | AUC=0.492\n",
      "[Arousal (Fold 2)] Ep 06/70 | Loss=0.1891 | Val Acc=71.43% | F1=0.833 | AUC=0.464\n",
      "[Arousal (Fold 2)] Ep 07/70 | Loss=0.1427 | Val Acc=69.05% | F1=0.780 | AUC=0.581\n",
      "[Arousal (Fold 2)] Ep 08/70 | Loss=0.1064 | Val Acc=61.90% | F1=0.733 | AUC=0.611\n",
      "[Arousal (Fold 2)] Ep 09/70 | Loss=0.0735 | Val Acc=35.71% | F1=0.229 | AUC=0.472\n",
      "[Arousal (Fold 2)] Ep 10/70 | Loss=0.0495 | Val Acc=69.05% | F1=0.812 | AUC=0.514\n",
      "[Arousal (Fold 2)] Ep 11/70 | Loss=0.0355 | Val Acc=47.62% | F1=0.522 | AUC=0.572\n",
      "[Arousal (Fold 2)] Ep 12/70 | Loss=0.0307 | Val Acc=73.81% | F1=0.845 | AUC=0.603\n",
      "[Arousal (Fold 2)] Ep 13/70 | Loss=0.0245 | Val Acc=61.90% | F1=0.733 | AUC=0.617\n",
      "[Arousal (Fold 2)] Ep 14/70 | Loss=0.0224 | Val Acc=40.48% | F1=0.324 | AUC=0.575\n",
      "[Arousal (Fold 2)] Ep 15/70 | Loss=0.0178 | Val Acc=59.52% | F1=0.679 | AUC=0.631\n",
      "[Arousal (Fold 2)] Ep 16/70 | Loss=0.0213 | Val Acc=71.43% | F1=0.824 | AUC=0.625\n",
      "[Arousal (Fold 2)] Ep 17/70 | Loss=0.0135 | Val Acc=66.67% | F1=0.767 | AUC=0.636\n",
      "[Arousal (Fold 2)] Ep 18/70 | Loss=0.0138 | Val Acc=57.14% | F1=0.667 | AUC=0.600\n",
      "[Arousal (Fold 2)] Ep 19/70 | Loss=0.0138 | Val Acc=57.14% | F1=0.667 | AUC=0.625\n",
      "[Arousal (Fold 2)] Ep 20/70 | Loss=0.0127 | Val Acc=78.57% | F1=0.857 | AUC=0.636\n",
      "[Arousal (Fold 2)] Ep 21/70 | Loss=0.0313 | Val Acc=33.33% | F1=0.176 | AUC=0.478\n",
      "[Arousal (Fold 2)] Ep 22/70 | Loss=0.0486 | Val Acc=73.81% | F1=0.845 | AUC=0.562\n",
      "[Arousal (Fold 2)] Ep 23/70 | Loss=0.0421 | Val Acc=42.86% | F1=0.429 | AUC=0.547\n",
      "[Arousal (Fold 2)] Ep 24/70 | Loss=0.0226 | Val Acc=54.76% | F1=0.655 | AUC=0.553\n",
      "[Arousal (Fold 2)] Ep 25/70 | Loss=0.0288 | Val Acc=66.67% | F1=0.794 | AUC=0.461\n",
      "[Arousal (Fold 2)] Ep 26/70 | Loss=0.0281 | Val Acc=73.81% | F1=0.845 | AUC=0.562\n",
      "[Arousal (Fold 2)] Ep 27/70 | Loss=0.0190 | Val Acc=71.43% | F1=0.829 | AUC=0.531\n",
      "[Arousal (Fold 2)] Ep 28/70 | Loss=0.0150 | Val Acc=73.81% | F1=0.845 | AUC=0.544\n",
      "[Arousal (Fold 2)] Ep 29/70 | Loss=0.0112 | Val Acc=54.76% | F1=0.689 | AUC=0.442\n",
      "[Arousal (Fold 2)] Ep 30/70 | Loss=0.0117 | Val Acc=40.48% | F1=0.324 | AUC=0.522\n",
      "[Arousal (Fold 2)] Ep 31/70 | Loss=0.0034 | Val Acc=64.29% | F1=0.737 | AUC=0.569\n",
      "[Arousal (Fold 2)] Ep 32/70 | Loss=0.0025 | Val Acc=50.00% | F1=0.553 | AUC=0.542\n",
      "[Arousal (Fold 2)] Ep 33/70 | Loss=0.0027 | Val Acc=42.86% | F1=0.368 | AUC=0.586\n",
      "[Arousal (Fold 2)] Ep 34/70 | Loss=0.0045 | Val Acc=71.43% | F1=0.829 | AUC=0.667\n",
      "[Arousal (Fold 2)] Ep 35/70 | Loss=0.0025 | Val Acc=69.05% | F1=0.794 | AUC=0.597\n",
      "[Arousal (Fold 2)] Ep 36/70 | Loss=0.0017 | Val Acc=73.81% | F1=0.831 | AUC=0.589\n",
      "[Arousal (Fold 2)] Ep 37/70 | Loss=0.0024 | Val Acc=73.81% | F1=0.841 | AUC=0.614\n",
      "[Arousal (Fold 2)] Ep 38/70 | Loss=0.0015 | Val Acc=71.43% | F1=0.818 | AUC=0.586\n",
      "â¹ [Arousal (Fold 2)] Early stopping at epoch 38\n",
      "\n",
      "ðŸ”š [Arousal (Fold 2)] TEST (best thr=0.100) | Acc=66.67% | F1=0.794 | AUC=0.510\n",
      "\n",
      "Fold 2 Results: Acc=0.6667, F1=0.7941, AUC=0.5097\n",
      "\n",
      "----- Arousal Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 3) | epochs=70\n",
      "[Arousal (Fold 3)] Ep 01/70 | Loss=0.3797 | Val Acc=30.95% | F1=0.065 | AUC=0.742\n",
      "[Arousal (Fold 3)] Ep 02/70 | Loss=0.3545 | Val Acc=66.67% | F1=0.800 | AUC=0.658\n",
      "[Arousal (Fold 3)] Ep 03/70 | Loss=0.3276 | Val Acc=69.05% | F1=0.817 | AUC=0.611\n",
      "[Arousal (Fold 3)] Ep 04/70 | Loss=0.3030 | Val Acc=71.43% | F1=0.833 | AUC=0.739\n",
      "[Arousal (Fold 3)] Ep 05/70 | Loss=0.2691 | Val Acc=66.67% | F1=0.800 | AUC=0.617\n",
      "[Arousal (Fold 3)] Ep 06/70 | Loss=0.2295 | Val Acc=64.29% | F1=0.776 | AUC=0.597\n",
      "[Arousal (Fold 3)] Ep 07/70 | Loss=0.1589 | Val Acc=66.67% | F1=0.800 | AUC=0.583\n",
      "[Arousal (Fold 3)] Ep 08/70 | Loss=0.1252 | Val Acc=61.90% | F1=0.680 | AUC=0.592\n",
      "[Arousal (Fold 3)] Ep 09/70 | Loss=0.0796 | Val Acc=47.62% | F1=0.450 | AUC=0.711\n",
      "[Arousal (Fold 3)] Ep 10/70 | Loss=0.0601 | Val Acc=38.10% | F1=0.278 | AUC=0.636\n",
      "[Arousal (Fold 3)] Ep 11/70 | Loss=0.0592 | Val Acc=66.67% | F1=0.767 | AUC=0.611\n",
      "[Arousal (Fold 3)] Ep 12/70 | Loss=0.0364 | Val Acc=45.24% | F1=0.410 | AUC=0.647\n",
      "[Arousal (Fold 3)] Ep 13/70 | Loss=0.0285 | Val Acc=64.29% | F1=0.737 | AUC=0.650\n",
      "[Arousal (Fold 3)] Ep 14/70 | Loss=0.0215 | Val Acc=71.43% | F1=0.824 | AUC=0.619\n",
      "[Arousal (Fold 3)] Ep 15/70 | Loss=0.0208 | Val Acc=69.05% | F1=0.806 | AUC=0.606\n",
      "[Arousal (Fold 3)] Ep 16/70 | Loss=0.0196 | Val Acc=59.52% | F1=0.691 | AUC=0.636\n",
      "[Arousal (Fold 3)] Ep 17/70 | Loss=0.0279 | Val Acc=61.90% | F1=0.714 | AUC=0.619\n",
      "[Arousal (Fold 3)] Ep 18/70 | Loss=0.0197 | Val Acc=64.29% | F1=0.746 | AUC=0.583\n",
      "[Arousal (Fold 3)] Ep 19/70 | Loss=0.0145 | Val Acc=64.29% | F1=0.754 | AUC=0.575\n",
      "[Arousal (Fold 3)] Ep 20/70 | Loss=0.0160 | Val Acc=61.90% | F1=0.742 | AUC=0.583\n",
      "[Arousal (Fold 3)] Ep 21/70 | Loss=0.0425 | Val Acc=52.38% | F1=0.545 | AUC=0.622\n",
      "[Arousal (Fold 3)] Ep 22/70 | Loss=0.0605 | Val Acc=26.19% | F1=0.000 | AUC=0.514\n",
      "[Arousal (Fold 3)] Ep 23/70 | Loss=0.0603 | Val Acc=35.71% | F1=0.270 | AUC=0.531\n",
      "[Arousal (Fold 3)] Ep 24/70 | Loss=0.0320 | Val Acc=69.05% | F1=0.817 | AUC=0.464\n",
      "[Arousal (Fold 3)] Ep 25/70 | Loss=0.0310 | Val Acc=28.57% | F1=0.000 | AUC=0.497\n",
      "[Arousal (Fold 3)] Ep 26/70 | Loss=0.0295 | Val Acc=69.05% | F1=0.817 | AUC=0.469\n",
      "[Arousal (Fold 3)] Ep 27/70 | Loss=0.0213 | Val Acc=57.14% | F1=0.654 | AUC=0.603\n",
      "[Arousal (Fold 3)] Ep 28/70 | Loss=0.0132 | Val Acc=66.67% | F1=0.781 | AUC=0.608\n",
      "[Arousal (Fold 3)] Ep 29/70 | Loss=0.0126 | Val Acc=69.05% | F1=0.817 | AUC=0.501\n",
      "[Arousal (Fold 3)] Ep 30/70 | Loss=0.0082 | Val Acc=42.86% | F1=0.368 | AUC=0.664\n",
      "[Arousal (Fold 3)] Ep 31/70 | Loss=0.0078 | Val Acc=59.52% | F1=0.691 | AUC=0.568\n",
      "â¹ [Arousal (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 3)] TEST (best thr=0.450) | Acc=78.57% | F1=0.870 | AUC=0.608\n",
      "\n",
      "Fold 3 Results: Acc=0.7857, F1=0.8696, AUC=0.6083\n",
      "\n",
      "----- Arousal Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 4) | epochs=70\n",
      "[Arousal (Fold 4)] Ep 01/70 | Loss=0.3691 | Val Acc=26.19% | F1=0.162 | AUC=0.444\n",
      "[Arousal (Fold 4)] Ep 02/70 | Loss=0.3487 | Val Acc=38.10% | F1=0.381 | AUC=0.475\n",
      "[Arousal (Fold 4)] Ep 03/70 | Loss=0.3270 | Val Acc=69.05% | F1=0.806 | AUC=0.408\n",
      "[Arousal (Fold 4)] Ep 04/70 | Loss=0.2947 | Val Acc=50.00% | F1=0.571 | AUC=0.492\n",
      "[Arousal (Fold 4)] Ep 05/70 | Loss=0.2444 | Val Acc=28.57% | F1=0.000 | AUC=0.472\n",
      "[Arousal (Fold 4)] Ep 06/70 | Loss=0.1989 | Val Acc=69.05% | F1=0.817 | AUC=0.528\n",
      "[Arousal (Fold 4)] Ep 07/70 | Loss=0.1401 | Val Acc=59.52% | F1=0.679 | AUC=0.542\n",
      "[Arousal (Fold 4)] Ep 08/70 | Loss=0.1034 | Val Acc=71.43% | F1=0.824 | AUC=0.511\n",
      "[Arousal (Fold 4)] Ep 09/70 | Loss=0.0750 | Val Acc=30.95% | F1=0.216 | AUC=0.533\n",
      "[Arousal (Fold 4)] Ep 10/70 | Loss=0.0579 | Val Acc=69.05% | F1=0.800 | AUC=0.528\n",
      "[Arousal (Fold 4)] Ep 11/70 | Loss=0.0405 | Val Acc=38.10% | F1=0.409 | AUC=0.486\n",
      "[Arousal (Fold 4)] Ep 12/70 | Loss=0.0249 | Val Acc=66.67% | F1=0.767 | AUC=0.572\n",
      "[Arousal (Fold 4)] Ep 13/70 | Loss=0.0271 | Val Acc=71.43% | F1=0.824 | AUC=0.575\n",
      "[Arousal (Fold 4)] Ep 14/70 | Loss=0.0199 | Val Acc=47.62% | F1=0.542 | AUC=0.506\n",
      "[Arousal (Fold 4)] Ep 15/70 | Loss=0.0218 | Val Acc=42.86% | F1=0.455 | AUC=0.517\n",
      "[Arousal (Fold 4)] Ep 16/70 | Loss=0.0178 | Val Acc=59.52% | F1=0.721 | AUC=0.547\n",
      "[Arousal (Fold 4)] Ep 17/70 | Loss=0.0171 | Val Acc=61.90% | F1=0.742 | AUC=0.542\n",
      "[Arousal (Fold 4)] Ep 18/70 | Loss=0.0166 | Val Acc=59.52% | F1=0.712 | AUC=0.519\n",
      "[Arousal (Fold 4)] Ep 19/70 | Loss=0.0161 | Val Acc=52.38% | F1=0.600 | AUC=0.539\n",
      "[Arousal (Fold 4)] Ep 20/70 | Loss=0.0130 | Val Acc=59.52% | F1=0.712 | AUC=0.531\n",
      "[Arousal (Fold 4)] Ep 21/70 | Loss=0.0543 | Val Acc=71.43% | F1=0.829 | AUC=0.503\n",
      "[Arousal (Fold 4)] Ep 22/70 | Loss=0.0569 | Val Acc=71.43% | F1=0.829 | AUC=0.517\n",
      "[Arousal (Fold 4)] Ep 23/70 | Loss=0.0696 | Val Acc=30.95% | F1=0.256 | AUC=0.450\n",
      "[Arousal (Fold 4)] Ep 24/70 | Loss=0.0201 | Val Acc=30.95% | F1=0.065 | AUC=0.600\n",
      "[Arousal (Fold 4)] Ep 25/70 | Loss=0.0352 | Val Acc=30.95% | F1=0.065 | AUC=0.544\n",
      "[Arousal (Fold 4)] Ep 26/70 | Loss=0.0167 | Val Acc=71.43% | F1=0.829 | AUC=0.471\n",
      "[Arousal (Fold 4)] Ep 27/70 | Loss=0.0235 | Val Acc=66.67% | F1=0.759 | AUC=0.640\n",
      "[Arousal (Fold 4)] Ep 28/70 | Loss=0.0341 | Val Acc=38.10% | F1=0.235 | AUC=0.539\n",
      "[Arousal (Fold 4)] Ep 29/70 | Loss=0.0408 | Val Acc=66.67% | F1=0.800 | AUC=0.488\n",
      "[Arousal (Fold 4)] Ep 30/70 | Loss=0.0103 | Val Acc=30.95% | F1=0.065 | AUC=0.531\n",
      "[Arousal (Fold 4)] Ep 31/70 | Loss=0.0083 | Val Acc=35.71% | F1=0.182 | AUC=0.478\n",
      "â¹ [Arousal (Fold 4)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 4)] TEST (best thr=0.100) | Acc=47.62% | F1=0.421 | AUC=0.581\n",
      "\n",
      "Fold 4 Results: Acc=0.4762, F1=0.4211, AUC=0.5806\n",
      "\n",
      "----- Arousal Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 5) | epochs=70\n",
      "[Arousal (Fold 5)] Ep 01/70 | Loss=0.3783 | Val Acc=41.46% | F1=0.368 | AUC=0.591\n",
      "[Arousal (Fold 5)] Ep 02/70 | Loss=0.3703 | Val Acc=68.29% | F1=0.800 | AUC=0.621\n",
      "[Arousal (Fold 5)] Ep 03/70 | Loss=0.3498 | Val Acc=70.73% | F1=0.829 | AUC=0.548\n",
      "[Arousal (Fold 5)] Ep 04/70 | Loss=0.3207 | Val Acc=68.29% | F1=0.812 | AUC=0.561\n",
      "[Arousal (Fold 5)] Ep 05/70 | Loss=0.2622 | Val Acc=36.59% | F1=0.278 | AUC=0.642\n",
      "[Arousal (Fold 5)] Ep 06/70 | Loss=0.2087 | Val Acc=70.73% | F1=0.829 | AUC=0.485\n",
      "[Arousal (Fold 5)] Ep 07/70 | Loss=0.1422 | Val Acc=36.59% | F1=0.278 | AUC=0.594\n",
      "[Arousal (Fold 5)] Ep 08/70 | Loss=0.0914 | Val Acc=68.29% | F1=0.787 | AUC=0.470\n",
      "[Arousal (Fold 5)] Ep 09/70 | Loss=0.0630 | Val Acc=70.73% | F1=0.829 | AUC=0.415\n",
      "[Arousal (Fold 5)] Ep 10/70 | Loss=0.0423 | Val Acc=36.59% | F1=0.278 | AUC=0.542\n",
      "[Arousal (Fold 5)] Ep 11/70 | Loss=0.0379 | Val Acc=63.41% | F1=0.769 | AUC=0.415\n",
      "[Arousal (Fold 5)] Ep 12/70 | Loss=0.0253 | Val Acc=58.54% | F1=0.667 | AUC=0.561\n",
      "[Arousal (Fold 5)] Ep 13/70 | Loss=0.0280 | Val Acc=58.54% | F1=0.702 | AUC=0.485\n",
      "[Arousal (Fold 5)] Ep 14/70 | Loss=0.0172 | Val Acc=58.54% | F1=0.712 | AUC=0.452\n",
      "[Arousal (Fold 5)] Ep 15/70 | Loss=0.0234 | Val Acc=46.34% | F1=0.500 | AUC=0.527\n",
      "[Arousal (Fold 5)] Ep 16/70 | Loss=0.0147 | Val Acc=58.54% | F1=0.712 | AUC=0.436\n",
      "[Arousal (Fold 5)] Ep 17/70 | Loss=0.0112 | Val Acc=56.10% | F1=0.679 | AUC=0.455\n",
      "[Arousal (Fold 5)] Ep 18/70 | Loss=0.0126 | Val Acc=63.41% | F1=0.754 | AUC=0.427\n",
      "[Arousal (Fold 5)] Ep 19/70 | Loss=0.0107 | Val Acc=60.98% | F1=0.742 | AUC=0.421\n",
      "[Arousal (Fold 5)] Ep 20/70 | Loss=0.0107 | Val Acc=63.41% | F1=0.754 | AUC=0.445\n",
      "[Arousal (Fold 5)] Ep 21/70 | Loss=0.0496 | Val Acc=34.15% | F1=0.182 | AUC=0.558\n",
      "[Arousal (Fold 5)] Ep 22/70 | Loss=0.0868 | Val Acc=29.27% | F1=0.065 | AUC=0.552\n",
      "[Arousal (Fold 5)] Ep 23/70 | Loss=0.0412 | Val Acc=70.73% | F1=0.829 | AUC=0.421\n",
      "[Arousal (Fold 5)] Ep 24/70 | Loss=0.0560 | Val Acc=70.73% | F1=0.829 | AUC=0.386\n",
      "[Arousal (Fold 5)] Ep 25/70 | Loss=0.0255 | Val Acc=60.98% | F1=0.742 | AUC=0.445\n",
      "[Arousal (Fold 5)] Ep 26/70 | Loss=0.0145 | Val Acc=51.22% | F1=0.583 | AUC=0.494\n",
      "[Arousal (Fold 5)] Ep 27/70 | Loss=0.0331 | Val Acc=46.34% | F1=0.560 | AUC=0.482\n",
      "[Arousal (Fold 5)] Ep 28/70 | Loss=0.0156 | Val Acc=68.29% | F1=0.800 | AUC=0.461\n",
      "[Arousal (Fold 5)] Ep 29/70 | Loss=0.0094 | Val Acc=36.59% | F1=0.235 | AUC=0.573\n",
      "[Arousal (Fold 5)] Ep 30/70 | Loss=0.0203 | Val Acc=73.17% | F1=0.836 | AUC=0.433\n",
      "[Arousal (Fold 5)] Ep 31/70 | Loss=0.0037 | Val Acc=46.34% | F1=0.560 | AUC=0.500\n",
      "[Arousal (Fold 5)] Ep 32/70 | Loss=0.0031 | Val Acc=58.54% | F1=0.721 | AUC=0.409\n",
      "[Arousal (Fold 5)] Ep 33/70 | Loss=0.0033 | Val Acc=70.73% | F1=0.829 | AUC=0.479\n",
      "[Arousal (Fold 5)] Ep 34/70 | Loss=0.0012 | Val Acc=48.78% | F1=0.512 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 35/70 | Loss=0.0015 | Val Acc=75.61% | F1=0.853 | AUC=0.476\n",
      "[Arousal (Fold 5)] Ep 36/70 | Loss=0.0064 | Val Acc=70.73% | F1=0.818 | AUC=0.450\n",
      "[Arousal (Fold 5)] Ep 37/70 | Loss=0.0017 | Val Acc=63.41% | F1=0.762 | AUC=0.456\n",
      "[Arousal (Fold 5)] Ep 38/70 | Loss=0.0017 | Val Acc=63.41% | F1=0.762 | AUC=0.430\n",
      "[Arousal (Fold 5)] Ep 39/70 | Loss=0.0019 | Val Acc=63.41% | F1=0.762 | AUC=0.450\n",
      "[Arousal (Fold 5)] Ep 40/70 | Loss=0.0023 | Val Acc=63.41% | F1=0.754 | AUC=0.456\n",
      "[Arousal (Fold 5)] Ep 41/70 | Loss=0.0242 | Val Acc=70.73% | F1=0.829 | AUC=0.450\n",
      "[Arousal (Fold 5)] Ep 42/70 | Loss=0.0395 | Val Acc=73.17% | F1=0.845 | AUC=0.508\n",
      "[Arousal (Fold 5)] Ep 43/70 | Loss=0.0203 | Val Acc=68.29% | F1=0.780 | AUC=0.515\n",
      "[Arousal (Fold 5)] Ep 44/70 | Loss=0.0382 | Val Acc=68.29% | F1=0.800 | AUC=0.427\n",
      "[Arousal (Fold 5)] Ep 45/70 | Loss=0.0140 | Val Acc=70.73% | F1=0.829 | AUC=0.417\n",
      "[Arousal (Fold 5)] Ep 46/70 | Loss=0.0043 | Val Acc=70.73% | F1=0.829 | AUC=0.479\n",
      "[Arousal (Fold 5)] Ep 47/70 | Loss=0.0073 | Val Acc=60.98% | F1=0.733 | AUC=0.535\n",
      "[Arousal (Fold 5)] Ep 48/70 | Loss=0.0026 | Val Acc=60.98% | F1=0.742 | AUC=0.414\n",
      "[Arousal (Fold 5)] Ep 49/70 | Loss=0.0075 | Val Acc=68.29% | F1=0.800 | AUC=0.471\n",
      "[Arousal (Fold 5)] Ep 50/70 | Loss=0.0018 | Val Acc=70.73% | F1=0.829 | AUC=0.494\n",
      "[Arousal (Fold 5)] Ep 51/70 | Loss=0.0011 | Val Acc=58.54% | F1=0.721 | AUC=0.421\n",
      "[Arousal (Fold 5)] Ep 52/70 | Loss=0.0062 | Val Acc=60.98% | F1=0.724 | AUC=0.521\n",
      "[Arousal (Fold 5)] Ep 53/70 | Loss=0.0016 | Val Acc=51.22% | F1=0.655 | AUC=0.420\n",
      "â¹ [Arousal (Fold 5)] Early stopping at epoch 53\n",
      "\n",
      "ðŸ”š [Arousal (Fold 5)] TEST (best thr=0.100) | Acc=56.10% | F1=0.679 | AUC=0.479\n",
      "\n",
      "Fold 5 Results: Acc=0.5610, F1=0.6786, AUC=0.4788\n",
      "\n",
      "----- Arousal Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 6) | epochs=70\n",
      "[Arousal (Fold 6)] Ep 01/70 | Loss=0.3831 | Val Acc=41.46% | F1=0.368 | AUC=0.603\n",
      "[Arousal (Fold 6)] Ep 02/70 | Loss=0.3698 | Val Acc=36.59% | F1=0.235 | AUC=0.485\n",
      "[Arousal (Fold 6)] Ep 03/70 | Loss=0.3473 | Val Acc=70.73% | F1=0.829 | AUC=0.627\n",
      "[Arousal (Fold 6)] Ep 04/70 | Loss=0.3128 | Val Acc=63.41% | F1=0.754 | AUC=0.552\n",
      "[Arousal (Fold 6)] Ep 05/70 | Loss=0.2628 | Val Acc=73.17% | F1=0.845 | AUC=0.603\n",
      "[Arousal (Fold 6)] Ep 06/70 | Loss=0.2039 | Val Acc=51.22% | F1=0.630 | AUC=0.497\n",
      "[Arousal (Fold 6)] Ep 07/70 | Loss=0.1634 | Val Acc=70.73% | F1=0.829 | AUC=0.645\n",
      "[Arousal (Fold 6)] Ep 08/70 | Loss=0.1126 | Val Acc=29.27% | F1=0.065 | AUC=0.348\n",
      "[Arousal (Fold 6)] Ep 09/70 | Loss=0.0830 | Val Acc=65.85% | F1=0.794 | AUC=0.485\n",
      "[Arousal (Fold 6)] Ep 10/70 | Loss=0.0663 | Val Acc=53.66% | F1=0.689 | AUC=0.400\n",
      "[Arousal (Fold 6)] Ep 11/70 | Loss=0.0455 | Val Acc=39.02% | F1=0.468 | AUC=0.452\n",
      "[Arousal (Fold 6)] Ep 12/70 | Loss=0.0349 | Val Acc=70.73% | F1=0.829 | AUC=0.345\n",
      "[Arousal (Fold 6)] Ep 13/70 | Loss=0.0290 | Val Acc=36.59% | F1=0.381 | AUC=0.433\n",
      "[Arousal (Fold 6)] Ep 14/70 | Loss=0.0296 | Val Acc=29.27% | F1=0.171 | AUC=0.430\n",
      "[Arousal (Fold 6)] Ep 15/70 | Loss=0.0257 | Val Acc=46.34% | F1=0.621 | AUC=0.306\n",
      "[Arousal (Fold 6)] Ep 16/70 | Loss=0.0255 | Val Acc=68.29% | F1=0.812 | AUC=0.345\n",
      "[Arousal (Fold 6)] Ep 17/70 | Loss=0.0207 | Val Acc=36.59% | F1=0.409 | AUC=0.376\n",
      "[Arousal (Fold 6)] Ep 18/70 | Loss=0.0173 | Val Acc=56.10% | F1=0.719 | AUC=0.385\n",
      "[Arousal (Fold 6)] Ep 19/70 | Loss=0.0161 | Val Acc=60.98% | F1=0.758 | AUC=0.391\n",
      "[Arousal (Fold 6)] Ep 20/70 | Loss=0.0188 | Val Acc=51.22% | F1=0.655 | AUC=0.379\n",
      "[Arousal (Fold 6)] Ep 21/70 | Loss=0.0348 | Val Acc=53.66% | F1=0.642 | AUC=0.579\n",
      "[Arousal (Fold 6)] Ep 22/70 | Loss=0.0549 | Val Acc=63.41% | F1=0.769 | AUC=0.448\n",
      "[Arousal (Fold 6)] Ep 23/70 | Loss=0.0563 | Val Acc=58.54% | F1=0.730 | AUC=0.452\n",
      "[Arousal (Fold 6)] Ep 24/70 | Loss=0.0311 | Val Acc=21.95% | F1=0.059 | AUC=0.445\n",
      "[Arousal (Fold 6)] Ep 25/70 | Loss=0.0292 | Val Acc=73.17% | F1=0.845 | AUC=0.398\n",
      "[Arousal (Fold 6)] Ep 26/70 | Loss=0.0295 | Val Acc=29.27% | F1=0.256 | AUC=0.355\n",
      "[Arousal (Fold 6)] Ep 27/70 | Loss=0.0151 | Val Acc=43.90% | F1=0.582 | AUC=0.400\n",
      "[Arousal (Fold 6)] Ep 28/70 | Loss=0.0113 | Val Acc=65.85% | F1=0.794 | AUC=0.429\n",
      "[Arousal (Fold 6)] Ep 29/70 | Loss=0.0116 | Val Acc=43.90% | F1=0.566 | AUC=0.364\n",
      "[Arousal (Fold 6)] Ep 30/70 | Loss=0.0087 | Val Acc=34.15% | F1=0.400 | AUC=0.364\n",
      "[Arousal (Fold 6)] Ep 31/70 | Loss=0.0057 | Val Acc=70.73% | F1=0.829 | AUC=0.339\n",
      "â¹ [Arousal (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 6)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.506\n",
      "\n",
      "Fold 6 Results: Acc=0.7317, F1=0.8451, AUC=0.5061\n",
      "\n",
      "----- Arousal Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 7) | epochs=70\n",
      "[Arousal (Fold 7)] Ep 01/70 | Loss=0.3820 | Val Acc=51.22% | F1=0.545 | AUC=0.618\n",
      "[Arousal (Fold 7)] Ep 02/70 | Loss=0.3620 | Val Acc=34.15% | F1=0.182 | AUC=0.582\n",
      "[Arousal (Fold 7)] Ep 03/70 | Loss=0.3519 | Val Acc=63.41% | F1=0.737 | AUC=0.667\n",
      "[Arousal (Fold 7)] Ep 04/70 | Loss=0.3219 | Val Acc=51.22% | F1=0.600 | AUC=0.576\n",
      "[Arousal (Fold 7)] Ep 05/70 | Loss=0.2773 | Val Acc=46.34% | F1=0.522 | AUC=0.567\n",
      "[Arousal (Fold 7)] Ep 06/70 | Loss=0.2262 | Val Acc=31.71% | F1=0.176 | AUC=0.536\n",
      "[Arousal (Fold 7)] Ep 07/70 | Loss=0.1790 | Val Acc=53.66% | F1=0.627 | AUC=0.530\n",
      "[Arousal (Fold 7)] Ep 08/70 | Loss=0.1187 | Val Acc=70.73% | F1=0.829 | AUC=0.591\n",
      "[Arousal (Fold 7)] Ep 09/70 | Loss=0.0742 | Val Acc=39.02% | F1=0.359 | AUC=0.442\n",
      "[Arousal (Fold 7)] Ep 10/70 | Loss=0.0641 | Val Acc=63.41% | F1=0.762 | AUC=0.518\n",
      "[Arousal (Fold 7)] Ep 11/70 | Loss=0.0432 | Val Acc=51.22% | F1=0.615 | AUC=0.445\n",
      "[Arousal (Fold 7)] Ep 12/70 | Loss=0.0371 | Val Acc=70.73% | F1=0.818 | AUC=0.527\n",
      "[Arousal (Fold 7)] Ep 13/70 | Loss=0.0337 | Val Acc=68.29% | F1=0.794 | AUC=0.552\n",
      "[Arousal (Fold 7)] Ep 14/70 | Loss=0.0214 | Val Acc=63.41% | F1=0.762 | AUC=0.509\n",
      "[Arousal (Fold 7)] Ep 15/70 | Loss=0.0200 | Val Acc=70.73% | F1=0.818 | AUC=0.573\n",
      "[Arousal (Fold 7)] Ep 16/70 | Loss=0.0181 | Val Acc=56.10% | F1=0.667 | AUC=0.512\n",
      "[Arousal (Fold 7)] Ep 17/70 | Loss=0.0159 | Val Acc=43.90% | F1=0.439 | AUC=0.464\n",
      "[Arousal (Fold 7)] Ep 18/70 | Loss=0.0159 | Val Acc=53.66% | F1=0.655 | AUC=0.494\n",
      "[Arousal (Fold 7)] Ep 19/70 | Loss=0.0171 | Val Acc=51.22% | F1=0.600 | AUC=0.464\n",
      "[Arousal (Fold 7)] Ep 20/70 | Loss=0.0151 | Val Acc=53.66% | F1=0.627 | AUC=0.515\n",
      "[Arousal (Fold 7)] Ep 21/70 | Loss=0.0347 | Val Acc=68.29% | F1=0.806 | AUC=0.515\n",
      "[Arousal (Fold 7)] Ep 22/70 | Loss=0.0660 | Val Acc=56.10% | F1=0.625 | AUC=0.561\n",
      "[Arousal (Fold 7)] Ep 23/70 | Loss=0.0509 | Val Acc=56.10% | F1=0.654 | AUC=0.518\n",
      "[Arousal (Fold 7)] Ep 24/70 | Loss=0.0419 | Val Acc=65.85% | F1=0.781 | AUC=0.530\n",
      "[Arousal (Fold 7)] Ep 25/70 | Loss=0.0398 | Val Acc=65.85% | F1=0.759 | AUC=0.524\n",
      "[Arousal (Fold 7)] Ep 26/70 | Loss=0.0119 | Val Acc=56.10% | F1=0.690 | AUC=0.573\n",
      "[Arousal (Fold 7)] Ep 27/70 | Loss=0.0081 | Val Acc=75.61% | F1=0.853 | AUC=0.553\n",
      "[Arousal (Fold 7)] Ep 28/70 | Loss=0.0146 | Val Acc=48.78% | F1=0.553 | AUC=0.542\n",
      "[Arousal (Fold 7)] Ep 29/70 | Loss=0.0113 | Val Acc=70.73% | F1=0.818 | AUC=0.558\n",
      "[Arousal (Fold 7)] Ep 30/70 | Loss=0.0053 | Val Acc=51.22% | F1=0.630 | AUC=0.464\n",
      "[Arousal (Fold 7)] Ep 31/70 | Loss=0.0102 | Val Acc=68.29% | F1=0.787 | AUC=0.538\n",
      "[Arousal (Fold 7)] Ep 32/70 | Loss=0.0044 | Val Acc=68.29% | F1=0.794 | AUC=0.541\n",
      "[Arousal (Fold 7)] Ep 33/70 | Loss=0.0024 | Val Acc=56.10% | F1=0.654 | AUC=0.533\n",
      "[Arousal (Fold 7)] Ep 34/70 | Loss=0.0017 | Val Acc=53.66% | F1=0.627 | AUC=0.500\n",
      "[Arousal (Fold 7)] Ep 35/70 | Loss=0.0034 | Val Acc=43.90% | F1=0.465 | AUC=0.482\n",
      "[Arousal (Fold 7)] Ep 36/70 | Loss=0.0034 | Val Acc=65.85% | F1=0.759 | AUC=0.542\n",
      "[Arousal (Fold 7)] Ep 37/70 | Loss=0.0026 | Val Acc=58.54% | F1=0.702 | AUC=0.542\n",
      "[Arousal (Fold 7)] Ep 38/70 | Loss=0.0049 | Val Acc=60.98% | F1=0.704 | AUC=0.555\n",
      "[Arousal (Fold 7)] Ep 39/70 | Loss=0.0041 | Val Acc=65.85% | F1=0.767 | AUC=0.567\n",
      "[Arousal (Fold 7)] Ep 40/70 | Loss=0.0026 | Val Acc=65.85% | F1=0.759 | AUC=0.567\n",
      "[Arousal (Fold 7)] Ep 41/70 | Loss=0.0052 | Val Acc=31.71% | F1=0.176 | AUC=0.445\n",
      "[Arousal (Fold 7)] Ep 42/70 | Loss=0.0348 | Val Acc=29.27% | F1=0.121 | AUC=0.500\n",
      "[Arousal (Fold 7)] Ep 43/70 | Loss=0.0351 | Val Acc=31.71% | F1=0.176 | AUC=0.476\n",
      "[Arousal (Fold 7)] Ep 44/70 | Loss=0.0358 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "[Arousal (Fold 7)] Ep 45/70 | Loss=0.0338 | Val Acc=34.15% | F1=0.229 | AUC=0.415\n",
      "â¹ [Arousal (Fold 7)] Early stopping at epoch 45\n",
      "\n",
      "ðŸ”š [Arousal (Fold 7)] TEST (best thr=0.150) | Acc=36.59% | F1=0.278 | AUC=0.594\n",
      "\n",
      "Fold 7 Results: Acc=0.3659, F1=0.2778, AUC=0.5939\n",
      "\n",
      "----- Arousal Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 8) | epochs=70\n",
      "[Arousal (Fold 8)] Ep 01/70 | Loss=0.3875 | Val Acc=41.46% | F1=0.368 | AUC=0.582\n",
      "[Arousal (Fold 8)] Ep 02/70 | Loss=0.3715 | Val Acc=36.59% | F1=0.350 | AUC=0.436\n",
      "[Arousal (Fold 8)] Ep 03/70 | Loss=0.3552 | Val Acc=60.98% | F1=0.750 | AUC=0.636\n",
      "[Arousal (Fold 8)] Ep 04/70 | Loss=0.3278 | Val Acc=36.59% | F1=0.350 | AUC=0.430\n",
      "[Arousal (Fold 8)] Ep 05/70 | Loss=0.2822 | Val Acc=73.17% | F1=0.845 | AUC=0.552\n",
      "[Arousal (Fold 8)] Ep 06/70 | Loss=0.2307 | Val Acc=51.22% | F1=0.565 | AUC=0.636\n",
      "[Arousal (Fold 8)] Ep 07/70 | Loss=0.1570 | Val Acc=41.46% | F1=0.368 | AUC=0.521\n",
      "[Arousal (Fold 8)] Ep 08/70 | Loss=0.0951 | Val Acc=60.98% | F1=0.714 | AUC=0.542\n",
      "[Arousal (Fold 8)] Ep 09/70 | Loss=0.0617 | Val Acc=70.73% | F1=0.829 | AUC=0.633\n",
      "[Arousal (Fold 8)] Ep 10/70 | Loss=0.0568 | Val Acc=36.59% | F1=0.381 | AUC=0.512\n",
      "[Arousal (Fold 8)] Ep 11/70 | Loss=0.0438 | Val Acc=70.73% | F1=0.829 | AUC=0.645\n",
      "[Arousal (Fold 8)] Ep 12/70 | Loss=0.0333 | Val Acc=70.73% | F1=0.829 | AUC=0.658\n",
      "[Arousal (Fold 8)] Ep 13/70 | Loss=0.0185 | Val Acc=70.73% | F1=0.824 | AUC=0.648\n",
      "[Arousal (Fold 8)] Ep 14/70 | Loss=0.0193 | Val Acc=43.90% | F1=0.511 | AUC=0.536\n",
      "[Arousal (Fold 8)] Ep 15/70 | Loss=0.0181 | Val Acc=58.54% | F1=0.730 | AUC=0.533\n",
      "[Arousal (Fold 8)] Ep 16/70 | Loss=0.0139 | Val Acc=41.46% | F1=0.520 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 17/70 | Loss=0.0119 | Val Acc=46.34% | F1=0.593 | AUC=0.564\n",
      "[Arousal (Fold 8)] Ep 18/70 | Loss=0.0133 | Val Acc=48.78% | F1=0.604 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 19/70 | Loss=0.0113 | Val Acc=48.78% | F1=0.604 | AUC=0.585\n",
      "[Arousal (Fold 8)] Ep 20/70 | Loss=0.0150 | Val Acc=51.22% | F1=0.630 | AUC=0.591\n",
      "[Arousal (Fold 8)] Ep 21/70 | Loss=0.0405 | Val Acc=36.59% | F1=0.278 | AUC=0.433\n",
      "[Arousal (Fold 8)] Ep 22/70 | Loss=0.0474 | Val Acc=31.71% | F1=0.125 | AUC=0.424\n",
      "[Arousal (Fold 8)] Ep 23/70 | Loss=0.0507 | Val Acc=70.73% | F1=0.829 | AUC=0.564\n",
      "[Arousal (Fold 8)] Ep 24/70 | Loss=0.0358 | Val Acc=39.02% | F1=0.390 | AUC=0.485\n",
      "[Arousal (Fold 8)] Ep 25/70 | Loss=0.0451 | Val Acc=36.59% | F1=0.381 | AUC=0.491\n",
      "[Arousal (Fold 8)] Ep 26/70 | Loss=0.0312 | Val Acc=68.29% | F1=0.806 | AUC=0.600\n",
      "[Arousal (Fold 8)] Ep 27/70 | Loss=0.0096 | Val Acc=68.29% | F1=0.806 | AUC=0.542\n",
      "[Arousal (Fold 8)] Ep 28/70 | Loss=0.0110 | Val Acc=60.98% | F1=0.750 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 29/70 | Loss=0.0107 | Val Acc=58.54% | F1=0.702 | AUC=0.636\n",
      "[Arousal (Fold 8)] Ep 30/70 | Loss=0.0064 | Val Acc=46.34% | F1=0.577 | AUC=0.545\n",
      "[Arousal (Fold 8)] Ep 31/70 | Loss=0.0092 | Val Acc=70.73% | F1=0.812 | AUC=0.564\n",
      "â¹ [Arousal (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 8)] TEST (best thr=0.500) | Acc=82.93% | F1=0.885 | AUC=0.770\n",
      "\n",
      "Fold 8 Results: Acc=0.8293, F1=0.8852, AUC=0.7697\n",
      "\n",
      "----- Arousal Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 9) | epochs=70\n",
      "[Arousal (Fold 9)] Ep 01/70 | Loss=0.3864 | Val Acc=63.41% | F1=0.769 | AUC=0.639\n",
      "[Arousal (Fold 9)] Ep 02/70 | Loss=0.3609 | Val Acc=41.46% | F1=0.429 | AUC=0.585\n",
      "[Arousal (Fold 9)] Ep 03/70 | Loss=0.3476 | Val Acc=70.73% | F1=0.818 | AUC=0.688\n",
      "[Arousal (Fold 9)] Ep 04/70 | Loss=0.3304 | Val Acc=75.61% | F1=0.844 | AUC=0.661\n",
      "[Arousal (Fold 9)] Ep 05/70 | Loss=0.2893 | Val Acc=68.29% | F1=0.780 | AUC=0.679\n",
      "[Arousal (Fold 9)] Ep 06/70 | Loss=0.2147 | Val Acc=26.83% | F1=0.000 | AUC=0.488\n",
      "[Arousal (Fold 9)] Ep 07/70 | Loss=0.1813 | Val Acc=39.02% | F1=0.419 | AUC=0.533\n",
      "[Arousal (Fold 9)] Ep 08/70 | Loss=0.1119 | Val Acc=24.39% | F1=0.061 | AUC=0.552\n",
      "[Arousal (Fold 9)] Ep 09/70 | Loss=0.0787 | Val Acc=63.41% | F1=0.746 | AUC=0.470\n",
      "[Arousal (Fold 9)] Ep 10/70 | Loss=0.0607 | Val Acc=63.41% | F1=0.727 | AUC=0.609\n",
      "[Arousal (Fold 9)] Ep 11/70 | Loss=0.0393 | Val Acc=70.73% | F1=0.793 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 12/70 | Loss=0.0305 | Val Acc=24.39% | F1=0.000 | AUC=0.618\n",
      "[Arousal (Fold 9)] Ep 13/70 | Loss=0.0326 | Val Acc=24.39% | F1=0.061 | AUC=0.682\n",
      "[Arousal (Fold 9)] Ep 14/70 | Loss=0.0235 | Val Acc=43.90% | F1=0.465 | AUC=0.630\n",
      "[Arousal (Fold 9)] Ep 15/70 | Loss=0.0210 | Val Acc=65.85% | F1=0.767 | AUC=0.545\n",
      "[Arousal (Fold 9)] Ep 16/70 | Loss=0.0146 | Val Acc=73.17% | F1=0.836 | AUC=0.470\n",
      "[Arousal (Fold 9)] Ep 17/70 | Loss=0.0184 | Val Acc=70.73% | F1=0.806 | AUC=0.494\n",
      "[Arousal (Fold 9)] Ep 18/70 | Loss=0.0182 | Val Acc=70.73% | F1=0.800 | AUC=0.550\n",
      "[Arousal (Fold 9)] Ep 19/70 | Loss=0.0151 | Val Acc=70.73% | F1=0.812 | AUC=0.500\n",
      "[Arousal (Fold 9)] Ep 20/70 | Loss=0.0186 | Val Acc=70.73% | F1=0.812 | AUC=0.524\n",
      "[Arousal (Fold 9)] Ep 21/70 | Loss=0.0372 | Val Acc=26.83% | F1=0.118 | AUC=0.636\n",
      "[Arousal (Fold 9)] Ep 22/70 | Loss=0.0797 | Val Acc=26.83% | F1=0.000 | AUC=0.603\n",
      "[Arousal (Fold 9)] Ep 23/70 | Loss=0.0728 | Val Acc=63.41% | F1=0.746 | AUC=0.497\n",
      "[Arousal (Fold 9)] Ep 24/70 | Loss=0.0303 | Val Acc=70.73% | F1=0.829 | AUC=0.374\n",
      "[Arousal (Fold 9)] Ep 25/70 | Loss=0.0227 | Val Acc=70.73% | F1=0.829 | AUC=0.339\n",
      "[Arousal (Fold 9)] Ep 26/70 | Loss=0.0385 | Val Acc=70.73% | F1=0.829 | AUC=0.533\n",
      "[Arousal (Fold 9)] Ep 27/70 | Loss=0.0181 | Val Acc=29.27% | F1=0.121 | AUC=0.718\n",
      "[Arousal (Fold 9)] Ep 28/70 | Loss=0.0185 | Val Acc=73.17% | F1=0.831 | AUC=0.515\n",
      "[Arousal (Fold 9)] Ep 29/70 | Loss=0.0160 | Val Acc=70.73% | F1=0.829 | AUC=0.411\n",
      "[Arousal (Fold 9)] Ep 30/70 | Loss=0.0038 | Val Acc=70.73% | F1=0.829 | AUC=0.408\n",
      "[Arousal (Fold 9)] Ep 31/70 | Loss=0.0052 | Val Acc=60.98% | F1=0.692 | AUC=0.585\n",
      "â¹ [Arousal (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 9)] TEST (best thr=0.100) | Acc=53.66% | F1=0.627 | AUC=0.509\n",
      "\n",
      "Fold 9 Results: Acc=0.5366, F1=0.6275, AUC=0.5091\n",
      "\n",
      "----- Arousal Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 10) | epochs=70\n",
      "[Arousal (Fold 10)] Ep 01/70 | Loss=0.3798 | Val Acc=34.15% | F1=0.182 | AUC=0.667\n",
      "[Arousal (Fold 10)] Ep 02/70 | Loss=0.3684 | Val Acc=36.59% | F1=0.350 | AUC=0.558\n",
      "[Arousal (Fold 10)] Ep 03/70 | Loss=0.3451 | Val Acc=43.90% | F1=0.439 | AUC=0.476\n",
      "[Arousal (Fold 10)] Ep 04/70 | Loss=0.3158 | Val Acc=34.15% | F1=0.229 | AUC=0.464\n",
      "[Arousal (Fold 10)] Ep 05/70 | Loss=0.2671 | Val Acc=68.29% | F1=0.812 | AUC=0.527\n",
      "[Arousal (Fold 10)] Ep 06/70 | Loss=0.2049 | Val Acc=63.41% | F1=0.776 | AUC=0.430\n",
      "[Arousal (Fold 10)] Ep 07/70 | Loss=0.1413 | Val Acc=29.27% | F1=0.065 | AUC=0.691\n",
      "[Arousal (Fold 10)] Ep 08/70 | Loss=0.1205 | Val Acc=58.54% | F1=0.721 | AUC=0.421\n",
      "[Arousal (Fold 10)] Ep 09/70 | Loss=0.0811 | Val Acc=26.83% | F1=0.000 | AUC=0.827\n",
      "[Arousal (Fold 10)] Ep 10/70 | Loss=0.0552 | Val Acc=68.29% | F1=0.812 | AUC=0.339\n",
      "[Arousal (Fold 10)] Ep 11/70 | Loss=0.0441 | Val Acc=70.73% | F1=0.829 | AUC=0.267\n",
      "[Arousal (Fold 10)] Ep 12/70 | Loss=0.0381 | Val Acc=60.98% | F1=0.750 | AUC=0.388\n",
      "[Arousal (Fold 10)] Ep 13/70 | Loss=0.0286 | Val Acc=65.85% | F1=0.781 | AUC=0.442\n",
      "[Arousal (Fold 10)] Ep 14/70 | Loss=0.0195 | Val Acc=65.85% | F1=0.767 | AUC=0.494\n",
      "[Arousal (Fold 10)] Ep 15/70 | Loss=0.0204 | Val Acc=65.85% | F1=0.788 | AUC=0.288\n",
      "[Arousal (Fold 10)] Ep 16/70 | Loss=0.0152 | Val Acc=65.85% | F1=0.781 | AUC=0.433\n",
      "[Arousal (Fold 10)] Ep 17/70 | Loss=0.0160 | Val Acc=58.54% | F1=0.702 | AUC=0.555\n",
      "[Arousal (Fold 10)] Ep 18/70 | Loss=0.0171 | Val Acc=70.73% | F1=0.806 | AUC=0.491\n",
      "[Arousal (Fold 10)] Ep 19/70 | Loss=0.0117 | Val Acc=63.41% | F1=0.746 | AUC=0.503\n",
      "[Arousal (Fold 10)] Ep 20/70 | Loss=0.0128 | Val Acc=68.29% | F1=0.787 | AUC=0.500\n",
      "[Arousal (Fold 10)] Ep 21/70 | Loss=0.0649 | Val Acc=31.71% | F1=0.125 | AUC=0.539\n",
      "[Arousal (Fold 10)] Ep 22/70 | Loss=0.0817 | Val Acc=26.83% | F1=0.000 | AUC=0.761\n",
      "[Arousal (Fold 10)] Ep 23/70 | Loss=0.0363 | Val Acc=68.29% | F1=0.812 | AUC=0.177\n",
      "[Arousal (Fold 10)] Ep 24/70 | Loss=0.0360 | Val Acc=41.46% | F1=0.333 | AUC=0.664\n",
      "[Arousal (Fold 10)] Ep 25/70 | Loss=0.0323 | Val Acc=65.85% | F1=0.794 | AUC=0.318\n",
      "[Arousal (Fold 10)] Ep 26/70 | Loss=0.0250 | Val Acc=34.15% | F1=0.182 | AUC=0.809\n",
      "[Arousal (Fold 10)] Ep 27/70 | Loss=0.0335 | Val Acc=26.83% | F1=0.000 | AUC=0.776\n",
      "[Arousal (Fold 10)] Ep 28/70 | Loss=0.0136 | Val Acc=68.29% | F1=0.800 | AUC=0.397\n",
      "[Arousal (Fold 10)] Ep 29/70 | Loss=0.0109 | Val Acc=70.73% | F1=0.829 | AUC=0.332\n",
      "[Arousal (Fold 10)] Ep 30/70 | Loss=0.0044 | Val Acc=70.73% | F1=0.824 | AUC=0.341\n",
      "[Arousal (Fold 10)] Ep 31/70 | Loss=0.0029 | Val Acc=60.98% | F1=0.733 | AUC=0.436\n",
      "â¹ [Arousal (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 10)] TEST (best thr=0.100) | Acc=75.61% | F1=0.839 | AUC=0.680\n",
      "\n",
      "Fold 10 Results: Acc=0.7561, F1=0.8387, AUC=0.6803\n",
      "\n",
      "===== FINAL Arousal 10-FOLD RESULTS =====\n",
      "Accuracy: 0.6328 Â± 0.1409\n",
      "F1-score: 0.6995 Â± 0.1943\n",
      "AUC:      0.5664 Â± 0.0970\n",
      "\n",
      "===== OVERALL SUMMARY =====\n",
      "Valence Acc: mean=0.5991, std=0.0349\n",
      "Arousal Acc: mean=0.6328, std=0.1409\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# CNN-only on READY-MADE SPECTROGRAMS\n",
    "# (No spectrogram conversion in this file)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------- DEVICE -------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Dataset wrapper (X already = spectrograms)\n",
    "# -------------------------------------------------\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.asarray(X, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]).float(),\n",
    "            torch.tensor(self.y[idx]).float(),\n",
    "        )\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Augmentations: SpecAugment + noise\n",
    "# (same as your original)\n",
    "# -------------------------------------------------\n",
    "def spec_augment(\n",
    "    X,\n",
    "    time_mask_width=4,\n",
    "    n_time_masks=2,\n",
    "    freq_mask_width=6,\n",
    "    n_freq_masks=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    X: [N, C, F, T] spectrograms\n",
    "    Applies random time & frequency masks (SpecAugment style).\n",
    "    \"\"\"\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "\n",
    "    for i in range(N):\n",
    "        # Time masks\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T:\n",
    "                continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "\n",
    "        # Frequency masks\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F:\n",
    "                continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. CNN-only model (non-temporal sequence model)\n",
    "# -------------------------------------------------\n",
    "class EEG_CNN_only(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-only model:\n",
    "      - several Conv2d blocks producing [B, 128, F', T']\n",
    "      - global pooling (adaptive avg) -> [B, 128]\n",
    "      - small MLP -> logit\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=14, hidden_feat=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # reduce spatial dims\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, hidden_feat, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_feat),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # -> [B, hidden_feat, 1, 1]\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_feat),\n",
    "            nn.Linear(hidden_feat, hidden_feat // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_feat // 2, 1),  # single logit\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, F, T]\n",
    "        x = self.cnn(x)                 # [B, hidden_feat, 1, 1]\n",
    "        x = x.view(x.size(0), -1)       # [B, hidden_feat]\n",
    "        out = self.classifier(x).squeeze(-1)  # [B]\n",
    "        return out\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Train ONE fold for one emotion\n",
    "# (keeps same behavior as your original)\n",
    "# -------------------------------------------------\n",
    "def train_one_fold_emotion(\n",
    "    X,\n",
    "    y_cont,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    emotion_name=\"Valence\",\n",
    "    epochs=70,\n",
    "    base_seed=42,\n",
    "    batch_size=16,\n",
    "):\n",
    "    # 1) Binarize labels w.r.t. TRAIN median\n",
    "    y_train_cont = y_cont[train_idx]\n",
    "    thr = np.median(y_train_cont)\n",
    "\n",
    "    y_train_bin = (y_train_cont >= thr).astype(float)\n",
    "    y_val_bin   = (y_cont[val_idx]  >= thr).astype(float)\n",
    "    y_test_bin  = (y_cont[test_idx] >= thr).astype(float)\n",
    "\n",
    "    print(f\"\\n[{emotion_name}] TRAIN median threshold = {thr:.4f}\")\n",
    "    print(\"  Train class counts:\", np.bincount(y_train_bin.astype(int)))\n",
    "    print(\"  Val   class counts:\", np.bincount(y_val_bin.astype(int)))\n",
    "    print(\"  Test  class counts:\", np.bincount(y_test_bin.astype(int)))\n",
    "\n",
    "    # 2) Standardize features using TRAIN only\n",
    "    X = np.nan_to_num(X)\n",
    "    X_train = X[train_idx]\n",
    "    X_val   = X[val_idx]\n",
    "    X_test  = X[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(len(train_idx), -1)\n",
    "    scaler.fit(X_train_flat)\n",
    "\n",
    "    def transform_block(X_block):\n",
    "        flat = X_block.reshape(X_block.shape[0], -1)\n",
    "        flat_scaled = scaler.transform(flat)\n",
    "        return flat_scaled.reshape(X_block.shape)\n",
    "\n",
    "    X_train_scaled = transform_block(X_train)\n",
    "    X_val_scaled   = transform_block(X_val)\n",
    "    X_test_scaled  = transform_block(X_test)\n",
    "\n",
    "    # 3) Strong augmentation: SpecAugment + noise\n",
    "    X_train_sa    = spec_augment(X_train_scaled)\n",
    "    X_train_noise = add_noise(X_train_scaled, std=0.03)\n",
    "\n",
    "    X_train_aug = np.concatenate(\n",
    "        [X_train_scaled, X_train_sa, X_train_noise],\n",
    "        axis=0,\n",
    "    )\n",
    "    y_train_aug = np.concatenate(\n",
    "        [y_train_bin, y_train_bin, y_train_bin],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # 4) DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        EEGDataset(X_train_aug, y_train_aug),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        EEGDataset(X_val_scaled, y_val_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        EEGDataset(X_test_scaled, y_test_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # 5) Model + optimizer + CLASS-WEIGHTED loss\n",
    "    torch.manual_seed(base_seed)\n",
    "    random.seed(base_seed)\n",
    "    np.random.seed(base_seed)\n",
    "\n",
    "    n_channels = X.shape[1]\n",
    "    model = EEG_CNN_only(n_channels=n_channels, hidden_feat=128).to(DEVICE)\n",
    "\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts) > 0 else 1\n",
    "    pos = class_counts[1] if len(class_counts) > 1 else 1\n",
    "    pos_weight_val = float(neg) / float(pos) if pos > 0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    print(f\"[{emotion_name}] pos_weight for BCEWithLogitsLoss = {pos_weight_val:.3f}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "    patience = 18\n",
    "    counter = 0\n",
    "\n",
    "    print(f\"\\nðŸš€ Training {emotion_name} | epochs={epochs}\")\n",
    "    for ep in range(1, epochs + 1):\n",
    "        # ---- train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            n_samples += xb.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss = total_loss / max(1, n_samples)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        y_true_val, y_prob_val = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy())\n",
    "                y_prob_val.extend(probs)\n",
    "\n",
    "        y_true_val = np.array(y_true_val)\n",
    "        y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        val_f1  = f1_score(y_true_val, y_pred_val)\n",
    "        try:\n",
    "            val_auc = roc_auc_score(y_true_val, y_prob_val)\n",
    "        except:\n",
    "            val_auc = float(\"nan\")\n",
    "\n",
    "        print(\n",
    "            f\"[{emotion_name}] Ep {ep:02d}/{epochs} | \"\n",
    "            f\"Loss={train_loss:.4f} | Val Acc={val_acc*100:.2f}% | \"\n",
    "            f\"F1={val_f1:.3f} | AUC={val_auc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # early stopping on val accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience:\n",
    "                print(f\"â¹ [{emotion_name}] Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    # ---- load best & evaluate on TEST (with best threshold) ----\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    y_true_test, y_prob_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy())\n",
    "            y_prob_test.extend(probs)\n",
    "\n",
    "    y_true_test = np.array(y_true_test)\n",
    "    y_prob_test = np.array(y_prob_test)\n",
    "\n",
    "    # search threshold for best TEST accuracy\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for thr_ in np.linspace(0.1, 0.9, 17):\n",
    "        yp = (y_prob_test >= thr_).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc = acc_thr\n",
    "            best_thr = thr_\n",
    "\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1  = f1_score(y_true_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nðŸ”š [{emotion_name}] TEST (best thr={best_thr:.3f}) \"\n",
    "        f\"| Acc={test_acc*100:.2f}% | F1={test_f1:.3f} | AUC={test_auc:.3f}\\n\"\n",
    "    )\n",
    "\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. 10-fold CV driver\n",
    "# (same as your original)\n",
    "# -------------------------------------------------\n",
    "def run_10fold_cv_emotion(X, y_cont, emotion_name=\"Valence\", epochs=70, base_seed=42):\n",
    "    \"\"\"\n",
    "    10-fold stratified CV:\n",
    "      - Outer fold chooses TEST.\n",
    "      - Remaining data is split into TRAIN / VAL\n",
    "        so that VAL size â‰ˆ TEST size.\n",
    "    \"\"\"\n",
    "    print(f\"\\n########## {emotion_name}: 10-fold STRATIFIED CV ##########\")\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "\n",
    "    # For stratification: global median threshold\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    print(f\"{emotion_name} global median (for stratification) = {global_thr:.4f}\")\n",
    "    print(\"Class counts:\", np.bincount(y_bin_global.astype(int)))\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=10,\n",
    "        shuffle=True,\n",
    "        random_state=base_seed,\n",
    "    )\n",
    "\n",
    "    fold_accs, fold_f1s, fold_aucs = [], [], []\n",
    "\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(\n",
    "        skf.split(np.arange(len(y_cont)), y_bin_global),\n",
    "        start=1\n",
    "    ):\n",
    "        # Split trainval into train & val with val size ~= test size\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "\n",
    "        tv_train_idx, tv_val_idx = train_test_split(\n",
    "            trainval_idx,\n",
    "            test_size=val_frac,\n",
    "            random_state=base_seed + fold,\n",
    "            shuffle=True,\n",
    "            stratify=y_trainval_bin,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n----- {emotion_name} Fold {fold}/10 -----\")\n",
    "        print(\n",
    "            f\"Train size={len(tv_train_idx)}, \"\n",
    "            f\"Val size={len(tv_val_idx)}, \"\n",
    "            f\"Test size={len(test_idx)}\"\n",
    "        )\n",
    "\n",
    "        acc, f1, auc = train_one_fold_emotion(\n",
    "            X,\n",
    "            y_cont,\n",
    "            tv_train_idx,\n",
    "            tv_val_idx,\n",
    "            test_idx,\n",
    "            emotion_name=f\"{emotion_name} (Fold {fold})\",\n",
    "            epochs=epochs,\n",
    "            base_seed=base_seed + fold,\n",
    "            batch_size=16,\n",
    "        )\n",
    "\n",
    "        fold_accs.append(acc)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_aucs.append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold} Results: \"\n",
    "            f\"Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\"\n",
    "        )\n",
    "\n",
    "    fold_accs = np.array(fold_accs)\n",
    "    fold_f1s  = np.array(fold_f1s)\n",
    "    fold_aucs = np.array(fold_aucs)\n",
    "\n",
    "    print(f\"\\n===== FINAL {emotion_name} 10-FOLD RESULTS =====\")\n",
    "    print(f\"Accuracy: {fold_accs.mean():.4f} Â± {fold_accs.std():.4f}\")\n",
    "    print(f\"F1-score: {fold_f1s.mean():.4f} Â± {fold_f1s.std():.4f}\")\n",
    "    print(f\"AUC:      {fold_aucs.mean():.4f} Â± {fold_aucs.std():.4f}\")\n",
    "\n",
    "    return fold_accs, fold_f1s, fold_aucs\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. MAIN (assumes X, y_val, y_aro already created)\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure you've already run the cell that does:\n",
    "    # X, y_val, y_aro, groups, channels = load_dreamer_and_build_spectrograms(...)\n",
    "    print(\"\\nâœ… Spectrogram dataset:\", X.shape)\n",
    "\n",
    "    # Valence\n",
    "    val_accs, val_f1s, val_aucs = run_10fold_cv_emotion(\n",
    "        X, y_val, emotion_name=\"Valence\", epochs=70, base_seed=42\n",
    "    )\n",
    "\n",
    "    # Arousal\n",
    "    aro_accs, aro_f1s, aro_aucs = run_10fold_cv_emotion(\n",
    "        X, y_aro, emotion_name=\"Arousal\", epochs=70, base_seed=142\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== OVERALL SUMMARY =====\")\n",
    "    print(f\"Valence Acc: mean={val_accs.mean():.4f}, std={val_accs.std():.4f}\")\n",
    "    print(f\"Arousal Acc: mean={aro_accs.mean():.4f}, std={aro_accs.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d59a1ad-ba3a-4c78-a46f-230d78e3b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa420c32-ca72-4c87-8172-5001443b896f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "âœ… Spectrogram dataset: (414, 14, 36, 32)\n",
      "\n",
      "########## Valence: 10-fold STRATIFIED CV ##########\n",
      "Valence global median (for stratification) = 3.0000\n",
      "Class counts: [161 253]\n",
      "\n",
      "----- Valence Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 1) | epochs=70\n",
      "[Valence (Fold 1)] Ep 01/70 | Loss=0.5438 | Val Acc=52.38% | F1=0.565 | AUC=0.584\n",
      "[Valence (Fold 1)] Ep 02/70 | Loss=0.5285 | Val Acc=47.62% | F1=0.476 | AUC=0.553\n",
      "[Valence (Fold 1)] Ep 03/70 | Loss=0.5041 | Val Acc=47.62% | F1=0.450 | AUC=0.560\n",
      "[Valence (Fold 1)] Ep 04/70 | Loss=0.4713 | Val Acc=50.00% | F1=0.488 | AUC=0.536\n",
      "[Valence (Fold 1)] Ep 05/70 | Loss=0.4028 | Val Acc=47.62% | F1=0.500 | AUC=0.546\n",
      "[Valence (Fold 1)] Ep 06/70 | Loss=0.2911 | Val Acc=40.48% | F1=0.359 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 07/70 | Loss=0.2344 | Val Acc=40.48% | F1=0.074 | AUC=0.543\n",
      "[Valence (Fold 1)] Ep 08/70 | Loss=0.1844 | Val Acc=61.90% | F1=0.765 | AUC=0.337\n",
      "[Valence (Fold 1)] Ep 09/70 | Loss=0.1330 | Val Acc=42.86% | F1=0.250 | AUC=0.596\n",
      "[Valence (Fold 1)] Ep 10/70 | Loss=0.0921 | Val Acc=54.76% | F1=0.655 | AUC=0.486\n",
      "[Valence (Fold 1)] Ep 11/70 | Loss=0.0796 | Val Acc=61.90% | F1=0.765 | AUC=0.553\n",
      "[Valence (Fold 1)] Ep 12/70 | Loss=0.0633 | Val Acc=61.90% | F1=0.758 | AUC=0.466\n",
      "[Valence (Fold 1)] Ep 13/70 | Loss=0.0382 | Val Acc=45.24% | F1=0.410 | AUC=0.478\n",
      "[Valence (Fold 1)] Ep 14/70 | Loss=0.0277 | Val Acc=47.62% | F1=0.577 | AUC=0.469\n",
      "[Valence (Fold 1)] Ep 15/70 | Loss=0.0267 | Val Acc=50.00% | F1=0.644 | AUC=0.447\n",
      "[Valence (Fold 1)] Ep 16/70 | Loss=0.0158 | Val Acc=50.00% | F1=0.604 | AUC=0.483\n",
      "[Valence (Fold 1)] Ep 17/70 | Loss=0.0151 | Val Acc=50.00% | F1=0.588 | AUC=0.505\n",
      "[Valence (Fold 1)] Ep 18/70 | Loss=0.0219 | Val Acc=50.00% | F1=0.533 | AUC=0.505\n",
      "[Valence (Fold 1)] Ep 19/70 | Loss=0.0195 | Val Acc=47.62% | F1=0.577 | AUC=0.495\n",
      "[Valence (Fold 1)] Ep 20/70 | Loss=0.0144 | Val Acc=50.00% | F1=0.588 | AUC=0.502\n",
      "[Valence (Fold 1)] Ep 21/70 | Loss=0.0622 | Val Acc=50.00% | F1=0.533 | AUC=0.529\n",
      "[Valence (Fold 1)] Ep 22/70 | Loss=0.0776 | Val Acc=57.14% | F1=0.654 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 23/70 | Loss=0.1385 | Val Acc=59.52% | F1=0.738 | AUC=0.505\n",
      "[Valence (Fold 1)] Ep 24/70 | Loss=0.0277 | Val Acc=52.38% | F1=0.500 | AUC=0.606\n",
      "[Valence (Fold 1)] Ep 25/70 | Loss=0.0366 | Val Acc=61.90% | F1=0.742 | AUC=0.567\n",
      "[Valence (Fold 1)] Ep 26/70 | Loss=0.0340 | Val Acc=38.10% | F1=0.133 | AUC=0.498\n",
      "[Valence (Fold 1)] Ep 27/70 | Loss=0.0193 | Val Acc=57.14% | F1=0.690 | AUC=0.565\n",
      "[Valence (Fold 1)] Ep 28/70 | Loss=0.0040 | Val Acc=64.29% | F1=0.762 | AUC=0.583\n",
      "[Valence (Fold 1)] Ep 29/70 | Loss=0.0135 | Val Acc=52.38% | F1=0.583 | AUC=0.500\n",
      "[Valence (Fold 1)] Ep 30/70 | Loss=0.0097 | Val Acc=66.67% | F1=0.774 | AUC=0.541\n",
      "[Valence (Fold 1)] Ep 31/70 | Loss=0.0159 | Val Acc=59.52% | F1=0.738 | AUC=0.445\n",
      "[Valence (Fold 1)] Ep 32/70 | Loss=0.0136 | Val Acc=57.14% | F1=0.700 | AUC=0.510\n",
      "[Valence (Fold 1)] Ep 33/70 | Loss=0.0096 | Val Acc=52.38% | F1=0.600 | AUC=0.469\n",
      "[Valence (Fold 1)] Ep 34/70 | Loss=0.0020 | Val Acc=52.38% | F1=0.655 | AUC=0.488\n",
      "[Valence (Fold 1)] Ep 35/70 | Loss=0.0019 | Val Acc=47.62% | F1=0.542 | AUC=0.459\n",
      "[Valence (Fold 1)] Ep 36/70 | Loss=0.0024 | Val Acc=52.38% | F1=0.615 | AUC=0.469\n",
      "[Valence (Fold 1)] Ep 37/70 | Loss=0.0023 | Val Acc=52.38% | F1=0.630 | AUC=0.486\n",
      "[Valence (Fold 1)] Ep 38/70 | Loss=0.0022 | Val Acc=47.62% | F1=0.542 | AUC=0.484\n",
      "[Valence (Fold 1)] Ep 39/70 | Loss=0.0026 | Val Acc=47.62% | F1=0.560 | AUC=0.486\n",
      "[Valence (Fold 1)] Ep 40/70 | Loss=0.0029 | Val Acc=47.62% | F1=0.560 | AUC=0.486\n",
      "[Valence (Fold 1)] Ep 41/70 | Loss=0.0015 | Val Acc=50.00% | F1=0.571 | AUC=0.483\n",
      "[Valence (Fold 1)] Ep 42/70 | Loss=0.0012 | Val Acc=54.76% | F1=0.642 | AUC=0.488\n",
      "[Valence (Fold 1)] Ep 43/70 | Loss=0.0187 | Val Acc=57.14% | F1=0.625 | AUC=0.500\n",
      "[Valence (Fold 1)] Ep 44/70 | Loss=0.0999 | Val Acc=61.90% | F1=0.765 | AUC=0.587\n",
      "[Valence (Fold 1)] Ep 45/70 | Loss=0.0616 | Val Acc=61.90% | F1=0.765 | AUC=0.502\n",
      "[Valence (Fold 1)] Ep 46/70 | Loss=0.0422 | Val Acc=40.48% | F1=0.194 | AUC=0.526\n",
      "[Valence (Fold 1)] Ep 47/70 | Loss=0.0262 | Val Acc=52.38% | F1=0.545 | AUC=0.567\n",
      "[Valence (Fold 1)] Ep 48/70 | Loss=0.0098 | Val Acc=66.67% | F1=0.741 | AUC=0.577\n",
      "â¹ [Valence (Fold 1)] Early stopping at epoch 48\n",
      "\n",
      "ðŸ”š [Valence (Fold 1)] TEST (best thr=0.100) | Acc=66.67% | F1=0.767 | AUC=0.558\n",
      "\n",
      "Fold 1 Results: Acc=0.6667, F1=0.7667, AUC=0.5577\n",
      "\n",
      "----- Valence Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 2) | epochs=70\n",
      "[Valence (Fold 2)] Ep 01/70 | Loss=0.5334 | Val Acc=47.62% | F1=0.389 | AUC=0.507\n",
      "[Valence (Fold 2)] Ep 02/70 | Loss=0.5154 | Val Acc=59.52% | F1=0.712 | AUC=0.483\n",
      "[Valence (Fold 2)] Ep 03/70 | Loss=0.4661 | Val Acc=47.62% | F1=0.476 | AUC=0.478\n",
      "[Valence (Fold 2)] Ep 04/70 | Loss=0.3995 | Val Acc=59.52% | F1=0.746 | AUC=0.423\n",
      "[Valence (Fold 2)] Ep 05/70 | Loss=0.2874 | Val Acc=45.24% | F1=0.465 | AUC=0.478\n",
      "[Valence (Fold 2)] Ep 06/70 | Loss=0.2234 | Val Acc=57.14% | F1=0.690 | AUC=0.425\n",
      "[Valence (Fold 2)] Ep 07/70 | Loss=0.1385 | Val Acc=40.48% | F1=0.138 | AUC=0.577\n",
      "[Valence (Fold 2)] Ep 08/70 | Loss=0.1289 | Val Acc=54.76% | F1=0.708 | AUC=0.421\n",
      "[Valence (Fold 2)] Ep 09/70 | Loss=0.0788 | Val Acc=42.86% | F1=0.455 | AUC=0.522\n",
      "[Valence (Fold 2)] Ep 10/70 | Loss=0.0633 | Val Acc=40.48% | F1=0.444 | AUC=0.486\n",
      "[Valence (Fold 2)] Ep 11/70 | Loss=0.0325 | Val Acc=50.00% | F1=0.432 | AUC=0.507\n",
      "[Valence (Fold 2)] Ep 12/70 | Loss=0.0385 | Val Acc=47.62% | F1=0.476 | AUC=0.546\n",
      "[Valence (Fold 2)] Ep 13/70 | Loss=0.0310 | Val Acc=59.52% | F1=0.746 | AUC=0.394\n",
      "[Valence (Fold 2)] Ep 14/70 | Loss=0.0212 | Val Acc=59.52% | F1=0.730 | AUC=0.385\n",
      "[Valence (Fold 2)] Ep 15/70 | Loss=0.0190 | Val Acc=52.38% | F1=0.565 | AUC=0.507\n",
      "[Valence (Fold 2)] Ep 16/70 | Loss=0.0163 | Val Acc=50.00% | F1=0.604 | AUC=0.450\n",
      "[Valence (Fold 2)] Ep 17/70 | Loss=0.0127 | Val Acc=47.62% | F1=0.577 | AUC=0.457\n",
      "[Valence (Fold 2)] Ep 18/70 | Loss=0.0147 | Val Acc=42.86% | F1=0.478 | AUC=0.500\n",
      "[Valence (Fold 2)] Ep 19/70 | Loss=0.0128 | Val Acc=42.86% | F1=0.520 | AUC=0.457\n",
      "[Valence (Fold 2)] Ep 20/70 | Loss=0.0116 | Val Acc=45.24% | F1=0.549 | AUC=0.457\n",
      "[Valence (Fold 2)] Ep 21/70 | Loss=0.0669 | Val Acc=45.24% | F1=0.410 | AUC=0.471\n",
      "[Valence (Fold 2)] Ep 22/70 | Loss=0.0708 | Val Acc=57.14% | F1=0.700 | AUC=0.505\n",
      "[Valence (Fold 2)] Ep 23/70 | Loss=0.0424 | Val Acc=59.52% | F1=0.585 | AUC=0.613\n",
      "[Valence (Fold 2)] Ep 24/70 | Loss=0.0628 | Val Acc=40.48% | F1=0.242 | AUC=0.567\n",
      "[Valence (Fold 2)] Ep 25/70 | Loss=0.0538 | Val Acc=57.14% | F1=0.727 | AUC=0.365\n",
      "[Valence (Fold 2)] Ep 26/70 | Loss=0.0200 | Val Acc=52.38% | F1=0.630 | AUC=0.421\n",
      "[Valence (Fold 2)] Ep 27/70 | Loss=0.0191 | Val Acc=45.24% | F1=0.258 | AUC=0.538\n",
      "[Valence (Fold 2)] Ep 28/70 | Loss=0.0284 | Val Acc=45.24% | F1=0.343 | AUC=0.517\n",
      "[Valence (Fold 2)] Ep 29/70 | Loss=0.0165 | Val Acc=45.24% | F1=0.258 | AUC=0.553\n",
      "[Valence (Fold 2)] Ep 30/70 | Loss=0.0133 | Val Acc=57.14% | F1=0.719 | AUC=0.413\n",
      "[Valence (Fold 2)] Ep 31/70 | Loss=0.0062 | Val Acc=45.24% | F1=0.465 | AUC=0.514\n",
      "â¹ [Valence (Fold 2)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 2)] TEST (best thr=0.200) | Acc=57.14% | F1=0.609 | AUC=0.546\n",
      "\n",
      "Fold 2 Results: Acc=0.5714, F1=0.6087, AUC=0.5457\n",
      "\n",
      "----- Valence Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 3) | epochs=70\n",
      "[Valence (Fold 3)] Ep 01/70 | Loss=0.5373 | Val Acc=57.14% | F1=0.690 | AUC=0.404\n",
      "[Valence (Fold 3)] Ep 02/70 | Loss=0.5234 | Val Acc=57.14% | F1=0.690 | AUC=0.462\n",
      "[Valence (Fold 3)] Ep 03/70 | Loss=0.4978 | Val Acc=42.86% | F1=0.200 | AUC=0.546\n",
      "[Valence (Fold 3)] Ep 04/70 | Loss=0.4658 | Val Acc=50.00% | F1=0.512 | AUC=0.507\n",
      "[Valence (Fold 3)] Ep 05/70 | Loss=0.3736 | Val Acc=66.67% | F1=0.781 | AUC=0.481\n",
      "[Valence (Fold 3)] Ep 06/70 | Loss=0.2755 | Val Acc=61.90% | F1=0.765 | AUC=0.416\n",
      "[Valence (Fold 3)] Ep 07/70 | Loss=0.1838 | Val Acc=52.38% | F1=0.600 | AUC=0.529\n",
      "[Valence (Fold 3)] Ep 08/70 | Loss=0.1326 | Val Acc=61.90% | F1=0.758 | AUC=0.406\n",
      "[Valence (Fold 3)] Ep 09/70 | Loss=0.1038 | Val Acc=61.90% | F1=0.758 | AUC=0.423\n",
      "[Valence (Fold 3)] Ep 10/70 | Loss=0.0645 | Val Acc=61.90% | F1=0.750 | AUC=0.385\n",
      "[Valence (Fold 3)] Ep 11/70 | Loss=0.0558 | Val Acc=45.24% | F1=0.303 | AUC=0.502\n",
      "[Valence (Fold 3)] Ep 12/70 | Loss=0.0414 | Val Acc=38.10% | F1=0.435 | AUC=0.416\n",
      "[Valence (Fold 3)] Ep 13/70 | Loss=0.0295 | Val Acc=59.52% | F1=0.721 | AUC=0.334\n",
      "[Valence (Fold 3)] Ep 14/70 | Loss=0.0253 | Val Acc=47.62% | F1=0.607 | AUC=0.358\n",
      "[Valence (Fold 3)] Ep 15/70 | Loss=0.0209 | Val Acc=45.24% | F1=0.566 | AUC=0.358\n",
      "[Valence (Fold 3)] Ep 16/70 | Loss=0.0168 | Val Acc=45.24% | F1=0.582 | AUC=0.377\n",
      "[Valence (Fold 3)] Ep 17/70 | Loss=0.0154 | Val Acc=42.86% | F1=0.538 | AUC=0.385\n",
      "[Valence (Fold 3)] Ep 18/70 | Loss=0.0150 | Val Acc=45.24% | F1=0.566 | AUC=0.397\n",
      "[Valence (Fold 3)] Ep 19/70 | Loss=0.0147 | Val Acc=40.48% | F1=0.510 | AUC=0.399\n",
      "[Valence (Fold 3)] Ep 20/70 | Loss=0.0142 | Val Acc=42.86% | F1=0.538 | AUC=0.394\n",
      "[Valence (Fold 3)] Ep 21/70 | Loss=0.0188 | Val Acc=47.62% | F1=0.389 | AUC=0.454\n",
      "[Valence (Fold 3)] Ep 22/70 | Loss=0.0375 | Val Acc=42.86% | F1=0.143 | AUC=0.589\n",
      "[Valence (Fold 3)] Ep 23/70 | Loss=0.0980 | Val Acc=40.48% | F1=0.074 | AUC=0.423\n",
      "[Valence (Fold 3)] Ep 24/70 | Loss=0.0752 | Val Acc=45.24% | F1=0.207 | AUC=0.596\n",
      "[Valence (Fold 3)] Ep 25/70 | Loss=0.0509 | Val Acc=59.52% | F1=0.738 | AUC=0.286\n",
      "[Valence (Fold 3)] Ep 26/70 | Loss=0.0378 | Val Acc=52.38% | F1=0.500 | AUC=0.462\n",
      "[Valence (Fold 3)] Ep 27/70 | Loss=0.0179 | Val Acc=45.24% | F1=0.566 | AUC=0.411\n",
      "[Valence (Fold 3)] Ep 28/70 | Loss=0.0191 | Val Acc=40.48% | F1=0.074 | AUC=0.594\n",
      "[Valence (Fold 3)] Ep 29/70 | Loss=0.0106 | Val Acc=54.76% | F1=0.689 | AUC=0.288\n",
      "[Valence (Fold 3)] Ep 30/70 | Loss=0.0190 | Val Acc=45.24% | F1=0.511 | AUC=0.413\n",
      "[Valence (Fold 3)] Ep 31/70 | Loss=0.0133 | Val Acc=52.38% | F1=0.655 | AUC=0.452\n",
      "â¹ [Valence (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 3)] TEST (best thr=0.100) | Acc=52.38% | F1=0.667 | AUC=0.560\n",
      "\n",
      "Fold 3 Results: Acc=0.5238, F1=0.6667, AUC=0.5601\n",
      "\n",
      "----- Valence Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [128 202]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [17 25]\n",
      "[Valence (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.634\n",
      "\n",
      "ðŸš€ Training Valence (Fold 4) | epochs=70\n",
      "[Valence (Fold 4)] Ep 01/70 | Loss=0.5346 | Val Acc=57.14% | F1=0.719 | AUC=0.632\n",
      "[Valence (Fold 4)] Ep 02/70 | Loss=0.5024 | Val Acc=59.52% | F1=0.738 | AUC=0.421\n",
      "[Valence (Fold 4)] Ep 03/70 | Loss=0.4643 | Val Acc=40.48% | F1=0.194 | AUC=0.464\n",
      "[Valence (Fold 4)] Ep 04/70 | Loss=0.4108 | Val Acc=42.86% | F1=0.250 | AUC=0.495\n",
      "[Valence (Fold 4)] Ep 05/70 | Loss=0.3169 | Val Acc=38.10% | F1=0.000 | AUC=0.481\n",
      "[Valence (Fold 4)] Ep 06/70 | Loss=0.2331 | Val Acc=45.24% | F1=0.378 | AUC=0.546\n",
      "[Valence (Fold 4)] Ep 07/70 | Loss=0.1576 | Val Acc=64.29% | F1=0.776 | AUC=0.536\n",
      "[Valence (Fold 4)] Ep 08/70 | Loss=0.1069 | Val Acc=42.86% | F1=0.294 | AUC=0.560\n",
      "[Valence (Fold 4)] Ep 09/70 | Loss=0.0738 | Val Acc=38.10% | F1=0.000 | AUC=0.502\n",
      "[Valence (Fold 4)] Ep 10/70 | Loss=0.0894 | Val Acc=45.24% | F1=0.439 | AUC=0.450\n",
      "[Valence (Fold 4)] Ep 11/70 | Loss=0.0465 | Val Acc=61.90% | F1=0.724 | AUC=0.587\n",
      "[Valence (Fold 4)] Ep 12/70 | Loss=0.0478 | Val Acc=52.38% | F1=0.524 | AUC=0.529\n",
      "[Valence (Fold 4)] Ep 13/70 | Loss=0.0408 | Val Acc=57.14% | F1=0.625 | AUC=0.510\n",
      "[Valence (Fold 4)] Ep 14/70 | Loss=0.0267 | Val Acc=61.90% | F1=0.758 | AUC=0.546\n",
      "[Valence (Fold 4)] Ep 15/70 | Loss=0.0222 | Val Acc=40.48% | F1=0.286 | AUC=0.457\n",
      "[Valence (Fold 4)] Ep 16/70 | Loss=0.0210 | Val Acc=52.38% | F1=0.524 | AUC=0.514\n",
      "[Valence (Fold 4)] Ep 17/70 | Loss=0.0198 | Val Acc=57.14% | F1=0.710 | AUC=0.546\n",
      "[Valence (Fold 4)] Ep 18/70 | Loss=0.0160 | Val Acc=57.14% | F1=0.654 | AUC=0.502\n",
      "[Valence (Fold 4)] Ep 19/70 | Loss=0.0154 | Val Acc=57.14% | F1=0.667 | AUC=0.514\n",
      "[Valence (Fold 4)] Ep 20/70 | Loss=0.0172 | Val Acc=54.76% | F1=0.655 | AUC=0.512\n",
      "[Valence (Fold 4)] Ep 21/70 | Loss=0.0974 | Val Acc=61.90% | F1=0.758 | AUC=0.421\n",
      "[Valence (Fold 4)] Ep 22/70 | Loss=0.0708 | Val Acc=59.52% | F1=0.721 | AUC=0.399\n",
      "[Valence (Fold 4)] Ep 23/70 | Loss=0.0859 | Val Acc=42.86% | F1=0.250 | AUC=0.471\n",
      "[Valence (Fold 4)] Ep 24/70 | Loss=0.0710 | Val Acc=50.00% | F1=0.644 | AUC=0.377\n",
      "[Valence (Fold 4)] Ep 25/70 | Loss=0.0589 | Val Acc=61.90% | F1=0.765 | AUC=0.469\n",
      "[Valence (Fold 4)] Ep 26/70 | Loss=0.0673 | Val Acc=64.29% | F1=0.776 | AUC=0.517\n",
      "[Valence (Fold 4)] Ep 27/70 | Loss=0.0322 | Val Acc=61.90% | F1=0.758 | AUC=0.425\n",
      "[Valence (Fold 4)] Ep 28/70 | Loss=0.0126 | Val Acc=61.90% | F1=0.733 | AUC=0.459\n",
      "[Valence (Fold 4)] Ep 29/70 | Loss=0.0044 | Val Acc=54.76% | F1=0.642 | AUC=0.440\n",
      "[Valence (Fold 4)] Ep 30/70 | Loss=0.0070 | Val Acc=52.38% | F1=0.630 | AUC=0.454\n",
      "[Valence (Fold 4)] Ep 31/70 | Loss=0.0137 | Val Acc=45.24% | F1=0.489 | AUC=0.440\n",
      "â¹ [Valence (Fold 4)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 4)] TEST (best thr=0.100) | Acc=64.29% | F1=0.634 | AUC=0.715\n",
      "\n",
      "Fold 4 Results: Acc=0.6429, F1=0.6341, AUC=0.7153\n",
      "\n",
      "----- Valence Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 5) | epochs=70\n",
      "[Valence (Fold 5)] Ep 01/70 | Loss=0.5369 | Val Acc=39.02% | F1=0.194 | AUC=0.505\n",
      "[Valence (Fold 5)] Ep 02/70 | Loss=0.5085 | Val Acc=36.59% | F1=0.316 | AUC=0.493\n",
      "[Valence (Fold 5)] Ep 03/70 | Loss=0.4823 | Val Acc=36.59% | F1=0.235 | AUC=0.422\n",
      "[Valence (Fold 5)] Ep 04/70 | Loss=0.4451 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 5)] Ep 05/70 | Loss=0.3746 | Val Acc=58.54% | F1=0.738 | AUC=0.540\n",
      "[Valence (Fold 5)] Ep 06/70 | Loss=0.2386 | Val Acc=58.54% | F1=0.738 | AUC=0.512\n",
      "[Valence (Fold 5)] Ep 07/70 | Loss=0.1956 | Val Acc=65.85% | F1=0.781 | AUC=0.557\n",
      "[Valence (Fold 5)] Ep 08/70 | Loss=0.1110 | Val Acc=58.54% | F1=0.738 | AUC=0.575\n",
      "[Valence (Fold 5)] Ep 09/70 | Loss=0.1119 | Val Acc=58.54% | F1=0.738 | AUC=0.620\n",
      "[Valence (Fold 5)] Ep 10/70 | Loss=0.0607 | Val Acc=46.34% | F1=0.542 | AUC=0.440\n",
      "[Valence (Fold 5)] Ep 11/70 | Loss=0.0470 | Val Acc=39.02% | F1=0.194 | AUC=0.422\n",
      "[Valence (Fold 5)] Ep 12/70 | Loss=0.0304 | Val Acc=58.54% | F1=0.667 | AUC=0.505\n",
      "[Valence (Fold 5)] Ep 13/70 | Loss=0.0362 | Val Acc=46.34% | F1=0.522 | AUC=0.518\n",
      "[Valence (Fold 5)] Ep 14/70 | Loss=0.0278 | Val Acc=58.54% | F1=0.712 | AUC=0.550\n",
      "[Valence (Fold 5)] Ep 15/70 | Loss=0.0142 | Val Acc=48.78% | F1=0.553 | AUC=0.485\n",
      "[Valence (Fold 5)] Ep 16/70 | Loss=0.0155 | Val Acc=53.66% | F1=0.655 | AUC=0.500\n",
      "[Valence (Fold 5)] Ep 17/70 | Loss=0.0147 | Val Acc=53.66% | F1=0.612 | AUC=0.472\n",
      "[Valence (Fold 5)] Ep 18/70 | Loss=0.0123 | Val Acc=56.10% | F1=0.667 | AUC=0.503\n",
      "[Valence (Fold 5)] Ep 19/70 | Loss=0.0124 | Val Acc=58.54% | F1=0.679 | AUC=0.495\n",
      "[Valence (Fold 5)] Ep 20/70 | Loss=0.0138 | Val Acc=60.98% | F1=0.714 | AUC=0.505\n",
      "[Valence (Fold 5)] Ep 21/70 | Loss=0.1131 | Val Acc=39.02% | F1=0.074 | AUC=0.393\n",
      "[Valence (Fold 5)] Ep 22/70 | Loss=0.0912 | Val Acc=39.02% | F1=0.000 | AUC=0.492\n",
      "[Valence (Fold 5)] Ep 23/70 | Loss=0.1006 | Val Acc=58.54% | F1=0.702 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 24/70 | Loss=0.0522 | Val Acc=34.15% | F1=0.000 | AUC=0.347\n",
      "[Valence (Fold 5)] Ep 25/70 | Loss=0.0376 | Val Acc=48.78% | F1=0.571 | AUC=0.453\n",
      "[Valence (Fold 5)] Ep 26/70 | Loss=0.0228 | Val Acc=34.15% | F1=0.182 | AUC=0.375\n",
      "[Valence (Fold 5)] Ep 27/70 | Loss=0.0158 | Val Acc=51.22% | F1=0.615 | AUC=0.463\n",
      "[Valence (Fold 5)] Ep 28/70 | Loss=0.0151 | Val Acc=56.10% | F1=0.667 | AUC=0.468\n",
      "[Valence (Fold 5)] Ep 29/70 | Loss=0.0163 | Val Acc=60.98% | F1=0.742 | AUC=0.470\n",
      "[Valence (Fold 5)] Ep 30/70 | Loss=0.0080 | Val Acc=53.66% | F1=0.627 | AUC=0.465\n",
      "[Valence (Fold 5)] Ep 31/70 | Loss=0.0026 | Val Acc=63.41% | F1=0.717 | AUC=0.483\n",
      "â¹ [Valence (Fold 5)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 5)] TEST (best thr=0.100) | Acc=56.10% | F1=0.679 | AUC=0.612\n",
      "\n",
      "Fold 5 Results: Acc=0.5610, F1=0.6786, AUC=0.6125\n",
      "\n",
      "----- Valence Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 6) | epochs=70\n",
      "[Valence (Fold 6)] Ep 01/70 | Loss=0.5343 | Val Acc=53.66% | F1=0.642 | AUC=0.518\n",
      "[Valence (Fold 6)] Ep 02/70 | Loss=0.5108 | Val Acc=53.66% | F1=0.667 | AUC=0.555\n",
      "[Valence (Fold 6)] Ep 03/70 | Loss=0.5052 | Val Acc=63.41% | F1=0.615 | AUC=0.667\n",
      "[Valence (Fold 6)] Ep 04/70 | Loss=0.4533 | Val Acc=53.66% | F1=0.678 | AUC=0.593\n",
      "[Valence (Fold 6)] Ep 05/70 | Loss=0.3811 | Val Acc=58.54% | F1=0.730 | AUC=0.402\n",
      "[Valence (Fold 6)] Ep 06/70 | Loss=0.2749 | Val Acc=56.10% | F1=0.719 | AUC=0.453\n",
      "[Valence (Fold 6)] Ep 07/70 | Loss=0.1849 | Val Acc=46.34% | F1=0.560 | AUC=0.508\n",
      "[Valence (Fold 6)] Ep 08/70 | Loss=0.1540 | Val Acc=51.22% | F1=0.500 | AUC=0.577\n",
      "[Valence (Fold 6)] Ep 09/70 | Loss=0.0779 | Val Acc=60.98% | F1=0.758 | AUC=0.347\n",
      "[Valence (Fold 6)] Ep 10/70 | Loss=0.0707 | Val Acc=56.10% | F1=0.679 | AUC=0.498\n",
      "[Valence (Fold 6)] Ep 11/70 | Loss=0.0534 | Val Acc=60.98% | F1=0.758 | AUC=0.433\n",
      "[Valence (Fold 6)] Ep 12/70 | Loss=0.0429 | Val Acc=53.66% | F1=0.698 | AUC=0.450\n",
      "[Valence (Fold 6)] Ep 13/70 | Loss=0.0315 | Val Acc=51.22% | F1=0.643 | AUC=0.445\n",
      "[Valence (Fold 6)] Ep 14/70 | Loss=0.0259 | Val Acc=43.90% | F1=0.489 | AUC=0.482\n",
      "[Valence (Fold 6)] Ep 15/70 | Loss=0.0216 | Val Acc=48.78% | F1=0.533 | AUC=0.527\n",
      "[Valence (Fold 6)] Ep 16/70 | Loss=0.0151 | Val Acc=58.54% | F1=0.702 | AUC=0.458\n",
      "[Valence (Fold 6)] Ep 17/70 | Loss=0.0167 | Val Acc=56.10% | F1=0.690 | AUC=0.445\n",
      "[Valence (Fold 6)] Ep 18/70 | Loss=0.0186 | Val Acc=46.34% | F1=0.560 | AUC=0.455\n",
      "[Valence (Fold 6)] Ep 19/70 | Loss=0.0130 | Val Acc=46.34% | F1=0.560 | AUC=0.480\n",
      "[Valence (Fold 6)] Ep 20/70 | Loss=0.0159 | Val Acc=46.34% | F1=0.522 | AUC=0.490\n",
      "[Valence (Fold 6)] Ep 21/70 | Loss=0.0403 | Val Acc=41.46% | F1=0.077 | AUC=0.387\n",
      "[Valence (Fold 6)] Ep 22/70 | Loss=0.1197 | Val Acc=43.90% | F1=0.148 | AUC=0.512\n",
      "[Valence (Fold 6)] Ep 23/70 | Loss=0.1018 | Val Acc=56.10% | F1=0.667 | AUC=0.570\n",
      "[Valence (Fold 6)] Ep 24/70 | Loss=0.0742 | Val Acc=48.78% | F1=0.488 | AUC=0.545\n",
      "[Valence (Fold 6)] Ep 25/70 | Loss=0.0326 | Val Acc=43.90% | F1=0.378 | AUC=0.527\n",
      "[Valence (Fold 6)] Ep 26/70 | Loss=0.0394 | Val Acc=46.34% | F1=0.389 | AUC=0.573\n",
      "[Valence (Fold 6)] Ep 27/70 | Loss=0.0345 | Val Acc=43.90% | F1=0.566 | AUC=0.445\n",
      "[Valence (Fold 6)] Ep 28/70 | Loss=0.0212 | Val Acc=58.54% | F1=0.712 | AUC=0.443\n",
      "[Valence (Fold 6)] Ep 29/70 | Loss=0.0061 | Val Acc=41.46% | F1=0.143 | AUC=0.545\n",
      "[Valence (Fold 6)] Ep 30/70 | Loss=0.0159 | Val Acc=48.78% | F1=0.488 | AUC=0.515\n",
      "[Valence (Fold 6)] Ep 31/70 | Loss=0.0057 | Val Acc=48.78% | F1=0.604 | AUC=0.495\n",
      "â¹ [Valence (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 6)] TEST (best thr=0.100) | Acc=68.29% | F1=0.772 | AUC=0.663\n",
      "\n",
      "Fold 6 Results: Acc=0.6829, F1=0.7719, AUC=0.6625\n",
      "\n",
      "----- Valence Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 7) | epochs=70\n",
      "[Valence (Fold 7)] Ep 01/70 | Loss=0.5310 | Val Acc=70.73% | F1=0.806 | AUC=0.758\n",
      "[Valence (Fold 7)] Ep 02/70 | Loss=0.5149 | Val Acc=48.78% | F1=0.323 | AUC=0.630\n",
      "[Valence (Fold 7)] Ep 03/70 | Loss=0.4795 | Val Acc=65.85% | F1=0.781 | AUC=0.770\n",
      "[Valence (Fold 7)] Ep 04/70 | Loss=0.4649 | Val Acc=60.98% | F1=0.758 | AUC=0.547\n",
      "[Valence (Fold 7)] Ep 05/70 | Loss=0.3965 | Val Acc=56.10% | F1=0.550 | AUC=0.688\n",
      "[Valence (Fold 7)] Ep 06/70 | Loss=0.2972 | Val Acc=65.85% | F1=0.750 | AUC=0.710\n",
      "[Valence (Fold 7)] Ep 07/70 | Loss=0.1910 | Val Acc=36.59% | F1=0.071 | AUC=0.448\n",
      "[Valence (Fold 7)] Ep 08/70 | Loss=0.1512 | Val Acc=60.98% | F1=0.724 | AUC=0.552\n",
      "[Valence (Fold 7)] Ep 09/70 | Loss=0.0866 | Val Acc=43.90% | F1=0.378 | AUC=0.500\n",
      "[Valence (Fold 7)] Ep 10/70 | Loss=0.0603 | Val Acc=53.66% | F1=0.642 | AUC=0.482\n",
      "[Valence (Fold 7)] Ep 11/70 | Loss=0.0367 | Val Acc=63.41% | F1=0.737 | AUC=0.588\n",
      "[Valence (Fold 7)] Ep 12/70 | Loss=0.0404 | Val Acc=60.98% | F1=0.758 | AUC=0.675\n",
      "[Valence (Fold 7)] Ep 13/70 | Loss=0.0265 | Val Acc=58.54% | F1=0.653 | AUC=0.557\n",
      "[Valence (Fold 7)] Ep 14/70 | Loss=0.0329 | Val Acc=51.22% | F1=0.600 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 15/70 | Loss=0.0196 | Val Acc=63.41% | F1=0.737 | AUC=0.562\n",
      "[Valence (Fold 7)] Ep 16/70 | Loss=0.0184 | Val Acc=65.85% | F1=0.781 | AUC=0.635\n",
      "[Valence (Fold 7)] Ep 17/70 | Loss=0.0211 | Val Acc=65.85% | F1=0.759 | AUC=0.565\n",
      "[Valence (Fold 7)] Ep 18/70 | Loss=0.0115 | Val Acc=58.54% | F1=0.691 | AUC=0.554\n",
      "[Valence (Fold 7)] Ep 19/70 | Loss=0.0131 | Val Acc=65.85% | F1=0.750 | AUC=0.573\n",
      "[Valence (Fold 7)] Ep 20/70 | Loss=0.0114 | Val Acc=63.41% | F1=0.737 | AUC=0.578\n",
      "[Valence (Fold 7)] Ep 21/70 | Loss=0.1023 | Val Acc=73.17% | F1=0.820 | AUC=0.657\n",
      "[Valence (Fold 7)] Ep 22/70 | Loss=0.0762 | Val Acc=60.98% | F1=0.758 | AUC=0.620\n",
      "[Valence (Fold 7)] Ep 23/70 | Loss=0.0558 | Val Acc=43.90% | F1=0.258 | AUC=0.480\n",
      "[Valence (Fold 7)] Ep 24/70 | Loss=0.0599 | Val Acc=63.41% | F1=0.769 | AUC=0.610\n",
      "[Valence (Fold 7)] Ep 25/70 | Loss=0.0199 | Val Acc=63.41% | F1=0.746 | AUC=0.550\n",
      "[Valence (Fold 7)] Ep 26/70 | Loss=0.0253 | Val Acc=36.59% | F1=0.235 | AUC=0.443\n",
      "[Valence (Fold 7)] Ep 27/70 | Loss=0.0321 | Val Acc=63.41% | F1=0.769 | AUC=0.565\n",
      "[Valence (Fold 7)] Ep 28/70 | Loss=0.0171 | Val Acc=68.29% | F1=0.794 | AUC=0.580\n",
      "[Valence (Fold 7)] Ep 29/70 | Loss=0.0165 | Val Acc=53.66% | F1=0.513 | AUC=0.630\n",
      "[Valence (Fold 7)] Ep 30/70 | Loss=0.0168 | Val Acc=58.54% | F1=0.638 | AUC=0.625\n",
      "[Valence (Fold 7)] Ep 31/70 | Loss=0.0050 | Val Acc=51.22% | F1=0.545 | AUC=0.587\n",
      "[Valence (Fold 7)] Ep 32/70 | Loss=0.0050 | Val Acc=63.41% | F1=0.727 | AUC=0.645\n",
      "[Valence (Fold 7)] Ep 33/70 | Loss=0.0042 | Val Acc=65.85% | F1=0.759 | AUC=0.562\n",
      "[Valence (Fold 7)] Ep 34/70 | Loss=0.0061 | Val Acc=60.98% | F1=0.714 | AUC=0.590\n",
      "[Valence (Fold 7)] Ep 35/70 | Loss=0.0020 | Val Acc=70.73% | F1=0.800 | AUC=0.561\n",
      "[Valence (Fold 7)] Ep 36/70 | Loss=0.0064 | Val Acc=56.10% | F1=0.667 | AUC=0.568\n",
      "[Valence (Fold 7)] Ep 37/70 | Loss=0.0018 | Val Acc=63.41% | F1=0.737 | AUC=0.557\n",
      "[Valence (Fold 7)] Ep 38/70 | Loss=0.0018 | Val Acc=60.98% | F1=0.714 | AUC=0.557\n",
      "[Valence (Fold 7)] Ep 39/70 | Loss=0.0061 | Val Acc=70.73% | F1=0.800 | AUC=0.552\n",
      "â¹ [Valence (Fold 7)] Early stopping at epoch 39\n",
      "\n",
      "ðŸ”š [Valence (Fold 7)] TEST (best thr=0.150) | Acc=51.22% | F1=0.630 | AUC=0.390\n",
      "\n",
      "Fold 7 Results: Acc=0.5122, F1=0.6296, AUC=0.3900\n",
      "\n",
      "----- Valence Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 8) | epochs=70\n",
      "[Valence (Fold 8)] Ep 01/70 | Loss=0.5381 | Val Acc=63.41% | F1=0.727 | AUC=0.548\n",
      "[Valence (Fold 8)] Ep 02/70 | Loss=0.5199 | Val Acc=68.29% | F1=0.764 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 03/70 | Loss=0.4833 | Val Acc=58.54% | F1=0.730 | AUC=0.688\n",
      "[Valence (Fold 8)] Ep 04/70 | Loss=0.4590 | Val Acc=48.78% | F1=0.432 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 05/70 | Loss=0.3859 | Val Acc=60.98% | F1=0.724 | AUC=0.615\n",
      "[Valence (Fold 8)] Ep 06/70 | Loss=0.3075 | Val Acc=39.02% | F1=0.074 | AUC=0.495\n",
      "[Valence (Fold 8)] Ep 07/70 | Loss=0.2110 | Val Acc=39.02% | F1=0.194 | AUC=0.713\n",
      "[Valence (Fold 8)] Ep 08/70 | Loss=0.1448 | Val Acc=60.98% | F1=0.750 | AUC=0.633\n",
      "[Valence (Fold 8)] Ep 09/70 | Loss=0.1260 | Val Acc=36.59% | F1=0.071 | AUC=0.522\n",
      "[Valence (Fold 8)] Ep 10/70 | Loss=0.0804 | Val Acc=60.98% | F1=0.680 | AUC=0.625\n",
      "[Valence (Fold 8)] Ep 11/70 | Loss=0.0423 | Val Acc=48.78% | F1=0.512 | AUC=0.610\n",
      "[Valence (Fold 8)] Ep 12/70 | Loss=0.0414 | Val Acc=58.54% | F1=0.605 | AUC=0.617\n",
      "[Valence (Fold 8)] Ep 13/70 | Loss=0.0302 | Val Acc=63.41% | F1=0.737 | AUC=0.662\n",
      "[Valence (Fold 8)] Ep 14/70 | Loss=0.0276 | Val Acc=60.98% | F1=0.652 | AUC=0.650\n",
      "[Valence (Fold 8)] Ep 15/70 | Loss=0.0207 | Val Acc=58.54% | F1=0.679 | AUC=0.637\n",
      "[Valence (Fold 8)] Ep 16/70 | Loss=0.0193 | Val Acc=53.66% | F1=0.558 | AUC=0.628\n",
      "[Valence (Fold 8)] Ep 17/70 | Loss=0.0222 | Val Acc=65.85% | F1=0.696 | AUC=0.653\n",
      "[Valence (Fold 8)] Ep 18/70 | Loss=0.0168 | Val Acc=63.41% | F1=0.681 | AUC=0.637\n",
      "[Valence (Fold 8)] Ep 19/70 | Loss=0.0139 | Val Acc=65.85% | F1=0.708 | AUC=0.635\n",
      "[Valence (Fold 8)] Ep 20/70 | Loss=0.0123 | Val Acc=65.85% | F1=0.720 | AUC=0.645\n",
      "[Valence (Fold 8)] Ep 21/70 | Loss=0.0967 | Val Acc=65.85% | F1=0.750 | AUC=0.588\n",
      "[Valence (Fold 8)] Ep 22/70 | Loss=0.0978 | Val Acc=43.90% | F1=0.343 | AUC=0.575\n",
      "[Valence (Fold 8)] Ep 23/70 | Loss=0.0851 | Val Acc=36.59% | F1=0.000 | AUC=0.560\n",
      "[Valence (Fold 8)] Ep 24/70 | Loss=0.0694 | Val Acc=58.54% | F1=0.738 | AUC=0.530\n",
      "[Valence (Fold 8)] Ep 25/70 | Loss=0.0637 | Val Acc=58.54% | F1=0.738 | AUC=0.450\n",
      "[Valence (Fold 8)] Ep 26/70 | Loss=0.0331 | Val Acc=53.66% | F1=0.486 | AUC=0.610\n",
      "[Valence (Fold 8)] Ep 27/70 | Loss=0.0213 | Val Acc=53.66% | F1=0.537 | AUC=0.610\n",
      "[Valence (Fold 8)] Ep 28/70 | Loss=0.0092 | Val Acc=58.54% | F1=0.738 | AUC=0.530\n",
      "[Valence (Fold 8)] Ep 29/70 | Loss=0.0155 | Val Acc=65.85% | F1=0.720 | AUC=0.623\n",
      "[Valence (Fold 8)] Ep 30/70 | Loss=0.0076 | Val Acc=43.90% | F1=0.378 | AUC=0.573\n",
      "[Valence (Fold 8)] Ep 31/70 | Loss=0.0077 | Val Acc=43.90% | F1=0.303 | AUC=0.615\n",
      "â¹ [Valence (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 8)] TEST (best thr=0.400) | Acc=48.78% | F1=0.488 | AUC=0.527\n",
      "\n",
      "Fold 8 Results: Acc=0.4878, F1=0.4878, AUC=0.5275\n",
      "\n",
      "----- Valence Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 9) | epochs=70\n",
      "[Valence (Fold 9)] Ep 01/70 | Loss=0.5302 | Val Acc=65.85% | F1=0.696 | AUC=0.633\n",
      "[Valence (Fold 9)] Ep 02/70 | Loss=0.4987 | Val Acc=43.90% | F1=0.378 | AUC=0.583\n",
      "[Valence (Fold 9)] Ep 03/70 | Loss=0.4795 | Val Acc=41.46% | F1=0.368 | AUC=0.523\n",
      "[Valence (Fold 9)] Ep 04/70 | Loss=0.4471 | Val Acc=46.34% | F1=0.389 | AUC=0.520\n",
      "[Valence (Fold 9)] Ep 05/70 | Loss=0.3794 | Val Acc=60.98% | F1=0.758 | AUC=0.565\n",
      "[Valence (Fold 9)] Ep 06/70 | Loss=0.3222 | Val Acc=41.46% | F1=0.455 | AUC=0.335\n",
      "[Valence (Fold 9)] Ep 07/70 | Loss=0.2176 | Val Acc=36.59% | F1=0.000 | AUC=0.390\n",
      "[Valence (Fold 9)] Ep 08/70 | Loss=0.1611 | Val Acc=36.59% | F1=0.188 | AUC=0.365\n",
      "[Valence (Fold 9)] Ep 09/70 | Loss=0.1105 | Val Acc=31.71% | F1=0.000 | AUC=0.323\n",
      "[Valence (Fold 9)] Ep 10/70 | Loss=0.0811 | Val Acc=58.54% | F1=0.738 | AUC=0.540\n",
      "[Valence (Fold 9)] Ep 11/70 | Loss=0.0597 | Val Acc=36.59% | F1=0.235 | AUC=0.475\n",
      "[Valence (Fold 9)] Ep 12/70 | Loss=0.0373 | Val Acc=43.90% | F1=0.489 | AUC=0.395\n",
      "[Valence (Fold 9)] Ep 13/70 | Loss=0.0383 | Val Acc=60.98% | F1=0.758 | AUC=0.568\n",
      "[Valence (Fold 9)] Ep 14/70 | Loss=0.0259 | Val Acc=60.98% | F1=0.742 | AUC=0.467\n",
      "[Valence (Fold 9)] Ep 15/70 | Loss=0.0205 | Val Acc=53.66% | F1=0.627 | AUC=0.417\n",
      "[Valence (Fold 9)] Ep 16/70 | Loss=0.0154 | Val Acc=58.54% | F1=0.691 | AUC=0.430\n",
      "[Valence (Fold 9)] Ep 17/70 | Loss=0.0151 | Val Acc=41.46% | F1=0.429 | AUC=0.395\n",
      "[Valence (Fold 9)] Ep 18/70 | Loss=0.0178 | Val Acc=48.78% | F1=0.571 | AUC=0.420\n",
      "[Valence (Fold 9)] Ep 19/70 | Loss=0.0143 | Val Acc=46.34% | F1=0.542 | AUC=0.427\n",
      "[Valence (Fold 9)] Ep 20/70 | Loss=0.0158 | Val Acc=53.66% | F1=0.627 | AUC=0.438\n",
      "[Valence (Fold 9)] Ep 21/70 | Loss=0.0608 | Val Acc=60.98% | F1=0.742 | AUC=0.448\n",
      "[Valence (Fold 9)] Ep 22/70 | Loss=0.0706 | Val Acc=53.66% | F1=0.689 | AUC=0.405\n",
      "[Valence (Fold 9)] Ep 23/70 | Loss=0.0616 | Val Acc=53.66% | F1=0.642 | AUC=0.455\n",
      "[Valence (Fold 9)] Ep 24/70 | Loss=0.0838 | Val Acc=34.15% | F1=0.308 | AUC=0.407\n",
      "[Valence (Fold 9)] Ep 25/70 | Loss=0.0354 | Val Acc=41.46% | F1=0.500 | AUC=0.335\n",
      "[Valence (Fold 9)] Ep 26/70 | Loss=0.0367 | Val Acc=48.78% | F1=0.553 | AUC=0.477\n",
      "[Valence (Fold 9)] Ep 27/70 | Loss=0.0180 | Val Acc=34.15% | F1=0.000 | AUC=0.400\n",
      "[Valence (Fold 9)] Ep 28/70 | Loss=0.0131 | Val Acc=60.98% | F1=0.750 | AUC=0.532\n",
      "[Valence (Fold 9)] Ep 29/70 | Loss=0.0166 | Val Acc=51.22% | F1=0.583 | AUC=0.422\n",
      "[Valence (Fold 9)] Ep 30/70 | Loss=0.0047 | Val Acc=58.54% | F1=0.702 | AUC=0.458\n",
      "[Valence (Fold 9)] Ep 31/70 | Loss=0.0040 | Val Acc=46.34% | F1=0.560 | AUC=0.330\n",
      "â¹ [Valence (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 9)] TEST (best thr=0.100) | Acc=56.10% | F1=0.679 | AUC=0.453\n",
      "\n",
      "Fold 9 Results: Acc=0.5610, F1=0.6786, AUC=0.4525\n",
      "\n",
      "----- Valence Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 10) | epochs=70\n",
      "[Valence (Fold 10)] Ep 01/70 | Loss=0.5312 | Val Acc=46.34% | F1=0.389 | AUC=0.465\n",
      "[Valence (Fold 10)] Ep 02/70 | Loss=0.5110 | Val Acc=48.78% | F1=0.553 | AUC=0.460\n",
      "[Valence (Fold 10)] Ep 03/70 | Loss=0.4717 | Val Acc=56.10% | F1=0.719 | AUC=0.448\n",
      "[Valence (Fold 10)] Ep 04/70 | Loss=0.4305 | Val Acc=36.59% | F1=0.000 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 05/70 | Loss=0.2850 | Val Acc=51.22% | F1=0.615 | AUC=0.487\n",
      "[Valence (Fold 10)] Ep 06/70 | Loss=0.1855 | Val Acc=34.15% | F1=0.000 | AUC=0.370\n",
      "[Valence (Fold 10)] Ep 07/70 | Loss=0.1503 | Val Acc=46.34% | F1=0.633 | AUC=0.448\n",
      "[Valence (Fold 10)] Ep 08/70 | Loss=0.1241 | Val Acc=43.90% | F1=0.566 | AUC=0.443\n",
      "[Valence (Fold 10)] Ep 09/70 | Loss=0.0861 | Val Acc=53.66% | F1=0.698 | AUC=0.403\n",
      "[Valence (Fold 10)] Ep 10/70 | Loss=0.0433 | Val Acc=36.59% | F1=0.000 | AUC=0.417\n",
      "[Valence (Fold 10)] Ep 11/70 | Loss=0.0369 | Val Acc=39.02% | F1=0.242 | AUC=0.475\n",
      "[Valence (Fold 10)] Ep 12/70 | Loss=0.0316 | Val Acc=46.34% | F1=0.421 | AUC=0.435\n",
      "[Valence (Fold 10)] Ep 13/70 | Loss=0.0244 | Val Acc=43.90% | F1=0.531 | AUC=0.475\n",
      "[Valence (Fold 10)] Ep 14/70 | Loss=0.0160 | Val Acc=43.90% | F1=0.566 | AUC=0.475\n",
      "[Valence (Fold 10)] Ep 15/70 | Loss=0.0141 | Val Acc=41.46% | F1=0.538 | AUC=0.457\n",
      "[Valence (Fold 10)] Ep 16/70 | Loss=0.0120 | Val Acc=46.34% | F1=0.560 | AUC=0.452\n",
      "[Valence (Fold 10)] Ep 17/70 | Loss=0.0096 | Val Acc=43.90% | F1=0.566 | AUC=0.455\n",
      "[Valence (Fold 10)] Ep 18/70 | Loss=0.0100 | Val Acc=53.66% | F1=0.537 | AUC=0.432\n",
      "[Valence (Fold 10)] Ep 19/70 | Loss=0.0121 | Val Acc=46.34% | F1=0.560 | AUC=0.432\n",
      "[Valence (Fold 10)] Ep 20/70 | Loss=0.0091 | Val Acc=43.90% | F1=0.531 | AUC=0.442\n",
      "[Valence (Fold 10)] Ep 21/70 | Loss=0.0265 | Val Acc=43.90% | F1=0.410 | AUC=0.422\n",
      "[Valence (Fold 10)] Ep 22/70 | Loss=0.1167 | Val Acc=46.34% | F1=0.560 | AUC=0.478\n",
      "[Valence (Fold 10)] Ep 23/70 | Loss=0.0673 | Val Acc=46.34% | F1=0.633 | AUC=0.395\n",
      "[Valence (Fold 10)] Ep 24/70 | Loss=0.0392 | Val Acc=51.22% | F1=0.677 | AUC=0.380\n",
      "[Valence (Fold 10)] Ep 25/70 | Loss=0.0573 | Val Acc=51.22% | F1=0.677 | AUC=0.460\n",
      "[Valence (Fold 10)] Ep 26/70 | Loss=0.0450 | Val Acc=46.34% | F1=0.621 | AUC=0.462\n",
      "[Valence (Fold 10)] Ep 27/70 | Loss=0.0396 | Val Acc=53.66% | F1=0.513 | AUC=0.473\n",
      "[Valence (Fold 10)] Ep 28/70 | Loss=0.0124 | Val Acc=56.10% | F1=0.625 | AUC=0.505\n",
      "[Valence (Fold 10)] Ep 29/70 | Loss=0.0153 | Val Acc=43.90% | F1=0.549 | AUC=0.440\n",
      "[Valence (Fold 10)] Ep 30/70 | Loss=0.0111 | Val Acc=51.22% | F1=0.667 | AUC=0.422\n",
      "[Valence (Fold 10)] Ep 31/70 | Loss=0.0028 | Val Acc=53.66% | F1=0.655 | AUC=0.453\n",
      "â¹ [Valence (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 10)] TEST (best thr=0.550) | Acc=60.98% | F1=0.714 | AUC=0.578\n",
      "\n",
      "Fold 10 Results: Acc=0.6098, F1=0.7143, AUC=0.5775\n",
      "\n",
      "===== FINAL Valence 10-FOLD RESULTS =====\n",
      "Accuracy: 0.5819 Â± 0.0632\n",
      "F1-score: 0.6637 Â± 0.0783\n",
      "AUC:      0.5601 Â± 0.0891\n",
      "\n",
      "########## Arousal: 10-fold STRATIFIED CV ##########\n",
      "Arousal global median (for stratification) = 3.0000\n",
      "Class counts: [114 300]\n",
      "\n",
      "----- Arousal Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 1) | epochs=70\n",
      "[Arousal (Fold 1)] Ep 01/70 | Loss=0.3747 | Val Acc=38.10% | F1=0.278 | AUC=0.631\n",
      "[Arousal (Fold 1)] Ep 02/70 | Loss=0.3620 | Val Acc=33.33% | F1=0.176 | AUC=0.556\n",
      "[Arousal (Fold 1)] Ep 03/70 | Loss=0.3392 | Val Acc=71.43% | F1=0.818 | AUC=0.550\n",
      "[Arousal (Fold 1)] Ep 04/70 | Loss=0.3019 | Val Acc=42.86% | F1=0.455 | AUC=0.661\n",
      "[Arousal (Fold 1)] Ep 05/70 | Loss=0.2383 | Val Acc=71.43% | F1=0.829 | AUC=0.683\n",
      "[Arousal (Fold 1)] Ep 06/70 | Loss=0.1591 | Val Acc=45.24% | F1=0.439 | AUC=0.581\n",
      "[Arousal (Fold 1)] Ep 07/70 | Loss=0.1061 | Val Acc=66.67% | F1=0.774 | AUC=0.608\n",
      "[Arousal (Fold 1)] Ep 08/70 | Loss=0.0756 | Val Acc=66.67% | F1=0.800 | AUC=0.636\n",
      "[Arousal (Fold 1)] Ep 09/70 | Loss=0.0707 | Val Acc=28.57% | F1=0.118 | AUC=0.492\n",
      "[Arousal (Fold 1)] Ep 10/70 | Loss=0.0522 | Val Acc=69.05% | F1=0.812 | AUC=0.706\n",
      "[Arousal (Fold 1)] Ep 11/70 | Loss=0.0344 | Val Acc=71.43% | F1=0.812 | AUC=0.689\n",
      "[Arousal (Fold 1)] Ep 12/70 | Loss=0.0234 | Val Acc=73.81% | F1=0.831 | AUC=0.597\n",
      "[Arousal (Fold 1)] Ep 13/70 | Loss=0.0196 | Val Acc=69.05% | F1=0.812 | AUC=0.728\n",
      "[Arousal (Fold 1)] Ep 14/70 | Loss=0.0206 | Val Acc=71.43% | F1=0.806 | AUC=0.681\n",
      "[Arousal (Fold 1)] Ep 15/70 | Loss=0.0111 | Val Acc=61.90% | F1=0.714 | AUC=0.619\n",
      "[Arousal (Fold 1)] Ep 16/70 | Loss=0.0101 | Val Acc=73.81% | F1=0.825 | AUC=0.656\n",
      "[Arousal (Fold 1)] Ep 17/70 | Loss=0.0117 | Val Acc=66.67% | F1=0.767 | AUC=0.639\n",
      "[Arousal (Fold 1)] Ep 18/70 | Loss=0.0089 | Val Acc=73.81% | F1=0.825 | AUC=0.653\n",
      "[Arousal (Fold 1)] Ep 19/70 | Loss=0.0098 | Val Acc=73.81% | F1=0.831 | AUC=0.644\n",
      "[Arousal (Fold 1)] Ep 20/70 | Loss=0.0107 | Val Acc=73.81% | F1=0.831 | AUC=0.639\n",
      "[Arousal (Fold 1)] Ep 21/70 | Loss=0.0387 | Val Acc=71.43% | F1=0.829 | AUC=0.775\n",
      "[Arousal (Fold 1)] Ep 22/70 | Loss=0.0635 | Val Acc=71.43% | F1=0.829 | AUC=0.689\n",
      "[Arousal (Fold 1)] Ep 23/70 | Loss=0.0569 | Val Acc=57.14% | F1=0.679 | AUC=0.489\n",
      "[Arousal (Fold 1)] Ep 24/70 | Loss=0.0373 | Val Acc=45.24% | F1=0.439 | AUC=0.533\n",
      "[Arousal (Fold 1)] Ep 25/70 | Loss=0.0270 | Val Acc=38.10% | F1=0.350 | AUC=0.506\n",
      "[Arousal (Fold 1)] Ep 26/70 | Loss=0.0124 | Val Acc=45.24% | F1=0.489 | AUC=0.581\n",
      "[Arousal (Fold 1)] Ep 27/70 | Loss=0.0351 | Val Acc=69.05% | F1=0.806 | AUC=0.636\n",
      "[Arousal (Fold 1)] Ep 28/70 | Loss=0.0148 | Val Acc=52.38% | F1=0.615 | AUC=0.542\n",
      "[Arousal (Fold 1)] Ep 29/70 | Loss=0.0039 | Val Acc=61.90% | F1=0.733 | AUC=0.583\n",
      "[Arousal (Fold 1)] Ep 30/70 | Loss=0.0030 | Val Acc=66.67% | F1=0.794 | AUC=0.764\n",
      "[Arousal (Fold 1)] Ep 31/70 | Loss=0.0024 | Val Acc=76.19% | F1=0.828 | AUC=0.683\n",
      "[Arousal (Fold 1)] Ep 32/70 | Loss=0.0035 | Val Acc=73.81% | F1=0.831 | AUC=0.639\n",
      "[Arousal (Fold 1)] Ep 33/70 | Loss=0.0023 | Val Acc=69.05% | F1=0.794 | AUC=0.622\n",
      "[Arousal (Fold 1)] Ep 34/70 | Loss=0.0041 | Val Acc=73.81% | F1=0.831 | AUC=0.672\n",
      "[Arousal (Fold 1)] Ep 35/70 | Loss=0.0015 | Val Acc=73.81% | F1=0.831 | AUC=0.661\n",
      "[Arousal (Fold 1)] Ep 36/70 | Loss=0.0026 | Val Acc=73.81% | F1=0.831 | AUC=0.656\n",
      "[Arousal (Fold 1)] Ep 37/70 | Loss=0.0019 | Val Acc=73.81% | F1=0.831 | AUC=0.631\n",
      "[Arousal (Fold 1)] Ep 38/70 | Loss=0.0018 | Val Acc=73.81% | F1=0.831 | AUC=0.633\n",
      "[Arousal (Fold 1)] Ep 39/70 | Loss=0.0015 | Val Acc=73.81% | F1=0.831 | AUC=0.633\n",
      "[Arousal (Fold 1)] Ep 40/70 | Loss=0.0016 | Val Acc=73.81% | F1=0.831 | AUC=0.631\n",
      "[Arousal (Fold 1)] Ep 41/70 | Loss=0.0073 | Val Acc=38.10% | F1=0.409 | AUC=0.525\n",
      "[Arousal (Fold 1)] Ep 42/70 | Loss=0.0317 | Val Acc=78.57% | F1=0.866 | AUC=0.753\n",
      "[Arousal (Fold 1)] Ep 43/70 | Loss=0.0802 | Val Acc=66.67% | F1=0.788 | AUC=0.642\n",
      "[Arousal (Fold 1)] Ep 44/70 | Loss=0.0215 | Val Acc=73.81% | F1=0.845 | AUC=0.550\n",
      "[Arousal (Fold 1)] Ep 45/70 | Loss=0.0237 | Val Acc=33.33% | F1=0.222 | AUC=0.556\n",
      "[Arousal (Fold 1)] Ep 46/70 | Loss=0.0149 | Val Acc=61.90% | F1=0.733 | AUC=0.575\n",
      "[Arousal (Fold 1)] Ep 47/70 | Loss=0.0022 | Val Acc=64.29% | F1=0.754 | AUC=0.586\n",
      "[Arousal (Fold 1)] Ep 48/70 | Loss=0.0029 | Val Acc=71.43% | F1=0.800 | AUC=0.669\n",
      "[Arousal (Fold 1)] Ep 49/70 | Loss=0.0028 | Val Acc=69.05% | F1=0.794 | AUC=0.678\n",
      "[Arousal (Fold 1)] Ep 50/70 | Loss=0.0060 | Val Acc=69.05% | F1=0.787 | AUC=0.647\n",
      "[Arousal (Fold 1)] Ep 51/70 | Loss=0.0018 | Val Acc=57.14% | F1=0.700 | AUC=0.572\n",
      "[Arousal (Fold 1)] Ep 52/70 | Loss=0.0009 | Val Acc=69.05% | F1=0.806 | AUC=0.586\n",
      "[Arousal (Fold 1)] Ep 53/70 | Loss=0.0007 | Val Acc=61.90% | F1=0.742 | AUC=0.589\n",
      "[Arousal (Fold 1)] Ep 54/70 | Loss=0.0007 | Val Acc=69.05% | F1=0.800 | AUC=0.603\n",
      "[Arousal (Fold 1)] Ep 55/70 | Loss=0.0008 | Val Acc=61.90% | F1=0.742 | AUC=0.628\n",
      "[Arousal (Fold 1)] Ep 56/70 | Loss=0.0008 | Val Acc=61.90% | F1=0.742 | AUC=0.617\n",
      "[Arousal (Fold 1)] Ep 57/70 | Loss=0.0007 | Val Acc=64.29% | F1=0.762 | AUC=0.619\n",
      "[Arousal (Fold 1)] Ep 58/70 | Loss=0.0006 | Val Acc=61.90% | F1=0.742 | AUC=0.619\n",
      "[Arousal (Fold 1)] Ep 59/70 | Loss=0.0007 | Val Acc=64.29% | F1=0.762 | AUC=0.619\n",
      "[Arousal (Fold 1)] Ep 60/70 | Loss=0.0007 | Val Acc=69.05% | F1=0.800 | AUC=0.631\n",
      "â¹ [Arousal (Fold 1)] Early stopping at epoch 60\n",
      "\n",
      "ðŸ”š [Arousal (Fold 1)] TEST (best thr=0.150) | Acc=66.67% | F1=0.781 | AUC=0.408\n",
      "\n",
      "Fold 1 Results: Acc=0.6667, F1=0.7812, AUC=0.4083\n",
      "\n",
      "----- Arousal Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 2) | epochs=70\n",
      "[Arousal (Fold 2)] Ep 01/70 | Loss=0.3733 | Val Acc=52.38% | F1=0.615 | AUC=0.486\n",
      "[Arousal (Fold 2)] Ep 02/70 | Loss=0.3585 | Val Acc=38.10% | F1=0.409 | AUC=0.428\n",
      "[Arousal (Fold 2)] Ep 03/70 | Loss=0.3371 | Val Acc=40.48% | F1=0.390 | AUC=0.589\n",
      "[Arousal (Fold 2)] Ep 04/70 | Loss=0.2967 | Val Acc=33.33% | F1=0.222 | AUC=0.356\n",
      "[Arousal (Fold 2)] Ep 05/70 | Loss=0.2062 | Val Acc=66.67% | F1=0.800 | AUC=0.517\n",
      "[Arousal (Fold 2)] Ep 06/70 | Loss=0.1489 | Val Acc=50.00% | F1=0.618 | AUC=0.394\n",
      "[Arousal (Fold 2)] Ep 07/70 | Loss=0.0979 | Val Acc=71.43% | F1=0.812 | AUC=0.539\n",
      "[Arousal (Fold 2)] Ep 08/70 | Loss=0.0742 | Val Acc=71.43% | F1=0.833 | AUC=0.511\n",
      "[Arousal (Fold 2)] Ep 09/70 | Loss=0.0541 | Val Acc=69.05% | F1=0.817 | AUC=0.458\n",
      "[Arousal (Fold 2)] Ep 10/70 | Loss=0.0371 | Val Acc=71.43% | F1=0.833 | AUC=0.442\n",
      "[Arousal (Fold 2)] Ep 11/70 | Loss=0.0255 | Val Acc=66.67% | F1=0.794 | AUC=0.464\n",
      "[Arousal (Fold 2)] Ep 12/70 | Loss=0.0194 | Val Acc=61.90% | F1=0.733 | AUC=0.514\n",
      "[Arousal (Fold 2)] Ep 13/70 | Loss=0.0163 | Val Acc=71.43% | F1=0.833 | AUC=0.497\n",
      "[Arousal (Fold 2)] Ep 14/70 | Loss=0.0139 | Val Acc=71.43% | F1=0.833 | AUC=0.511\n",
      "[Arousal (Fold 2)] Ep 15/70 | Loss=0.0162 | Val Acc=59.52% | F1=0.730 | AUC=0.531\n",
      "[Arousal (Fold 2)] Ep 16/70 | Loss=0.0080 | Val Acc=69.05% | F1=0.806 | AUC=0.497\n",
      "[Arousal (Fold 2)] Ep 17/70 | Loss=0.0079 | Val Acc=69.05% | F1=0.806 | AUC=0.506\n",
      "[Arousal (Fold 2)] Ep 18/70 | Loss=0.0077 | Val Acc=71.43% | F1=0.824 | AUC=0.500\n",
      "[Arousal (Fold 2)] Ep 19/70 | Loss=0.0086 | Val Acc=59.52% | F1=0.730 | AUC=0.514\n",
      "[Arousal (Fold 2)] Ep 20/70 | Loss=0.0076 | Val Acc=64.29% | F1=0.769 | AUC=0.514\n",
      "[Arousal (Fold 2)] Ep 21/70 | Loss=0.0517 | Val Acc=61.90% | F1=0.724 | AUC=0.569\n",
      "[Arousal (Fold 2)] Ep 22/70 | Loss=0.0579 | Val Acc=71.43% | F1=0.833 | AUC=0.650\n",
      "[Arousal (Fold 2)] Ep 23/70 | Loss=0.0496 | Val Acc=69.05% | F1=0.806 | AUC=0.458\n",
      "[Arousal (Fold 2)] Ep 24/70 | Loss=0.0447 | Val Acc=71.43% | F1=0.833 | AUC=0.619\n",
      "[Arousal (Fold 2)] Ep 25/70 | Loss=0.0300 | Val Acc=42.86% | F1=0.556 | AUC=0.339\n",
      "[Arousal (Fold 2)] Ep 26/70 | Loss=0.0141 | Val Acc=40.48% | F1=0.359 | AUC=0.464\n",
      "[Arousal (Fold 2)] Ep 27/70 | Loss=0.0061 | Val Acc=69.05% | F1=0.812 | AUC=0.489\n",
      "[Arousal (Fold 2)] Ep 28/70 | Loss=0.0072 | Val Acc=33.33% | F1=0.333 | AUC=0.394\n",
      "[Arousal (Fold 2)] Ep 29/70 | Loss=0.0041 | Val Acc=64.29% | F1=0.783 | AUC=0.492\n",
      "[Arousal (Fold 2)] Ep 30/70 | Loss=0.0089 | Val Acc=66.67% | F1=0.794 | AUC=0.467\n",
      "[Arousal (Fold 2)] Ep 31/70 | Loss=0.0021 | Val Acc=54.76% | F1=0.689 | AUC=0.456\n",
      "â¹ [Arousal (Fold 2)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 2)] TEST (best thr=0.100) | Acc=66.67% | F1=0.774 | AUC=0.642\n",
      "\n",
      "Fold 2 Results: Acc=0.6667, F1=0.7742, AUC=0.6417\n",
      "\n",
      "----- Arousal Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 3) | epochs=70\n",
      "[Arousal (Fold 3)] Ep 01/70 | Loss=0.3749 | Val Acc=45.24% | F1=0.410 | AUC=0.658\n",
      "[Arousal (Fold 3)] Ep 02/70 | Loss=0.3595 | Val Acc=66.67% | F1=0.741 | AUC=0.678\n",
      "[Arousal (Fold 3)] Ep 03/70 | Loss=0.3242 | Val Acc=76.19% | F1=0.844 | AUC=0.589\n",
      "[Arousal (Fold 3)] Ep 04/70 | Loss=0.2959 | Val Acc=71.43% | F1=0.833 | AUC=0.550\n",
      "[Arousal (Fold 3)] Ep 05/70 | Loss=0.2118 | Val Acc=47.62% | F1=0.500 | AUC=0.622\n",
      "[Arousal (Fold 3)] Ep 06/70 | Loss=0.1416 | Val Acc=42.86% | F1=0.429 | AUC=0.508\n",
      "[Arousal (Fold 3)] Ep 07/70 | Loss=0.0816 | Val Acc=69.05% | F1=0.817 | AUC=0.453\n",
      "[Arousal (Fold 3)] Ep 08/70 | Loss=0.0682 | Val Acc=59.52% | F1=0.721 | AUC=0.442\n",
      "[Arousal (Fold 3)] Ep 09/70 | Loss=0.0357 | Val Acc=71.43% | F1=0.833 | AUC=0.506\n",
      "[Arousal (Fold 3)] Ep 10/70 | Loss=0.0362 | Val Acc=30.95% | F1=0.171 | AUC=0.556\n",
      "[Arousal (Fold 3)] Ep 11/70 | Loss=0.0303 | Val Acc=59.52% | F1=0.702 | AUC=0.439\n",
      "[Arousal (Fold 3)] Ep 12/70 | Loss=0.0143 | Val Acc=52.38% | F1=0.655 | AUC=0.475\n",
      "[Arousal (Fold 3)] Ep 13/70 | Loss=0.0147 | Val Acc=38.10% | F1=0.350 | AUC=0.533\n",
      "[Arousal (Fold 3)] Ep 14/70 | Loss=0.0119 | Val Acc=57.14% | F1=0.710 | AUC=0.447\n",
      "[Arousal (Fold 3)] Ep 15/70 | Loss=0.0087 | Val Acc=57.14% | F1=0.710 | AUC=0.444\n",
      "[Arousal (Fold 3)] Ep 16/70 | Loss=0.0107 | Val Acc=66.67% | F1=0.794 | AUC=0.469\n",
      "[Arousal (Fold 3)] Ep 17/70 | Loss=0.0083 | Val Acc=57.14% | F1=0.710 | AUC=0.492\n",
      "[Arousal (Fold 3)] Ep 18/70 | Loss=0.0088 | Val Acc=57.14% | F1=0.710 | AUC=0.467\n",
      "[Arousal (Fold 3)] Ep 19/70 | Loss=0.0076 | Val Acc=57.14% | F1=0.710 | AUC=0.453\n",
      "[Arousal (Fold 3)] Ep 20/70 | Loss=0.0070 | Val Acc=57.14% | F1=0.710 | AUC=0.453\n",
      "[Arousal (Fold 3)] Ep 21/70 | Loss=0.0273 | Val Acc=69.05% | F1=0.817 | AUC=0.564\n",
      "[Arousal (Fold 3)] Ep 22/70 | Loss=0.0704 | Val Acc=21.43% | F1=0.000 | AUC=0.442\n",
      "[Arousal (Fold 3)] Ep 23/70 | Loss=0.1083 | Val Acc=71.43% | F1=0.833 | AUC=0.353\n",
      "[Arousal (Fold 3)] Ep 24/70 | Loss=0.0644 | Val Acc=66.67% | F1=0.800 | AUC=0.422\n",
      "[Arousal (Fold 3)] Ep 25/70 | Loss=0.0216 | Val Acc=47.62% | F1=0.542 | AUC=0.464\n",
      "[Arousal (Fold 3)] Ep 26/70 | Loss=0.0115 | Val Acc=69.05% | F1=0.817 | AUC=0.497\n",
      "[Arousal (Fold 3)] Ep 27/70 | Loss=0.0186 | Val Acc=59.52% | F1=0.730 | AUC=0.567\n",
      "[Arousal (Fold 3)] Ep 28/70 | Loss=0.0063 | Val Acc=66.67% | F1=0.767 | AUC=0.592\n",
      "[Arousal (Fold 3)] Ep 29/70 | Loss=0.0027 | Val Acc=69.05% | F1=0.806 | AUC=0.617\n",
      "[Arousal (Fold 3)] Ep 30/70 | Loss=0.0025 | Val Acc=64.29% | F1=0.762 | AUC=0.592\n",
      "[Arousal (Fold 3)] Ep 31/70 | Loss=0.0026 | Val Acc=64.29% | F1=0.783 | AUC=0.542\n",
      "â¹ [Arousal (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 3)] TEST (best thr=0.100) | Acc=71.43% | F1=0.833 | AUC=0.636\n",
      "\n",
      "Fold 3 Results: Acc=0.7143, F1=0.8333, AUC=0.6361\n",
      "\n",
      "----- Arousal Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 4) | epochs=70\n",
      "[Arousal (Fold 4)] Ep 01/70 | Loss=0.3702 | Val Acc=33.33% | F1=0.364 | AUC=0.336\n",
      "[Arousal (Fold 4)] Ep 02/70 | Loss=0.3462 | Val Acc=66.67% | F1=0.794 | AUC=0.336\n",
      "[Arousal (Fold 4)] Ep 03/70 | Loss=0.3216 | Val Acc=33.33% | F1=0.263 | AUC=0.453\n",
      "[Arousal (Fold 4)] Ep 04/70 | Loss=0.3018 | Val Acc=45.24% | F1=0.566 | AUC=0.406\n",
      "[Arousal (Fold 4)] Ep 05/70 | Loss=0.2265 | Val Acc=69.05% | F1=0.817 | AUC=0.344\n",
      "[Arousal (Fold 4)] Ep 06/70 | Loss=0.1732 | Val Acc=69.05% | F1=0.817 | AUC=0.383\n",
      "[Arousal (Fold 4)] Ep 07/70 | Loss=0.1158 | Val Acc=47.62% | F1=0.577 | AUC=0.542\n",
      "[Arousal (Fold 4)] Ep 08/70 | Loss=0.0898 | Val Acc=61.90% | F1=0.758 | AUC=0.369\n",
      "[Arousal (Fold 4)] Ep 09/70 | Loss=0.0648 | Val Acc=66.67% | F1=0.800 | AUC=0.303\n",
      "[Arousal (Fold 4)] Ep 10/70 | Loss=0.0479 | Val Acc=66.67% | F1=0.800 | AUC=0.356\n",
      "[Arousal (Fold 4)] Ep 11/70 | Loss=0.0476 | Val Acc=64.29% | F1=0.783 | AUC=0.353\n",
      "[Arousal (Fold 4)] Ep 12/70 | Loss=0.0352 | Val Acc=33.33% | F1=0.300 | AUC=0.550\n",
      "[Arousal (Fold 4)] Ep 13/70 | Loss=0.0304 | Val Acc=57.14% | F1=0.710 | AUC=0.536\n",
      "[Arousal (Fold 4)] Ep 14/70 | Loss=0.0287 | Val Acc=61.90% | F1=0.733 | AUC=0.533\n",
      "[Arousal (Fold 4)] Ep 15/70 | Loss=0.0262 | Val Acc=66.67% | F1=0.788 | AUC=0.519\n",
      "[Arousal (Fold 4)] Ep 16/70 | Loss=0.0244 | Val Acc=64.29% | F1=0.769 | AUC=0.553\n",
      "[Arousal (Fold 4)] Ep 17/70 | Loss=0.0202 | Val Acc=64.29% | F1=0.776 | AUC=0.556\n",
      "[Arousal (Fold 4)] Ep 18/70 | Loss=0.0254 | Val Acc=69.05% | F1=0.800 | AUC=0.550\n",
      "[Arousal (Fold 4)] Ep 19/70 | Loss=0.0231 | Val Acc=64.29% | F1=0.776 | AUC=0.572\n",
      "[Arousal (Fold 4)] Ep 20/70 | Loss=0.0213 | Val Acc=66.67% | F1=0.788 | AUC=0.556\n",
      "[Arousal (Fold 4)] Ep 21/70 | Loss=0.0430 | Val Acc=30.95% | F1=0.216 | AUC=0.456\n",
      "[Arousal (Fold 4)] Ep 22/70 | Loss=0.0672 | Val Acc=61.90% | F1=0.758 | AUC=0.503\n",
      "[Arousal (Fold 4)] Ep 23/70 | Loss=0.0382 | Val Acc=71.43% | F1=0.833 | AUC=0.497\n",
      "[Arousal (Fold 4)] Ep 24/70 | Loss=0.0733 | Val Acc=64.29% | F1=0.783 | AUC=0.550\n",
      "[Arousal (Fold 4)] Ep 25/70 | Loss=0.0289 | Val Acc=28.57% | F1=0.000 | AUC=0.403\n",
      "[Arousal (Fold 4)] Ep 26/70 | Loss=0.0236 | Val Acc=69.05% | F1=0.817 | AUC=0.469\n",
      "[Arousal (Fold 4)] Ep 27/70 | Loss=0.0173 | Val Acc=64.29% | F1=0.746 | AUC=0.558\n",
      "[Arousal (Fold 4)] Ep 28/70 | Loss=0.0103 | Val Acc=54.76% | F1=0.627 | AUC=0.622\n",
      "[Arousal (Fold 4)] Ep 29/70 | Loss=0.0080 | Val Acc=52.38% | F1=0.630 | AUC=0.581\n",
      "[Arousal (Fold 4)] Ep 30/70 | Loss=0.0067 | Val Acc=61.90% | F1=0.750 | AUC=0.586\n",
      "[Arousal (Fold 4)] Ep 31/70 | Loss=0.0067 | Val Acc=50.00% | F1=0.604 | AUC=0.578\n",
      "[Arousal (Fold 4)] Ep 32/70 | Loss=0.0049 | Val Acc=66.67% | F1=0.781 | AUC=0.525\n",
      "[Arousal (Fold 4)] Ep 33/70 | Loss=0.0033 | Val Acc=66.67% | F1=0.781 | AUC=0.514\n",
      "[Arousal (Fold 4)] Ep 34/70 | Loss=0.0025 | Val Acc=61.90% | F1=0.758 | AUC=0.519\n",
      "[Arousal (Fold 4)] Ep 35/70 | Loss=0.0020 | Val Acc=66.67% | F1=0.781 | AUC=0.528\n",
      "[Arousal (Fold 4)] Ep 36/70 | Loss=0.0019 | Val Acc=64.29% | F1=0.776 | AUC=0.497\n",
      "[Arousal (Fold 4)] Ep 37/70 | Loss=0.0019 | Val Acc=64.29% | F1=0.769 | AUC=0.525\n",
      "[Arousal (Fold 4)] Ep 38/70 | Loss=0.0018 | Val Acc=64.29% | F1=0.769 | AUC=0.531\n",
      "[Arousal (Fold 4)] Ep 39/70 | Loss=0.0023 | Val Acc=64.29% | F1=0.769 | AUC=0.531\n",
      "[Arousal (Fold 4)] Ep 40/70 | Loss=0.0018 | Val Acc=64.29% | F1=0.769 | AUC=0.525\n",
      "[Arousal (Fold 4)] Ep 41/70 | Loss=0.0068 | Val Acc=64.29% | F1=0.783 | AUC=0.347\n",
      "â¹ [Arousal (Fold 4)] Early stopping at epoch 41\n",
      "\n",
      "ðŸ”š [Arousal (Fold 4)] TEST (best thr=0.200) | Acc=69.05% | F1=0.812 | AUC=0.539\n",
      "\n",
      "Fold 4 Results: Acc=0.6905, F1=0.8116, AUC=0.5389\n",
      "\n",
      "----- Arousal Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 5) | epochs=70\n",
      "[Arousal (Fold 5)] Ep 01/70 | Loss=0.3841 | Val Acc=56.10% | F1=0.625 | AUC=0.627\n",
      "[Arousal (Fold 5)] Ep 02/70 | Loss=0.3652 | Val Acc=51.22% | F1=0.583 | AUC=0.558\n",
      "[Arousal (Fold 5)] Ep 03/70 | Loss=0.3395 | Val Acc=34.15% | F1=0.229 | AUC=0.618\n",
      "[Arousal (Fold 5)] Ep 04/70 | Loss=0.2974 | Val Acc=46.34% | F1=0.421 | AUC=0.624\n",
      "[Arousal (Fold 5)] Ep 05/70 | Loss=0.2143 | Val Acc=70.73% | F1=0.818 | AUC=0.536\n",
      "[Arousal (Fold 5)] Ep 06/70 | Loss=0.1562 | Val Acc=48.78% | F1=0.533 | AUC=0.570\n",
      "[Arousal (Fold 5)] Ep 07/70 | Loss=0.1238 | Val Acc=53.66% | F1=0.612 | AUC=0.642\n",
      "[Arousal (Fold 5)] Ep 08/70 | Loss=0.0786 | Val Acc=60.98% | F1=0.758 | AUC=0.418\n",
      "[Arousal (Fold 5)] Ep 09/70 | Loss=0.0431 | Val Acc=65.85% | F1=0.788 | AUC=0.494\n",
      "[Arousal (Fold 5)] Ep 10/70 | Loss=0.0312 | Val Acc=73.17% | F1=0.841 | AUC=0.245\n",
      "[Arousal (Fold 5)] Ep 11/70 | Loss=0.0364 | Val Acc=48.78% | F1=0.533 | AUC=0.624\n",
      "[Arousal (Fold 5)] Ep 12/70 | Loss=0.0191 | Val Acc=68.29% | F1=0.806 | AUC=0.458\n",
      "[Arousal (Fold 5)] Ep 13/70 | Loss=0.0195 | Val Acc=63.41% | F1=0.776 | AUC=0.500\n",
      "[Arousal (Fold 5)] Ep 14/70 | Loss=0.0103 | Val Acc=58.54% | F1=0.702 | AUC=0.552\n",
      "[Arousal (Fold 5)] Ep 15/70 | Loss=0.0095 | Val Acc=60.98% | F1=0.758 | AUC=0.506\n",
      "[Arousal (Fold 5)] Ep 16/70 | Loss=0.0091 | Val Acc=60.98% | F1=0.733 | AUC=0.527\n",
      "[Arousal (Fold 5)] Ep 17/70 | Loss=0.0077 | Val Acc=60.98% | F1=0.733 | AUC=0.527\n",
      "[Arousal (Fold 5)] Ep 18/70 | Loss=0.0074 | Val Acc=63.41% | F1=0.754 | AUC=0.527\n",
      "[Arousal (Fold 5)] Ep 19/70 | Loss=0.0073 | Val Acc=60.98% | F1=0.733 | AUC=0.530\n",
      "[Arousal (Fold 5)] Ep 20/70 | Loss=0.0076 | Val Acc=60.98% | F1=0.733 | AUC=0.521\n",
      "[Arousal (Fold 5)] Ep 21/70 | Loss=0.0570 | Val Acc=46.34% | F1=0.476 | AUC=0.609\n",
      "[Arousal (Fold 5)] Ep 22/70 | Loss=0.0571 | Val Acc=39.02% | F1=0.359 | AUC=0.712\n",
      "[Arousal (Fold 5)] Ep 23/70 | Loss=0.0795 | Val Acc=70.73% | F1=0.812 | AUC=0.555\n",
      "[Arousal (Fold 5)] Ep 24/70 | Loss=0.0662 | Val Acc=65.85% | F1=0.788 | AUC=0.545\n",
      "[Arousal (Fold 5)] Ep 25/70 | Loss=0.0199 | Val Acc=65.85% | F1=0.781 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 26/70 | Loss=0.0069 | Val Acc=70.73% | F1=0.818 | AUC=0.579\n",
      "[Arousal (Fold 5)] Ep 27/70 | Loss=0.0144 | Val Acc=73.17% | F1=0.841 | AUC=0.379\n",
      "[Arousal (Fold 5)] Ep 28/70 | Loss=0.0058 | Val Acc=68.29% | F1=0.806 | AUC=0.497\n",
      "[Arousal (Fold 5)] Ep 29/70 | Loss=0.0097 | Val Acc=53.66% | F1=0.655 | AUC=0.497\n",
      "[Arousal (Fold 5)] Ep 30/70 | Loss=0.0064 | Val Acc=65.85% | F1=0.781 | AUC=0.542\n",
      "[Arousal (Fold 5)] Ep 31/70 | Loss=0.0034 | Val Acc=63.41% | F1=0.762 | AUC=0.527\n",
      "â¹ [Arousal (Fold 5)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 5)] TEST (best thr=0.100) | Acc=68.29% | F1=0.812 | AUC=0.497\n",
      "\n",
      "Fold 5 Results: Acc=0.6829, F1=0.8116, AUC=0.4970\n",
      "\n",
      "----- Arousal Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 6) | epochs=70\n",
      "[Arousal (Fold 6)] Ep 01/70 | Loss=0.3837 | Val Acc=68.29% | F1=0.800 | AUC=0.667\n",
      "[Arousal (Fold 6)] Ep 02/70 | Loss=0.3655 | Val Acc=56.10% | F1=0.710 | AUC=0.524\n",
      "[Arousal (Fold 6)] Ep 03/70 | Loss=0.3501 | Val Acc=36.59% | F1=0.278 | AUC=0.545\n",
      "[Arousal (Fold 6)] Ep 04/70 | Loss=0.3060 | Val Acc=68.29% | F1=0.812 | AUC=0.318\n",
      "[Arousal (Fold 6)] Ep 05/70 | Loss=0.2413 | Val Acc=36.59% | F1=0.409 | AUC=0.364\n",
      "[Arousal (Fold 6)] Ep 06/70 | Loss=0.1506 | Val Acc=56.10% | F1=0.719 | AUC=0.445\n",
      "[Arousal (Fold 6)] Ep 07/70 | Loss=0.1082 | Val Acc=51.22% | F1=0.655 | AUC=0.518\n",
      "[Arousal (Fold 6)] Ep 08/70 | Loss=0.0554 | Val Acc=56.10% | F1=0.719 | AUC=0.409\n",
      "[Arousal (Fold 6)] Ep 09/70 | Loss=0.1089 | Val Acc=65.85% | F1=0.794 | AUC=0.470\n",
      "[Arousal (Fold 6)] Ep 10/70 | Loss=0.0557 | Val Acc=58.54% | F1=0.691 | AUC=0.494\n",
      "[Arousal (Fold 6)] Ep 11/70 | Loss=0.0289 | Val Acc=46.34% | F1=0.542 | AUC=0.458\n",
      "[Arousal (Fold 6)] Ep 12/70 | Loss=0.0213 | Val Acc=70.73% | F1=0.829 | AUC=0.558\n",
      "[Arousal (Fold 6)] Ep 13/70 | Loss=0.0158 | Val Acc=65.85% | F1=0.794 | AUC=0.509\n",
      "[Arousal (Fold 6)] Ep 14/70 | Loss=0.0153 | Val Acc=65.85% | F1=0.794 | AUC=0.445\n",
      "[Arousal (Fold 6)] Ep 15/70 | Loss=0.0152 | Val Acc=41.46% | F1=0.478 | AUC=0.464\n",
      "[Arousal (Fold 6)] Ep 16/70 | Loss=0.0094 | Val Acc=60.98% | F1=0.758 | AUC=0.433\n",
      "[Arousal (Fold 6)] Ep 17/70 | Loss=0.0091 | Val Acc=58.54% | F1=0.730 | AUC=0.500\n",
      "[Arousal (Fold 6)] Ep 18/70 | Loss=0.0090 | Val Acc=58.54% | F1=0.730 | AUC=0.527\n",
      "[Arousal (Fold 6)] Ep 19/70 | Loss=0.0080 | Val Acc=58.54% | F1=0.730 | AUC=0.527\n",
      "[Arousal (Fold 6)] Ep 20/70 | Loss=0.0116 | Val Acc=56.10% | F1=0.710 | AUC=0.494\n",
      "[Arousal (Fold 6)] Ep 21/70 | Loss=0.0117 | Val Acc=68.29% | F1=0.812 | AUC=0.552\n",
      "[Arousal (Fold 6)] Ep 22/70 | Loss=0.0772 | Val Acc=56.10% | F1=0.700 | AUC=0.436\n",
      "[Arousal (Fold 6)] Ep 23/70 | Loss=0.0751 | Val Acc=65.85% | F1=0.781 | AUC=0.576\n",
      "[Arousal (Fold 6)] Ep 24/70 | Loss=0.0240 | Val Acc=60.98% | F1=0.742 | AUC=0.576\n",
      "[Arousal (Fold 6)] Ep 25/70 | Loss=0.0117 | Val Acc=68.29% | F1=0.806 | AUC=0.503\n",
      "[Arousal (Fold 6)] Ep 26/70 | Loss=0.0141 | Val Acc=56.10% | F1=0.700 | AUC=0.391\n",
      "[Arousal (Fold 6)] Ep 27/70 | Loss=0.0141 | Val Acc=48.78% | F1=0.533 | AUC=0.473\n",
      "[Arousal (Fold 6)] Ep 28/70 | Loss=0.0167 | Val Acc=53.66% | F1=0.698 | AUC=0.370\n",
      "[Arousal (Fold 6)] Ep 29/70 | Loss=0.0061 | Val Acc=68.29% | F1=0.812 | AUC=0.538\n",
      "[Arousal (Fold 6)] Ep 30/70 | Loss=0.0018 | Val Acc=51.22% | F1=0.643 | AUC=0.433\n",
      "[Arousal (Fold 6)] Ep 31/70 | Loss=0.0026 | Val Acc=60.98% | F1=0.733 | AUC=0.506\n",
      "â¹ [Arousal (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 6)] TEST (best thr=0.100) | Acc=78.05% | F1=0.862 | AUC=0.736\n",
      "\n",
      "Fold 6 Results: Acc=0.7805, F1=0.8615, AUC=0.7364\n",
      "\n",
      "----- Arousal Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 7) | epochs=70\n",
      "[Arousal (Fold 7)] Ep 01/70 | Loss=0.3820 | Val Acc=29.27% | F1=0.065 | AUC=0.461\n",
      "[Arousal (Fold 7)] Ep 02/70 | Loss=0.3680 | Val Acc=58.54% | F1=0.653 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 03/70 | Loss=0.3468 | Val Acc=73.17% | F1=0.841 | AUC=0.521\n",
      "[Arousal (Fold 7)] Ep 04/70 | Loss=0.3269 | Val Acc=43.90% | F1=0.465 | AUC=0.573\n",
      "[Arousal (Fold 7)] Ep 05/70 | Loss=0.3025 | Val Acc=48.78% | F1=0.553 | AUC=0.561\n",
      "[Arousal (Fold 7)] Ep 06/70 | Loss=0.2500 | Val Acc=63.41% | F1=0.706 | AUC=0.667\n",
      "[Arousal (Fold 7)] Ep 07/70 | Loss=0.1956 | Val Acc=75.61% | F1=0.821 | AUC=0.745\n",
      "[Arousal (Fold 7)] Ep 08/70 | Loss=0.1400 | Val Acc=73.17% | F1=0.845 | AUC=0.479\n",
      "[Arousal (Fold 7)] Ep 09/70 | Loss=0.1189 | Val Acc=48.78% | F1=0.533 | AUC=0.618\n",
      "[Arousal (Fold 7)] Ep 10/70 | Loss=0.0699 | Val Acc=56.10% | F1=0.609 | AUC=0.667\n",
      "[Arousal (Fold 7)] Ep 11/70 | Loss=0.0519 | Val Acc=39.02% | F1=0.324 | AUC=0.618\n",
      "[Arousal (Fold 7)] Ep 12/70 | Loss=0.0388 | Val Acc=26.83% | F1=0.062 | AUC=0.564\n",
      "[Arousal (Fold 7)] Ep 13/70 | Loss=0.0301 | Val Acc=65.85% | F1=0.759 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 14/70 | Loss=0.0400 | Val Acc=43.90% | F1=0.465 | AUC=0.624\n",
      "[Arousal (Fold 7)] Ep 15/70 | Loss=0.0197 | Val Acc=70.73% | F1=0.800 | AUC=0.609\n",
      "[Arousal (Fold 7)] Ep 16/70 | Loss=0.0183 | Val Acc=73.17% | F1=0.836 | AUC=0.606\n",
      "[Arousal (Fold 7)] Ep 17/70 | Loss=0.0153 | Val Acc=68.29% | F1=0.780 | AUC=0.606\n",
      "[Arousal (Fold 7)] Ep 18/70 | Loss=0.0148 | Val Acc=65.85% | F1=0.767 | AUC=0.615\n",
      "[Arousal (Fold 7)] Ep 19/70 | Loss=0.0136 | Val Acc=63.41% | F1=0.754 | AUC=0.612\n",
      "[Arousal (Fold 7)] Ep 20/70 | Loss=0.0115 | Val Acc=68.29% | F1=0.794 | AUC=0.609\n",
      "[Arousal (Fold 7)] Ep 21/70 | Loss=0.0326 | Val Acc=73.17% | F1=0.845 | AUC=0.421\n",
      "[Arousal (Fold 7)] Ep 22/70 | Loss=0.0859 | Val Acc=73.17% | F1=0.831 | AUC=0.748\n",
      "[Arousal (Fold 7)] Ep 23/70 | Loss=0.0703 | Val Acc=48.78% | F1=0.533 | AUC=0.636\n",
      "[Arousal (Fold 7)] Ep 24/70 | Loss=0.0363 | Val Acc=73.17% | F1=0.836 | AUC=0.812\n",
      "[Arousal (Fold 7)] Ep 25/70 | Loss=0.0363 | Val Acc=29.27% | F1=0.121 | AUC=0.642\n",
      "[Arousal (Fold 7)] Ep 26/70 | Loss=0.0253 | Val Acc=73.17% | F1=0.820 | AUC=0.621\n",
      "[Arousal (Fold 7)] Ep 27/70 | Loss=0.0053 | Val Acc=51.22% | F1=0.565 | AUC=0.636\n",
      "[Arousal (Fold 7)] Ep 28/70 | Loss=0.0102 | Val Acc=70.73% | F1=0.812 | AUC=0.655\n",
      "[Arousal (Fold 7)] Ep 29/70 | Loss=0.0088 | Val Acc=73.17% | F1=0.845 | AUC=0.527\n",
      "[Arousal (Fold 7)] Ep 30/70 | Loss=0.0165 | Val Acc=68.29% | F1=0.812 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 31/70 | Loss=0.0045 | Val Acc=70.73% | F1=0.829 | AUC=0.618\n",
      "â¹ [Arousal (Fold 7)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 7)] TEST (best thr=0.100) | Acc=70.73% | F1=0.829 | AUC=0.412\n",
      "\n",
      "Fold 7 Results: Acc=0.7073, F1=0.8286, AUC=0.4121\n",
      "\n",
      "----- Arousal Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 8) | epochs=70\n",
      "[Arousal (Fold 8)] Ep 01/70 | Loss=0.3883 | Val Acc=36.59% | F1=0.235 | AUC=0.655\n",
      "[Arousal (Fold 8)] Ep 02/70 | Loss=0.3684 | Val Acc=73.17% | F1=0.831 | AUC=0.615\n",
      "[Arousal (Fold 8)] Ep 03/70 | Loss=0.3480 | Val Acc=63.41% | F1=0.717 | AUC=0.639\n",
      "[Arousal (Fold 8)] Ep 04/70 | Loss=0.2967 | Val Acc=73.17% | F1=0.845 | AUC=0.406\n",
      "[Arousal (Fold 8)] Ep 05/70 | Loss=0.2351 | Val Acc=26.83% | F1=0.000 | AUC=0.594\n",
      "[Arousal (Fold 8)] Ep 06/70 | Loss=0.1628 | Val Acc=43.90% | F1=0.489 | AUC=0.491\n",
      "[Arousal (Fold 8)] Ep 07/70 | Loss=0.0961 | Val Acc=73.17% | F1=0.845 | AUC=0.494\n",
      "[Arousal (Fold 8)] Ep 08/70 | Loss=0.0572 | Val Acc=56.10% | F1=0.667 | AUC=0.533\n",
      "[Arousal (Fold 8)] Ep 09/70 | Loss=0.0735 | Val Acc=60.98% | F1=0.750 | AUC=0.388\n",
      "[Arousal (Fold 8)] Ep 10/70 | Loss=0.0331 | Val Acc=70.73% | F1=0.829 | AUC=0.597\n",
      "[Arousal (Fold 8)] Ep 11/70 | Loss=0.0242 | Val Acc=63.41% | F1=0.762 | AUC=0.494\n",
      "[Arousal (Fold 8)] Ep 12/70 | Loss=0.0135 | Val Acc=60.98% | F1=0.758 | AUC=0.448\n",
      "[Arousal (Fold 8)] Ep 13/70 | Loss=0.0132 | Val Acc=48.78% | F1=0.588 | AUC=0.527\n",
      "[Arousal (Fold 8)] Ep 14/70 | Loss=0.0112 | Val Acc=46.34% | F1=0.500 | AUC=0.558\n",
      "[Arousal (Fold 8)] Ep 15/70 | Loss=0.0099 | Val Acc=53.66% | F1=0.655 | AUC=0.518\n",
      "[Arousal (Fold 8)] Ep 16/70 | Loss=0.0078 | Val Acc=58.54% | F1=0.730 | AUC=0.482\n",
      "[Arousal (Fold 8)] Ep 17/70 | Loss=0.0084 | Val Acc=70.73% | F1=0.824 | AUC=0.445\n",
      "[Arousal (Fold 8)] Ep 18/70 | Loss=0.0085 | Val Acc=58.54% | F1=0.702 | AUC=0.518\n",
      "[Arousal (Fold 8)] Ep 19/70 | Loss=0.0112 | Val Acc=63.41% | F1=0.769 | AUC=0.464\n",
      "[Arousal (Fold 8)] Ep 20/70 | Loss=0.0069 | Val Acc=58.54% | F1=0.721 | AUC=0.479\n",
      "[Arousal (Fold 8)] Ep 21/70 | Loss=0.0692 | Val Acc=39.02% | F1=0.324 | AUC=0.627\n",
      "[Arousal (Fold 8)] Ep 22/70 | Loss=0.0634 | Val Acc=36.59% | F1=0.350 | AUC=0.433\n",
      "[Arousal (Fold 8)] Ep 23/70 | Loss=0.0384 | Val Acc=56.10% | F1=0.591 | AUC=0.633\n",
      "[Arousal (Fold 8)] Ep 24/70 | Loss=0.0268 | Val Acc=43.90% | F1=0.511 | AUC=0.527\n",
      "[Arousal (Fold 8)] Ep 25/70 | Loss=0.0287 | Val Acc=73.17% | F1=0.845 | AUC=0.576\n",
      "[Arousal (Fold 8)] Ep 26/70 | Loss=0.0150 | Val Acc=68.29% | F1=0.806 | AUC=0.412\n",
      "[Arousal (Fold 8)] Ep 27/70 | Loss=0.0157 | Val Acc=65.85% | F1=0.781 | AUC=0.442\n",
      "[Arousal (Fold 8)] Ep 28/70 | Loss=0.0077 | Val Acc=65.85% | F1=0.781 | AUC=0.458\n",
      "[Arousal (Fold 8)] Ep 29/70 | Loss=0.0033 | Val Acc=68.29% | F1=0.806 | AUC=0.473\n",
      "[Arousal (Fold 8)] Ep 30/70 | Loss=0.0033 | Val Acc=68.29% | F1=0.806 | AUC=0.444\n",
      "[Arousal (Fold 8)] Ep 31/70 | Loss=0.0023 | Val Acc=56.10% | F1=0.700 | AUC=0.397\n",
      "â¹ [Arousal (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 8)] TEST (best thr=0.750) | Acc=70.73% | F1=0.806 | AUC=0.673\n",
      "\n",
      "Fold 8 Results: Acc=0.7073, F1=0.8065, AUC=0.6727\n",
      "\n",
      "----- Arousal Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 9) | epochs=70\n",
      "[Arousal (Fold 9)] Ep 01/70 | Loss=0.3775 | Val Acc=68.29% | F1=0.764 | AUC=0.715\n",
      "[Arousal (Fold 9)] Ep 02/70 | Loss=0.3678 | Val Acc=63.41% | F1=0.746 | AUC=0.552\n",
      "[Arousal (Fold 9)] Ep 03/70 | Loss=0.3525 | Val Acc=65.85% | F1=0.794 | AUC=0.636\n",
      "[Arousal (Fold 9)] Ep 04/70 | Loss=0.3097 | Val Acc=34.15% | F1=0.182 | AUC=0.639\n",
      "[Arousal (Fold 9)] Ep 05/70 | Loss=0.2710 | Val Acc=60.98% | F1=0.742 | AUC=0.667\n",
      "[Arousal (Fold 9)] Ep 06/70 | Loss=0.1650 | Val Acc=73.17% | F1=0.845 | AUC=0.394\n",
      "[Arousal (Fold 9)] Ep 07/70 | Loss=0.1399 | Val Acc=70.73% | F1=0.829 | AUC=0.509\n",
      "[Arousal (Fold 9)] Ep 08/70 | Loss=0.0816 | Val Acc=29.27% | F1=0.171 | AUC=0.506\n",
      "[Arousal (Fold 9)] Ep 09/70 | Loss=0.0625 | Val Acc=39.02% | F1=0.359 | AUC=0.539\n",
      "[Arousal (Fold 9)] Ep 10/70 | Loss=0.0372 | Val Acc=70.73% | F1=0.829 | AUC=0.515\n",
      "[Arousal (Fold 9)] Ep 11/70 | Loss=0.0329 | Val Acc=73.17% | F1=0.841 | AUC=0.576\n",
      "[Arousal (Fold 9)] Ep 12/70 | Loss=0.0248 | Val Acc=39.02% | F1=0.490 | AUC=0.482\n",
      "[Arousal (Fold 9)] Ep 13/70 | Loss=0.0205 | Val Acc=70.73% | F1=0.829 | AUC=0.609\n",
      "[Arousal (Fold 9)] Ep 14/70 | Loss=0.0162 | Val Acc=63.41% | F1=0.754 | AUC=0.494\n",
      "[Arousal (Fold 9)] Ep 15/70 | Loss=0.0207 | Val Acc=65.85% | F1=0.767 | AUC=0.564\n",
      "[Arousal (Fold 9)] Ep 16/70 | Loss=0.0121 | Val Acc=68.29% | F1=0.800 | AUC=0.512\n",
      "[Arousal (Fold 9)] Ep 17/70 | Loss=0.0120 | Val Acc=63.41% | F1=0.737 | AUC=0.533\n",
      "[Arousal (Fold 9)] Ep 18/70 | Loss=0.0111 | Val Acc=68.29% | F1=0.806 | AUC=0.533\n",
      "[Arousal (Fold 9)] Ep 19/70 | Loss=0.0104 | Val Acc=65.85% | F1=0.774 | AUC=0.527\n",
      "[Arousal (Fold 9)] Ep 20/70 | Loss=0.0108 | Val Acc=65.85% | F1=0.774 | AUC=0.527\n",
      "[Arousal (Fold 9)] Ep 21/70 | Loss=0.0369 | Val Acc=73.17% | F1=0.845 | AUC=0.439\n",
      "[Arousal (Fold 9)] Ep 22/70 | Loss=0.0604 | Val Acc=68.29% | F1=0.806 | AUC=0.621\n",
      "[Arousal (Fold 9)] Ep 23/70 | Loss=0.0842 | Val Acc=70.73% | F1=0.829 | AUC=0.645\n",
      "[Arousal (Fold 9)] Ep 24/70 | Loss=0.0269 | Val Acc=29.27% | F1=0.171 | AUC=0.576\n",
      "[Arousal (Fold 9)] Ep 25/70 | Loss=0.0309 | Val Acc=43.90% | F1=0.410 | AUC=0.697\n",
      "[Arousal (Fold 9)] Ep 26/70 | Loss=0.0258 | Val Acc=68.29% | F1=0.794 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 27/70 | Loss=0.0147 | Val Acc=70.73% | F1=0.829 | AUC=0.594\n",
      "[Arousal (Fold 9)] Ep 28/70 | Loss=0.0090 | Val Acc=70.73% | F1=0.829 | AUC=0.509\n",
      "[Arousal (Fold 9)] Ep 29/70 | Loss=0.0069 | Val Acc=56.10% | F1=0.690 | AUC=0.512\n",
      "[Arousal (Fold 9)] Ep 30/70 | Loss=0.0085 | Val Acc=65.85% | F1=0.774 | AUC=0.521\n",
      "[Arousal (Fold 9)] Ep 31/70 | Loss=0.0027 | Val Acc=73.17% | F1=0.836 | AUC=0.564\n",
      "â¹ [Arousal (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 9)] TEST (best thr=0.100) | Acc=70.73% | F1=0.829 | AUC=0.315\n",
      "\n",
      "Fold 9 Results: Acc=0.7073, F1=0.8286, AUC=0.3152\n",
      "\n",
      "----- Arousal Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 10) | epochs=70\n",
      "[Arousal (Fold 10)] Ep 01/70 | Loss=0.3802 | Val Acc=39.02% | F1=0.286 | AUC=0.712\n",
      "[Arousal (Fold 10)] Ep 02/70 | Loss=0.3732 | Val Acc=43.90% | F1=0.378 | AUC=0.709\n",
      "[Arousal (Fold 10)] Ep 03/70 | Loss=0.3511 | Val Acc=70.73% | F1=0.818 | AUC=0.670\n",
      "[Arousal (Fold 10)] Ep 04/70 | Loss=0.3413 | Val Acc=70.73% | F1=0.824 | AUC=0.594\n",
      "[Arousal (Fold 10)] Ep 05/70 | Loss=0.2787 | Val Acc=39.02% | F1=0.324 | AUC=0.597\n",
      "[Arousal (Fold 10)] Ep 06/70 | Loss=0.2340 | Val Acc=73.17% | F1=0.845 | AUC=0.606\n",
      "[Arousal (Fold 10)] Ep 07/70 | Loss=0.1585 | Val Acc=68.29% | F1=0.812 | AUC=0.724\n",
      "[Arousal (Fold 10)] Ep 08/70 | Loss=0.0851 | Val Acc=46.34% | F1=0.522 | AUC=0.564\n",
      "[Arousal (Fold 10)] Ep 09/70 | Loss=0.0651 | Val Acc=31.71% | F1=0.125 | AUC=0.612\n",
      "[Arousal (Fold 10)] Ep 10/70 | Loss=0.0353 | Val Acc=73.17% | F1=0.845 | AUC=0.506\n",
      "[Arousal (Fold 10)] Ep 11/70 | Loss=0.0223 | Val Acc=65.85% | F1=0.788 | AUC=0.630\n",
      "[Arousal (Fold 10)] Ep 12/70 | Loss=0.0180 | Val Acc=56.10% | F1=0.640 | AUC=0.615\n",
      "[Arousal (Fold 10)] Ep 13/70 | Loss=0.0137 | Val Acc=68.29% | F1=0.794 | AUC=0.600\n",
      "[Arousal (Fold 10)] Ep 14/70 | Loss=0.0125 | Val Acc=70.73% | F1=0.812 | AUC=0.579\n",
      "[Arousal (Fold 10)] Ep 15/70 | Loss=0.0112 | Val Acc=70.73% | F1=0.824 | AUC=0.630\n",
      "[Arousal (Fold 10)] Ep 16/70 | Loss=0.0082 | Val Acc=70.73% | F1=0.812 | AUC=0.600\n",
      "[Arousal (Fold 10)] Ep 17/70 | Loss=0.0075 | Val Acc=73.17% | F1=0.825 | AUC=0.615\n",
      "[Arousal (Fold 10)] Ep 18/70 | Loss=0.0074 | Val Acc=73.17% | F1=0.825 | AUC=0.552\n",
      "[Arousal (Fold 10)] Ep 19/70 | Loss=0.0073 | Val Acc=73.17% | F1=0.825 | AUC=0.594\n",
      "[Arousal (Fold 10)] Ep 20/70 | Loss=0.0098 | Val Acc=70.73% | F1=0.812 | AUC=0.573\n",
      "[Arousal (Fold 10)] Ep 21/70 | Loss=0.0581 | Val Acc=70.73% | F1=0.778 | AUC=0.721\n",
      "[Arousal (Fold 10)] Ep 22/70 | Loss=0.0432 | Val Acc=75.61% | F1=0.857 | AUC=0.682\n",
      "[Arousal (Fold 10)] Ep 23/70 | Loss=0.0561 | Val Acc=75.61% | F1=0.857 | AUC=0.748\n",
      "[Arousal (Fold 10)] Ep 24/70 | Loss=0.0308 | Val Acc=41.46% | F1=0.333 | AUC=0.591\n",
      "[Arousal (Fold 10)] Ep 25/70 | Loss=0.0211 | Val Acc=60.98% | F1=0.714 | AUC=0.627\n",
      "[Arousal (Fold 10)] Ep 26/70 | Loss=0.0050 | Val Acc=63.41% | F1=0.746 | AUC=0.500\n",
      "[Arousal (Fold 10)] Ep 27/70 | Loss=0.0057 | Val Acc=68.29% | F1=0.806 | AUC=0.615\n",
      "[Arousal (Fold 10)] Ep 28/70 | Loss=0.0062 | Val Acc=70.73% | F1=0.812 | AUC=0.530\n",
      "[Arousal (Fold 10)] Ep 29/70 | Loss=0.0049 | Val Acc=51.22% | F1=0.583 | AUC=0.576\n",
      "[Arousal (Fold 10)] Ep 30/70 | Loss=0.0029 | Val Acc=53.66% | F1=0.655 | AUC=0.527\n",
      "[Arousal (Fold 10)] Ep 31/70 | Loss=0.0037 | Val Acc=73.17% | F1=0.825 | AUC=0.639\n",
      "[Arousal (Fold 10)] Ep 32/70 | Loss=0.0016 | Val Acc=73.17% | F1=0.825 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 33/70 | Loss=0.0036 | Val Acc=73.17% | F1=0.825 | AUC=0.618\n",
      "[Arousal (Fold 10)] Ep 34/70 | Loss=0.0012 | Val Acc=73.17% | F1=0.825 | AUC=0.598\n",
      "[Arousal (Fold 10)] Ep 35/70 | Loss=0.0013 | Val Acc=73.17% | F1=0.825 | AUC=0.597\n",
      "[Arousal (Fold 10)] Ep 36/70 | Loss=0.0015 | Val Acc=70.73% | F1=0.806 | AUC=0.573\n",
      "[Arousal (Fold 10)] Ep 37/70 | Loss=0.0011 | Val Acc=73.17% | F1=0.825 | AUC=0.597\n",
      "[Arousal (Fold 10)] Ep 38/70 | Loss=0.0012 | Val Acc=73.17% | F1=0.825 | AUC=0.615\n",
      "[Arousal (Fold 10)] Ep 39/70 | Loss=0.0010 | Val Acc=73.17% | F1=0.825 | AUC=0.615\n",
      "[Arousal (Fold 10)] Ep 40/70 | Loss=0.0013 | Val Acc=73.17% | F1=0.825 | AUC=0.627\n",
      "â¹ [Arousal (Fold 10)] Early stopping at epoch 40\n",
      "\n",
      "ðŸ”š [Arousal (Fold 10)] TEST (best thr=0.100) | Acc=70.73% | F1=0.812 | AUC=0.582\n",
      "\n",
      "Fold 10 Results: Acc=0.7073, F1=0.8125, AUC=0.5818\n",
      "\n",
      "===== FINAL Arousal 10-FOLD RESULTS =====\n",
      "Accuracy: 0.7031 Â± 0.0307\n",
      "F1-score: 0.8150 Â± 0.0240\n",
      "AUC:      0.5440 Â± 0.1278\n",
      "\n",
      "===== OVERALL SUMMARY =====\n",
      "Valence Acc: mean=0.5819, std=0.0632\n",
      "Arousal Acc: mean=0.7031, std=0.0307\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# CNN + RNN (GRU) on READY-MADE SPECTROGRAMS\n",
    "# (No spectrogram conversion in this file)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------- DEVICE -------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Dataset wrapper (X already = spectrograms)\n",
    "# -------------------------------------------------\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.asarray(X, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]).float(),\n",
    "            torch.tensor(self.y[idx]).float(),\n",
    "        )\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Augmentations: SpecAugment + noise\n",
    "# (same as your original)\n",
    "# -------------------------------------------------\n",
    "def spec_augment(\n",
    "    X,\n",
    "    time_mask_width=4,\n",
    "    n_time_masks=2,\n",
    "    freq_mask_width=6,\n",
    "    n_freq_masks=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    X: [N, C, F, T] spectrograms\n",
    "    Applies random time & frequency masks (SpecAugment style).\n",
    "    \"\"\"\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "\n",
    "    for i in range(N):\n",
    "        # Time masks\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T:\n",
    "                continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "\n",
    "        # Frequency masks\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F:\n",
    "                continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. CNN + RNN (GRU) model (time-aware)\n",
    "# -------------------------------------------------\n",
    "class EEG_CNN_GRU(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN extracts [B, feat, F', T'].\n",
    "    Average over frequency -> [B, feat, T'] -> permute -> [B, T', feat]\n",
    "    GRU processes the temporal sequence; we use last-layer final hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=14, feat=128, rnn_hidden=64, rnn_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN backbone (similar capacity to your BiLSTM CNN)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),   # downsample spatial dims\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(128, feat, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(feat),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # keep spatial resolution in time dimension; frequency will be averaged\n",
    "        )\n",
    "\n",
    "        # GRU: input_size = feat (features per time step)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=feat,\n",
    "            hidden_size=rnn_hidden,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout if rnn_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # classifier from final hidden state of last GRU layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(rnn_hidden),\n",
    "            nn.Linear(rnn_hidden, rnn_hidden // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, F, T]\n",
    "        x = self.cnn(x)             # [B, feat, F', T']\n",
    "        # average over frequency -> [B, feat, T']\n",
    "        x = x.mean(dim=2)\n",
    "        # permute to [B, T', feat] for RNN\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # GRU: output, h_n\n",
    "        # h_n: [num_layers, B, hidden_size]\n",
    "        _, h_n = self.gru(x)\n",
    "        # take last layer's hidden state\n",
    "        h_last = h_n[-1]            # [B, hidden_size]\n",
    "\n",
    "        out = self.fc(h_last).squeeze(-1)  # [B]\n",
    "        return out\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Train ONE fold for one emotion\n",
    "# (keeps same behavior as your original)\n",
    "# -------------------------------------------------\n",
    "def train_one_fold_emotion(\n",
    "    X,\n",
    "    y_cont,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    emotion_name=\"Valence\",\n",
    "    epochs=70,\n",
    "    base_seed=42,\n",
    "    batch_size=16,\n",
    "):\n",
    "    # 1) Binarize labels w.r.t. TRAIN median\n",
    "    y_train_cont = y_cont[train_idx]\n",
    "    thr = np.median(y_train_cont)\n",
    "\n",
    "    y_train_bin = (y_train_cont >= thr).astype(float)\n",
    "    y_val_bin   = (y_cont[val_idx]  >= thr).astype(float)\n",
    "    y_test_bin  = (y_cont[test_idx] >= thr).astype(float)\n",
    "\n",
    "    print(f\"\\n[{emotion_name}] TRAIN median threshold = {thr:.4f}\")\n",
    "    print(\"  Train class counts:\", np.bincount(y_train_bin.astype(int)))\n",
    "    print(\"  Val   class counts:\", np.bincount(y_val_bin.astype(int)))\n",
    "    print(\"  Test  class counts:\", np.bincount(y_test_bin.astype(int)))\n",
    "\n",
    "    # 2) Standardize features using TRAIN only\n",
    "    X = np.nan_to_num(X)\n",
    "    X_train = X[train_idx]\n",
    "    X_val   = X[val_idx]\n",
    "    X_test  = X[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(len(train_idx), -1)\n",
    "    scaler.fit(X_train_flat)\n",
    "\n",
    "    def transform_block(X_block):\n",
    "        flat = X_block.reshape(X_block.shape[0], -1)\n",
    "        flat_scaled = scaler.transform(flat)\n",
    "        return flat_scaled.reshape(X_block.shape)\n",
    "\n",
    "    X_train_scaled = transform_block(X_train)\n",
    "    X_val_scaled   = transform_block(X_val)\n",
    "    X_test_scaled  = transform_block(X_test)\n",
    "\n",
    "    # 3) Strong augmentation: SpecAugment + noise\n",
    "    X_train_sa    = spec_augment(X_train_scaled)\n",
    "    X_train_noise = add_noise(X_train_scaled, std=0.03)\n",
    "\n",
    "    X_train_aug = np.concatenate(\n",
    "        [X_train_scaled, X_train_sa, X_train_noise],\n",
    "        axis=0,\n",
    "    )\n",
    "    y_train_aug = np.concatenate(\n",
    "        [y_train_bin, y_train_bin, y_train_bin],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # 4) DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        EEGDataset(X_train_aug, y_train_aug),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        EEGDataset(X_val_scaled, y_val_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        EEGDataset(X_test_scaled, y_test_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # 5) Model + optimizer + CLASS-WEIGHTED loss\n",
    "    torch.manual_seed(base_seed)\n",
    "    random.seed(base_seed)\n",
    "    np.random.seed(base_seed)\n",
    "\n",
    "    n_channels = X.shape[1]\n",
    "    model = EEG_CNN_GRU(\n",
    "        n_channels=n_channels,\n",
    "        feat=128,\n",
    "        rnn_hidden=64,\n",
    "        rnn_layers=2,\n",
    "        dropout=0.3\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts) > 0 else 1\n",
    "    pos = class_counts[1] if len(class_counts) > 1 else 1\n",
    "    pos_weight_val = float(neg) / float(pos) if pos > 0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    print(f\"[{emotion_name}] pos_weight for BCEWithLogitsLoss = {pos_weight_val:.3f}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "    patience = 18\n",
    "    counter = 0\n",
    "\n",
    "    print(f\"\\nðŸš€ Training {emotion_name} | epochs={epochs}\")\n",
    "    for ep in range(1, epochs + 1):\n",
    "        # ---- train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            n_samples += xb.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss = total_loss / max(1, n_samples)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        y_true_val, y_prob_val = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy())\n",
    "                y_prob_val.extend(probs)\n",
    "\n",
    "        y_true_val = np.array(y_true_val)\n",
    "        y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        val_f1  = f1_score(y_true_val, y_pred_val)\n",
    "        try:\n",
    "            val_auc = roc_auc_score(y_true_val, y_prob_val)\n",
    "        except:\n",
    "            val_auc = float(\"nan\")\n",
    "\n",
    "        print(\n",
    "            f\"[{emotion_name}] Ep {ep:02d}/{epochs} | \"\n",
    "            f\"Loss={train_loss:.4f} | Val Acc={val_acc*100:.2f}% | \"\n",
    "            f\"F1={val_f1:.3f} | AUC={val_auc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # early stopping on val accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience:\n",
    "                print(f\"â¹ [{emotion_name}] Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    # ---- load best & evaluate on TEST (with best threshold) ----\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    y_true_test, y_prob_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy())\n",
    "            y_prob_test.extend(probs)\n",
    "\n",
    "    y_true_test = np.array(y_true_test)\n",
    "    y_prob_test = np.array(y_prob_test)\n",
    "\n",
    "    # search threshold for best TEST accuracy\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for thr_ in np.linspace(0.1, 0.9, 17):\n",
    "        yp = (y_prob_test >= thr_).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc = acc_thr\n",
    "            best_thr = thr_\n",
    "\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1  = f1_score(y_true_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nðŸ”š [{emotion_name}] TEST (best thr={best_thr:.3f}) \"\n",
    "        f\"| Acc={test_acc*100:.2f}% | F1={test_f1:.3f} | AUC={test_auc:.3f}\\n\"\n",
    "    )\n",
    "\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. 10-fold CV driver\n",
    "# (same as your original)\n",
    "# -------------------------------------------------\n",
    "def run_10fold_cv_emotion(X, y_cont, emotion_name=\"Valence\", epochs=70, base_seed=42):\n",
    "    \"\"\"\n",
    "    10-fold stratified CV:\n",
    "      - Outer fold chooses TEST.\n",
    "      - Remaining data is split into TRAIN / VAL\n",
    "        so that VAL size â‰ˆ TEST size.\n",
    "    \"\"\"\n",
    "    print(f\"\\n########## {emotion_name}: 10-fold STRATIFIED CV ##########\")\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "\n",
    "    # For stratification: global median threshold\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    print(f\"{emotion_name} global median (for stratification) = {global_thr:.4f}\")\n",
    "    print(\"Class counts:\", np.bincount(y_bin_global.astype(int)))\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=10,\n",
    "        shuffle=True,\n",
    "        random_state=base_seed,\n",
    "    )\n",
    "\n",
    "    fold_accs, fold_f1s, fold_aucs = [], [], []\n",
    "\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(\n",
    "        skf.split(np.arange(len(y_cont)), y_bin_global),\n",
    "        start=1\n",
    "    ):\n",
    "        # Split trainval into train & val with val size ~= test size\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "\n",
    "        tv_train_idx, tv_val_idx = train_test_split(\n",
    "            trainval_idx,\n",
    "            test_size=val_frac,\n",
    "            random_state=base_seed + fold,\n",
    "            shuffle=True,\n",
    "            stratify=y_trainval_bin,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n----- {emotion_name} Fold {fold}/10 -----\")\n",
    "        print(\n",
    "            f\"Train size={len(tv_train_idx)}, \"\n",
    "            f\"Val size={len(tv_val_idx)}, \"\n",
    "            f\"Test size={len(test_idx)}\"\n",
    "        )\n",
    "\n",
    "        acc, f1, auc = train_one_fold_emotion(\n",
    "            X,\n",
    "            y_cont,\n",
    "            tv_train_idx,\n",
    "            tv_val_idx,\n",
    "            test_idx,\n",
    "            emotion_name=f\"{emotion_name} (Fold {fold})\",\n",
    "            epochs=epochs,\n",
    "            base_seed=base_seed + fold,\n",
    "            batch_size=16,\n",
    "        )\n",
    "\n",
    "        fold_accs.append(acc)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_aucs.append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold} Results: \"\n",
    "            f\"Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\"\n",
    "        )\n",
    "\n",
    "    fold_accs = np.array(fold_accs)\n",
    "    fold_f1s  = np.array(fold_f1s)\n",
    "    fold_aucs = np.array(fold_aucs)\n",
    "\n",
    "    print(f\"\\n===== FINAL {emotion_name} 10-FOLD RESULTS =====\")\n",
    "    print(f\"Accuracy: {fold_accs.mean():.4f} Â± {fold_accs.std():.4f}\")\n",
    "    print(f\"F1-score: {fold_f1s.mean():.4f} Â± {fold_f1s.std():.4f}\")\n",
    "    print(f\"AUC:      {fold_aucs.mean():.4f} Â± {fold_aucs.std():.4f}\")\n",
    "\n",
    "    return fold_accs, fold_f1s, fold_aucs\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. MAIN (assumes X, y_val, y_aro already created)\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure you've already run the cell that does:\n",
    "    # X, y_val, y_aro, groups, channels = load_dreamer_and_build_spectrograms(...)\n",
    "    print(\"\\nâœ… Spectrogram dataset:\", X.shape)\n",
    "\n",
    "    # Valence\n",
    "    val_accs, val_f1s, val_aucs = run_10fold_cv_emotion(\n",
    "        X, y_val, emotion_name=\"Valence\", epochs=70, base_seed=42\n",
    "    )\n",
    "\n",
    "    # Arousal\n",
    "    aro_accs, aro_f1s, aro_aucs = run_10fold_cv_emotion(\n",
    "        X, y_aro, emotion_name=\"Arousal\", epochs=70, base_seed=142\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== OVERALL SUMMARY =====\")\n",
    "    print(f\"Valence Acc: mean={val_accs.mean():.4f}, std={val_accs.std():.4f}\")\n",
    "    print(f\"Arousal Acc: mean={aro_accs.mean():.4f}, std={aro_accs.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4869f33e-19c8-4255-a48e-5f363bf2be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e80d950-51bd-4943-b0de-e41c8d83a8bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SVM (CPU)\n",
      "\n",
      "âœ… Spectrogram dataset: (414, 14, 36, 32)\n",
      "\n",
      "########## Valence: 10-fold STRATIFIED CV (SVM) ##########\n",
      "Valence global median (for stratification) = 3.0000\n",
      "Class counts: [161 253]\n",
      "\n",
      "----- Valence Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 1)] class_weight for SVC = {0: 1.2790697674418605, 1: 0.8208955223880597}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 1) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 1)] Validation | Val Acc=64.29% | F1=0.754 | AUC=0.524\n",
      "\n",
      "ðŸ”š [Valence (Fold 1)] TEST (best thr=0.500) | Acc=71.43% | F1=0.812 | AUC=0.627\n",
      "\n",
      "Fold 1 Results: Acc=0.7143, F1=0.8125, AUC=0.6274\n",
      "\n",
      "----- Valence Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 2)] class_weight for SVC = {0: 1.2790697674418605, 1: 0.8208955223880597}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 2) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 2)] Validation | Val Acc=50.00% | F1=0.604 | AUC=0.534\n",
      "\n",
      "ðŸ”š [Valence (Fold 2)] TEST (best thr=0.100) | Acc=61.90% | F1=0.765 | AUC=0.507\n",
      "\n",
      "Fold 2 Results: Acc=0.6190, F1=0.7647, AUC=0.5072\n",
      "\n",
      "----- Valence Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 3)] class_weight for SVC = {0: 1.2790697674418605, 1: 0.8208955223880597}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 3) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 3)] Validation | Val Acc=57.14% | F1=0.679 | AUC=0.471\n",
      "\n",
      "ðŸ”š [Valence (Fold 3)] TEST (best thr=0.150) | Acc=61.90% | F1=0.742 | AUC=0.558\n",
      "\n",
      "Fold 3 Results: Acc=0.6190, F1=0.7419, AUC=0.5577\n",
      "\n",
      "----- Valence Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [128 202]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [17 25]\n",
      "[Valence (Fold 4)] class_weight for SVC = {0: 1.2890625, 1: 0.8168316831683168}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 4) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 4)] Validation | Val Acc=59.52% | F1=0.638 | AUC=0.700\n",
      "\n",
      "ðŸ”š [Valence (Fold 4)] TEST (best thr=0.100) | Acc=59.52% | F1=0.730 | AUC=0.494\n",
      "\n",
      "Fold 4 Results: Acc=0.5952, F1=0.7302, AUC=0.4941\n",
      "\n",
      "----- Valence Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 5)] class_weight for SVC = {0: 1.2868217054263567, 1: 0.8177339901477833}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 5) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 5)] Validation | Val Acc=58.54% | F1=0.653 | AUC=0.648\n",
      "\n",
      "ðŸ”š [Valence (Fold 5)] TEST (best thr=0.100) | Acc=63.41% | F1=0.762 | AUC=0.460\n",
      "\n",
      "Fold 5 Results: Acc=0.6341, F1=0.7619, AUC=0.4600\n",
      "\n",
      "----- Valence Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 6)] class_weight for SVC = {0: 1.2868217054263567, 1: 0.8177339901477833}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 6) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 6)] Validation | Val Acc=48.78% | F1=0.618 | AUC=0.485\n",
      "\n",
      "ðŸ”š [Valence (Fold 6)] TEST (best thr=0.700) | Acc=73.17% | F1=0.756 | AUC=0.753\n",
      "\n",
      "Fold 6 Results: Acc=0.7317, F1=0.7556, AUC=0.7525\n",
      "\n",
      "----- Valence Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 7)] class_weight for SVC = {0: 1.2868217054263567, 1: 0.8177339901477833}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 7) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 7)] Validation | Val Acc=58.54% | F1=0.721 | AUC=0.453\n",
      "\n",
      "ðŸ”š [Valence (Fold 7)] TEST (best thr=0.100) | Acc=60.98% | F1=0.758 | AUC=0.453\n",
      "\n",
      "Fold 7 Results: Acc=0.6098, F1=0.7576, AUC=0.4525\n",
      "\n",
      "----- Valence Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 8)] class_weight for SVC = {0: 1.2868217054263567, 1: 0.8177339901477833}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 8) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 8)] Validation | Val Acc=60.98% | F1=0.714 | AUC=0.522\n",
      "\n",
      "ðŸ”š [Valence (Fold 8)] TEST (best thr=0.250) | Acc=63.41% | F1=0.769 | AUC=0.562\n",
      "\n",
      "Fold 8 Results: Acc=0.6341, F1=0.7692, AUC=0.5625\n",
      "\n",
      "----- Valence Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 9)] class_weight for SVC = {0: 1.2868217054263567, 1: 0.8177339901477833}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 9) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 9)] Validation | Val Acc=65.85% | F1=0.759 | AUC=0.695\n",
      "\n",
      "ðŸ”š [Valence (Fold 9)] TEST (best thr=0.100) | Acc=60.98% | F1=0.758 | AUC=0.395\n",
      "\n",
      "Fold 9 Results: Acc=0.6098, F1=0.7576, AUC=0.3950\n",
      "\n",
      "----- Valence Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 10)] class_weight for SVC = {0: 1.2868217054263567, 1: 0.8177339901477833}\n",
      "\n",
      "ðŸš€ Training SVM for Valence (Fold 10) (this may take a while depending on augmented train size)\n",
      "[Valence (Fold 10)] Validation | Val Acc=51.22% | F1=0.630 | AUC=0.520\n",
      "\n",
      "ðŸ”š [Valence (Fold 10)] TEST (best thr=0.350) | Acc=73.17% | F1=0.814 | AUC=0.688\n",
      "\n",
      "Fold 10 Results: Acc=0.7317, F1=0.8136, AUC=0.6875\n",
      "\n",
      "===== FINAL Valence 10-FOLD RESULTS (SVM) =====\n",
      "Accuracy: 0.6499 Â± 0.0511\n",
      "F1-score: 0.7665 Â± 0.0256\n",
      "AUC:      0.5496 Â± 0.1062\n",
      "\n",
      "########## Arousal: 10-fold STRATIFIED CV (SVM) ##########\n",
      "Arousal global median (for stratification) = 3.0000\n",
      "Class counts: [114 300]\n",
      "\n",
      "----- Arousal Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 1)] class_weight for SVC = {0: 1.8333333333333333, 1: 0.6875}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 1) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 1)] Validation | Val Acc=71.43% | F1=0.829 | AUC=0.564\n",
      "\n",
      "ðŸ”š [Arousal (Fold 1)] TEST (best thr=0.450) | Acc=73.81% | F1=0.841 | AUC=0.514\n",
      "\n",
      "Fold 1 Results: Acc=0.7381, F1=0.8406, AUC=0.5139\n",
      "\n",
      "----- Arousal Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 2)] class_weight for SVC = {0: 1.8333333333333333, 1: 0.6875}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 2) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 2)] Validation | Val Acc=64.29% | F1=0.783 | AUC=0.444\n",
      "\n",
      "ðŸ”š [Arousal (Fold 2)] TEST (best thr=0.100) | Acc=69.05% | F1=0.817 | AUC=0.528\n",
      "\n",
      "Fold 2 Results: Acc=0.6905, F1=0.8169, AUC=0.5278\n",
      "\n",
      "----- Arousal Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 3)] class_weight for SVC = {0: 1.8333333333333333, 1: 0.6875}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 3) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 3)] Validation | Val Acc=69.05% | F1=0.817 | AUC=0.517\n",
      "\n",
      "ðŸ”š [Arousal (Fold 3)] TEST (best thr=0.100) | Acc=71.43% | F1=0.833 | AUC=0.711\n",
      "\n",
      "Fold 3 Results: Acc=0.7143, F1=0.8333, AUC=0.7111\n",
      "\n",
      "----- Arousal Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 4)] class_weight for SVC = {0: 1.8333333333333333, 1: 0.6875}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 4) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 4)] Validation | Val Acc=64.29% | F1=0.783 | AUC=0.336\n",
      "\n",
      "ðŸ”š [Arousal (Fold 4)] TEST (best thr=0.200) | Acc=73.81% | F1=0.845 | AUC=0.558\n",
      "\n",
      "Fold 4 Results: Acc=0.7381, F1=0.8451, AUC=0.5583\n",
      "\n",
      "----- Arousal Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 5)] class_weight for SVC = {0: 1.8043478260869565, 1: 0.6916666666666667}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 5) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 5)] Validation | Val Acc=73.17% | F1=0.845 | AUC=0.458\n",
      "\n",
      "ðŸ”š [Arousal (Fold 5)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.548\n",
      "\n",
      "Fold 5 Results: Acc=0.7317, F1=0.8451, AUC=0.5485\n",
      "\n",
      "----- Arousal Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 6)] class_weight for SVC = {0: 1.8043478260869565, 1: 0.6916666666666667}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 6) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 6)] Validation | Val Acc=65.85% | F1=0.750 | AUC=0.655\n",
      "\n",
      "ðŸ”š [Arousal (Fold 6)] TEST (best thr=0.300) | Acc=78.05% | F1=0.866 | AUC=0.609\n",
      "\n",
      "Fold 6 Results: Acc=0.7805, F1=0.8657, AUC=0.6091\n",
      "\n",
      "----- Arousal Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 7)] class_weight for SVC = {0: 1.8043478260869565, 1: 0.6916666666666667}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 7) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 7)] Validation | Val Acc=70.73% | F1=0.829 | AUC=0.624\n",
      "\n",
      "ðŸ”š [Arousal (Fold 7)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.621\n",
      "\n",
      "Fold 7 Results: Acc=0.7317, F1=0.8451, AUC=0.6212\n",
      "\n",
      "----- Arousal Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 8)] class_weight for SVC = {0: 1.8043478260869565, 1: 0.6916666666666667}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 8) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 8)] Validation | Val Acc=70.73% | F1=0.818 | AUC=0.602\n",
      "\n",
      "ðŸ”š [Arousal (Fold 8)] TEST (best thr=0.100) | Acc=70.73% | F1=0.829 | AUC=0.567\n",
      "\n",
      "Fold 8 Results: Acc=0.7073, F1=0.8286, AUC=0.5667\n",
      "\n",
      "----- Arousal Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 9)] class_weight for SVC = {0: 1.8043478260869565, 1: 0.6916666666666667}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 9) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 9)] Validation | Val Acc=63.41% | F1=0.776 | AUC=0.594\n",
      "\n",
      "ðŸ”š [Arousal (Fold 9)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.530\n",
      "\n",
      "Fold 9 Results: Acc=0.7317, F1=0.8451, AUC=0.5303\n",
      "\n",
      "----- Arousal Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 10)] class_weight for SVC = {0: 1.8043478260869565, 1: 0.6916666666666667}\n",
      "\n",
      "ðŸš€ Training SVM for Arousal (Fold 10) (this may take a while depending on augmented train size)\n",
      "[Arousal (Fold 10)] Validation | Val Acc=70.73% | F1=0.824 | AUC=0.594\n",
      "\n",
      "ðŸ”š [Arousal (Fold 10)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.579\n",
      "\n",
      "Fold 10 Results: Acc=0.7317, F1=0.8451, AUC=0.5788\n",
      "\n",
      "===== FINAL Arousal 10-FOLD RESULTS (SVM) =====\n",
      "Accuracy: 0.7296 Â± 0.0224\n",
      "F1-score: 0.8410 Â± 0.0122\n",
      "AUC:      0.5766 Â± 0.0555\n",
      "\n",
      "===== OVERALL SUMMARY (SVM) =====\n",
      "Valence Acc: mean=0.6499, std=0.0511\n",
      "Arousal Acc: mean=0.7296, std=0.0224\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# SVM (RBF) on READY-MADE SPECTROGRAMS\n",
    "# (No spectrogram conversion in this file)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ----------------- (optional) torch left out since not needed for SVM -------------------\n",
    "# DEVICE not required for SVM; keeping a small print for parity\n",
    "print(\"Running SVM (CPU)\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Augmentations: SpecAugment + noise\n",
    "# (same as your original)\n",
    "# -------------------------------------------------\n",
    "def spec_augment(\n",
    "    X,\n",
    "    time_mask_width=4,\n",
    "    n_time_masks=2,\n",
    "    freq_mask_width=6,\n",
    "    n_freq_masks=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    X: [N, C, F, T] spectrograms\n",
    "    Applies random time & frequency masks (SpecAugment style).\n",
    "    \"\"\"\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "\n",
    "    for i in range(N):\n",
    "        # Time masks\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T:\n",
    "                continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "\n",
    "        # Frequency masks\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F:\n",
    "                continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Train ONE fold for one emotion (SVM)\n",
    "# -------------------------------------------------\n",
    "def train_one_fold_emotion(\n",
    "    X,\n",
    "    y_cont,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    emotion_name=\"Valence\",\n",
    "    epochs=1,            # kept for API parity; ignored for SVM\n",
    "    base_seed=42,\n",
    "    batch_size=16,       # ignored for SVM\n",
    "):\n",
    "    # 1) Binarize labels w.r.t. TRAIN median\n",
    "    y_train_cont = y_cont[train_idx]\n",
    "    thr = np.median(y_train_cont)\n",
    "\n",
    "    y_train_bin = (y_train_cont >= thr).astype(int)\n",
    "    y_val_bin   = (y_cont[val_idx]  >= thr).astype(int)\n",
    "    y_test_bin  = (y_cont[test_idx] >= thr).astype(int)\n",
    "\n",
    "    print(f\"\\n[{emotion_name}] TRAIN median threshold = {thr:.4f}\")\n",
    "    print(\"  Train class counts:\", np.bincount(y_train_bin.astype(int)))\n",
    "    print(\"  Val   class counts:\", np.bincount(y_val_bin.astype(int)))\n",
    "    print(\"  Test  class counts:\", np.bincount(y_test_bin.astype(int)))\n",
    "\n",
    "    # 2) Standardize features using TRAIN only\n",
    "    X = np.nan_to_num(X)\n",
    "    X_train = X[train_idx]\n",
    "    X_val   = X[val_idx]\n",
    "    X_test  = X[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(len(train_idx), -1)\n",
    "    scaler.fit(X_train_flat)\n",
    "\n",
    "    def transform_flat(X_block):\n",
    "        flat = X_block.reshape(X_block.shape[0], -1)\n",
    "        flat_scaled = scaler.transform(flat)\n",
    "        return flat_scaled\n",
    "\n",
    "    X_train_scaled_flat = transform_flat(X_train)\n",
    "    X_val_scaled_flat   = transform_flat(X_val)\n",
    "    X_test_scaled_flat  = transform_flat(X_test)\n",
    "\n",
    "    # 3) Strong augmentation: SpecAugment + noise (on the ORIGINAL scaled shaped data)\n",
    "    # We must reshape augmented data to flat form for SVM\n",
    "    X_train_scaled = X_train.reshape(X_train.shape)  # copy shape reference\n",
    "    # Actually use the scaled **shaped** version to apply spec_augment and add_noise consistently:\n",
    "    X_train_shaped_scaled = X_train.reshape(X_train.shape)  # placeholder: we'll reconstruct shaped scaled block\n",
    "    # To get shaped scaled block: inverse of flattening is needed: use the scaled flat and reshape\n",
    "    X_train_shaped_scaled = X_train_scaled_flat.reshape(X_train.shape)\n",
    "\n",
    "    X_train_sa_shaped = spec_augment(X_train_shaped_scaled)\n",
    "    X_train_noise_shaped = add_noise(X_train_shaped_scaled, std=0.03)\n",
    "\n",
    "    # Flatten augmented to feed SVM\n",
    "    X_train_sa_flat    = X_train_sa_shaped.reshape(X_train_sa_shaped.shape[0], -1)\n",
    "    X_train_noise_flat = X_train_noise_shaped.reshape(X_train_noise_shaped.shape[0], -1)\n",
    "\n",
    "    # Already have X_train_scaled_flat (original scaled)\n",
    "    X_train_aug_flat = np.concatenate(\n",
    "        [X_train_scaled_flat, X_train_sa_flat, X_train_noise_flat],\n",
    "        axis=0,\n",
    "    )\n",
    "    y_train_aug = np.concatenate(\n",
    "        [y_train_bin, y_train_bin, y_train_bin],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # 4) Validation / test flat arrays already prepared\n",
    "    X_val_flat = X_val_scaled_flat\n",
    "    y_val = y_val_bin\n",
    "    X_test_flat = X_test_scaled_flat\n",
    "    y_test = y_test_bin\n",
    "\n",
    "    # 5) SVM classifier + class weighting\n",
    "    np.random.seed(base_seed)\n",
    "    random.seed(base_seed)\n",
    "\n",
    "    # compute class_weight dict (so SVC sees balanced classes)\n",
    "    classes = np.unique(y_train_bin)\n",
    "    cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_bin)\n",
    "    class_weight_dict = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "    print(f\"[{emotion_name}] class_weight for SVC = {class_weight_dict}\")\n",
    "\n",
    "    clf = SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        class_weight=class_weight_dict,\n",
    "        probability=True,    # enable predict_proba for thresholding/AUC\n",
    "        random_state=base_seed,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nðŸš€ Training SVM for {emotion_name} (this may take a while depending on augmented train size)\")\n",
    "    clf.fit(X_train_aug_flat, y_train_aug)\n",
    "\n",
    "    # ---- validation evaluation (for parity printing) ----\n",
    "    y_prob_val = clf.predict_proba(X_val_flat)[:, 1]\n",
    "    y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    val_f1  = f1_score(y_val, y_pred_val)\n",
    "    try:\n",
    "        val_auc = roc_auc_score(y_val, y_prob_val)\n",
    "    except:\n",
    "        val_auc = float(\"nan\")\n",
    "\n",
    "    print(\n",
    "        f\"[{emotion_name}] Validation | Val Acc={val_acc*100:.2f}% | \"\n",
    "        f\"F1={val_f1:.3f} | AUC={val_auc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---- TEST evaluation & threshold search ----\n",
    "    y_prob_test = clf.predict_proba(X_test_flat)[:, 1]\n",
    "\n",
    "    # search threshold for best TEST accuracy\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for thr_ in np.linspace(0.1, 0.9, 17):\n",
    "        yp = (y_prob_test >= thr_).astype(int)\n",
    "        acc_thr = accuracy_score(y_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc = acc_thr\n",
    "            best_thr = thr_\n",
    "\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_test, y_pred_best)\n",
    "    test_f1  = f1_score(y_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nðŸ”š [{emotion_name}] TEST (best thr={best_thr:.3f}) \"\n",
    "        f\"| Acc={test_acc*100:.2f}% | F1={test_f1:.3f} | AUC={test_auc:.3f}\\n\"\n",
    "    )\n",
    "\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. 10-fold CV driver (same splitting as your original)\n",
    "# -------------------------------------------------\n",
    "def run_10fold_cv_emotion(X, y_cont, emotion_name=\"Valence\", epochs=1, base_seed=42):\n",
    "    \"\"\"\n",
    "    10-fold stratified CV:\n",
    "      - Outer fold chooses TEST.\n",
    "      - Remaining data is split into TRAIN / VAL\n",
    "        so that VAL size â‰ˆ TEST size.\n",
    "    \"\"\"\n",
    "    print(f\"\\n########## {emotion_name}: 10-fold STRATIFIED CV (SVM) ##########\")\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "\n",
    "    # For stratification: global median threshold\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    print(f\"{emotion_name} global median (for stratification) = {global_thr:.4f}\")\n",
    "    print(\"Class counts:\", np.bincount(y_bin_global.astype(int)))\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=10,\n",
    "        shuffle=True,\n",
    "        random_state=base_seed,\n",
    "    )\n",
    "\n",
    "    fold_accs, fold_f1s, fold_aucs = [], [], []\n",
    "\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(\n",
    "        skf.split(np.arange(len(y_cont)), y_bin_global),\n",
    "        start=1\n",
    "    ):\n",
    "        # Split trainval into train & val with val size ~= test size\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "\n",
    "        tv_train_idx, tv_val_idx = train_test_split(\n",
    "            trainval_idx,\n",
    "            test_size=val_frac,\n",
    "            random_state=base_seed + fold,\n",
    "            shuffle=True,\n",
    "            stratify=y_trainval_bin,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n----- {emotion_name} Fold {fold}/10 -----\")\n",
    "        print(\n",
    "            f\"Train size={len(tv_train_idx)}, \"\n",
    "            f\"Val size={len(tv_val_idx)}, \"\n",
    "            f\"Test size={len(test_idx)}\"\n",
    "        )\n",
    "\n",
    "        acc, f1, auc = train_one_fold_emotion(\n",
    "            X,\n",
    "            y_cont,\n",
    "            tv_train_idx,\n",
    "            tv_val_idx,\n",
    "            test_idx,\n",
    "            emotion_name=f\"{emotion_name} (Fold {fold})\",\n",
    "            epochs=epochs,\n",
    "            base_seed=base_seed + fold,\n",
    "            batch_size=16,\n",
    "        )\n",
    "\n",
    "        fold_accs.append(acc)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_aucs.append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold} Results: \"\n",
    "            f\"Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\"\n",
    "        )\n",
    "\n",
    "    fold_accs = np.array(fold_accs)\n",
    "    fold_f1s  = np.array(fold_f1s)\n",
    "    fold_aucs = np.array(fold_aucs)\n",
    "\n",
    "    print(f\"\\n===== FINAL {emotion_name} 10-FOLD RESULTS (SVM) =====\")\n",
    "    print(f\"Accuracy: {fold_accs.mean():.4f} Â± {fold_accs.std():.4f}\")\n",
    "    print(f\"F1-score: {fold_f1s.mean():.4f} Â± {fold_f1s.std():.4f}\")\n",
    "    print(f\"AUC:      {fold_aucs.mean():.4f} Â± {fold_aucs.std():.4f}\")\n",
    "\n",
    "    return fold_accs, fold_f1s, fold_aucs\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. MAIN (assumes X, y_val, y_aro already created)\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure you've already run the cell that does:\n",
    "    # X, y_val, y_aro, groups, channels = load_dreamer_and_build_spectrograms(...)\n",
    "    print(\"\\nâœ… Spectrogram dataset:\", X.shape)\n",
    "\n",
    "    # Valence\n",
    "    val_accs, val_f1s, val_aucs = run_10fold_cv_emotion(\n",
    "        X, y_val, emotion_name=\"Valence\", epochs=1, base_seed=42\n",
    "    )\n",
    "\n",
    "    # Arousal\n",
    "    aro_accs, aro_f1s, aro_aucs = run_10fold_cv_emotion(\n",
    "        X, y_aro, emotion_name=\"Arousal\", epochs=1, base_seed=142\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== OVERALL SUMMARY (SVM) =====\")\n",
    "    print(f\"Valence Acc: mean={val_accs.mean():.4f}, std={val_accs.std():.4f}\")\n",
    "    print(f\"Arousal Acc: mean={aro_accs.mean():.4f}, std={aro_accs.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152fb623-9a38-46db-87ea-07e9cdc6a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gcForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ed17613-c889-4bc6-9530-8c09610d3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since deep-forest and other gcForest packages aren't available in your environment, I made a pure scikit-learn cascade-forest implementation you can run immediately with no external dependencies.\n",
    "#It mimics gcForest / DeepForest behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5836acc-c8ad-4b66-b363-c2de873d441b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Spectrogram dataset: (414, 14, 36, 32)\n",
      "\n",
      "########## Valence: 10-fold STRATIFIED CV (Cascade) ##########\n",
      "Valence global median (for stratification) = 3.0000\n",
      "Class counts: [161 253]\n",
      "\n",
      "----- Valence Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5373\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5072\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5601\n",
      "\n",
      "ðŸ”š [Valence (Fold 1)] TEST (best thr=0.550) | Acc=66.67% | F1=0.759 | AUC=0.456\n",
      "\n",
      "Fold 1 Results: Acc=0.6667, F1=0.7586, AUC=0.4555\n",
      "\n",
      "----- Valence Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6022\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5877\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6562\n",
      "\n",
      "ðŸ”š [Valence (Fold 2)] TEST (best thr=0.400) | Acc=64.29% | F1=0.769 | AUC=0.486\n",
      "\n",
      "Fold 2 Results: Acc=0.6429, F1=0.7692, AUC=0.4856\n",
      "\n",
      "----- Valence Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4026\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4291\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4375\n",
      "\n",
      "ðŸ”š [Valence (Fold 3)] TEST (best thr=0.100) | Acc=61.90% | F1=0.765 | AUC=0.524\n",
      "\n",
      "Fold 3 Results: Acc=0.6190, F1=0.7647, AUC=0.5240\n",
      "\n",
      "----- Valence Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [128 202]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [17 25]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6478\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6478\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6490\n",
      "\n",
      "ðŸ”š [Valence (Fold 4)] TEST (best thr=0.100) | Acc=59.52% | F1=0.746 | AUC=0.393\n",
      "\n",
      "Fold 4 Results: Acc=0.5952, F1=0.7463, AUC=0.3929\n",
      "\n",
      "----- Valence Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6812\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.7250\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6463\n",
      "\n",
      "ðŸ”š [Valence (Fold 5)] TEST (best thr=0.450) | Acc=68.29% | F1=0.780 | AUC=0.641\n",
      "\n",
      "Fold 5 Results: Acc=0.6829, F1=0.7797, AUC=0.6413\n",
      "\n",
      "----- Valence Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4363\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5175\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4925\n",
      "\n",
      "ðŸ”š [Valence (Fold 6)] TEST (best thr=0.450) | Acc=70.73% | F1=0.793 | AUC=0.752\n",
      "\n",
      "Fold 6 Results: Acc=0.7073, F1=0.7931, AUC=0.7525\n",
      "\n",
      "----- Valence Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4300\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5200\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5312\n",
      "\n",
      "ðŸ”š [Valence (Fold 7)] TEST (best thr=0.450) | Acc=63.41% | F1=0.754 | AUC=0.404\n",
      "\n",
      "Fold 7 Results: Acc=0.6341, F1=0.7541, AUC=0.4037\n",
      "\n",
      "----- Valence Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4275\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5013\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4562\n",
      "\n",
      "ðŸ”š [Valence (Fold 8)] TEST (best thr=0.550) | Acc=63.41% | F1=0.762 | AUC=0.583\n",
      "\n",
      "Fold 8 Results: Acc=0.6341, F1=0.7619, AUC=0.5825\n",
      "\n",
      "----- Valence Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5300\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4762\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5688\n",
      "\n",
      "ðŸ”š [Valence (Fold 9)] TEST (best thr=0.100) | Acc=60.98% | F1=0.758 | AUC=0.524\n",
      "\n",
      "Fold 9 Results: Acc=0.6098, F1=0.7576, AUC=0.5237\n",
      "\n",
      "----- Valence Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4450\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4262\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4487\n",
      "\n",
      "ðŸ”š [Valence (Fold 10)] TEST (best thr=0.500) | Acc=68.29% | F1=0.745 | AUC=0.600\n",
      "\n",
      "Fold 10 Results: Acc=0.6829, F1=0.7451, AUC=0.6000\n",
      "\n",
      "===== FINAL Valence 10-FOLD RESULTS (Cascade) =====\n",
      "Accuracy: 0.6475 Â± 0.0343\n",
      "F1-score: 0.7630 Â± 0.0140\n",
      "AUC:      0.5362 Â± 0.1056\n",
      "\n",
      "########## Arousal: 10-fold STRATIFIED CV (Cascade) ##########\n",
      "Arousal global median (for stratification) = 3.0000\n",
      "Class counts: [114 300]\n",
      "\n",
      "----- Arousal Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6208\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6097\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5986\n",
      "No improvement for 2 layers â€” stopping cascade.\n",
      "\n",
      "ðŸ”š [Arousal (Fold 1)] TEST (best thr=0.100) | Acc=71.43% | F1=0.833 | AUC=0.539\n",
      "\n",
      "Fold 1 Results: Acc=0.7143, F1=0.8333, AUC=0.5389\n",
      "\n",
      "----- Arousal Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5653\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5611\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5375\n",
      "No improvement for 2 layers â€” stopping cascade.\n",
      "\n",
      "ðŸ”š [Arousal (Fold 2)] TEST (best thr=0.300) | Acc=73.81% | F1=0.845 | AUC=0.562\n",
      "\n",
      "Fold 2 Results: Acc=0.7381, F1=0.8451, AUC=0.5625\n",
      "\n",
      "----- Arousal Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6000\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4819\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.3889\n",
      "No improvement for 2 layers â€” stopping cascade.\n",
      "\n",
      "ðŸ”š [Arousal (Fold 3)] TEST (best thr=0.500) | Acc=78.57% | F1=0.857 | AUC=0.599\n",
      "\n",
      "Fold 3 Results: Acc=0.7857, F1=0.8571, AUC=0.5986\n",
      "\n",
      "----- Arousal Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4819\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5000\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4403\n",
      "\n",
      "ðŸ”š [Arousal (Fold 4)] TEST (best thr=0.100) | Acc=71.43% | F1=0.833 | AUC=0.518\n",
      "\n",
      "Fold 4 Results: Acc=0.7143, F1=0.8333, AUC=0.5181\n",
      "\n",
      "----- Arousal Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5788\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4712\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5485\n",
      "No improvement for 2 layers â€” stopping cascade.\n",
      "\n",
      "ðŸ”š [Arousal (Fold 5)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.580\n",
      "\n",
      "Fold 5 Results: Acc=0.7317, F1=0.8451, AUC=0.5803\n",
      "\n",
      "----- Arousal Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6167\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5561\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5394\n",
      "No improvement for 2 layers â€” stopping cascade.\n",
      "\n",
      "ðŸ”š [Arousal (Fold 6)] TEST (best thr=0.600) | Acc=75.61% | F1=0.833 | AUC=0.665\n",
      "\n",
      "Fold 6 Results: Acc=0.7561, F1=0.8333, AUC=0.6652\n",
      "\n",
      "----- Arousal Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6591\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6227\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6712\n",
      "\n",
      "ðŸ”š [Arousal (Fold 7)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.524\n",
      "\n",
      "Fold 7 Results: Acc=0.7317, F1=0.8451, AUC=0.5242\n",
      "\n",
      "----- Arousal Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6545\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5591\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6091\n",
      "No improvement for 2 layers â€” stopping cascade.\n",
      "\n",
      "ðŸ”š [Arousal (Fold 8)] TEST (best thr=0.500) | Acc=82.93% | F1=0.896 | AUC=0.574\n",
      "\n",
      "Fold 8 Results: Acc=0.8293, F1=0.8955, AUC=0.5742\n",
      "\n",
      "----- Arousal Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6273\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5697\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6318\n",
      "\n",
      "ðŸ”š [Arousal (Fold 9)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.459\n",
      "\n",
      "Fold 9 Results: Acc=0.7317, F1=0.8451, AUC=0.4591\n",
      "\n",
      "----- Arousal Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4076\n",
      "\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4985\n",
      "\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5303\n",
      "\n",
      "ðŸ”š [Arousal (Fold 10)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.585\n",
      "\n",
      "Fold 10 Results: Acc=0.7317, F1=0.8451, AUC=0.5848\n",
      "\n",
      "===== FINAL Arousal 10-FOLD RESULTS (Cascade) =====\n",
      "Accuracy: 0.7465 Â± 0.0339\n",
      "F1-score: 0.8478 Â± 0.0174\n",
      "AUC:      0.5606 Â± 0.0524\n",
      "\n",
      "===== OVERALL SUMMARY (Cascade) =====\n",
      "Valence Acc: mean=0.6475, std=0.0343\n",
      "Aroual Acc:  mean=0.7465, std=0.0339\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Pure-sklearn Cascade Forest (gcForest-like)\n",
    "# Works without external packages (Windows / Python 3.10+)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# ----------------- Augmentations (same as yours) -----------------\n",
    "def spec_augment(X, time_mask_width=4, n_time_masks=2, freq_mask_width=6, n_freq_masks=2):\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "    for i in range(N):\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T:\n",
    "                continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F:\n",
    "                continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "    return X_aug\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# ----------------- Cascade helpers -----------------\n",
    "def fit_layer_oof_preds(X, y, estimators, n_splits=5, random_state=0):\n",
    "    \"\"\"\n",
    "    For each estimator in `estimators`, perform OOF predictions (probabilities)\n",
    "    using StratifiedKFold to produce meta-features.\n",
    "    Returns:\n",
    "      - meta_train: shape (n_samples, n_estimators*2)  (prob for class1 and class0? we'll use prob for class1 only)\n",
    "      - fitted_estimators: list of estimators fitted on full X\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    meta_cols = []\n",
    "    oof_preds = []\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for est in estimators:\n",
    "        # oof prob for positive class\n",
    "        oof = np.zeros(n_samples, dtype=np.float32)\n",
    "        for tr, te in skf.split(np.arange(n_samples), y):\n",
    "            est_clone = deepcopy(est)\n",
    "            est_clone.random_state = random_state  # attempt to fix randomness if attr exists\n",
    "            est_clone.fit(X[tr], y[tr])\n",
    "            if hasattr(est_clone, \"predict_proba\"):\n",
    "                p = est_clone.predict_proba(X[te])[:, 1]\n",
    "            else:\n",
    "                # fallback to decision_function mapped to [0,1]\n",
    "                if hasattr(est_clone, \"decision_function\"):\n",
    "                    scores = est_clone.decision_function(X[te])\n",
    "                    p = 1.0 / (1.0 + np.exp(-scores))\n",
    "                else:\n",
    "                    p = est_clone.predict(X[te])\n",
    "            oof[te] = p\n",
    "        oof_preds.append(oof.reshape(-1, 1))\n",
    "\n",
    "    meta_train = np.hstack(oof_preds)  # shape (n_samples, n_estimators)\n",
    "    # now fit each estimator on full data (for later inference)\n",
    "    fitted = []\n",
    "    for est in estimators:\n",
    "        est_clone = deepcopy(est)\n",
    "        try:\n",
    "            est_clone.random_state = random_state\n",
    "        except Exception:\n",
    "            pass\n",
    "        est_clone.fit(X, y)\n",
    "        fitted.append(est_clone)\n",
    "    return meta_train, fitted\n",
    "\n",
    "def transform_with_layer(X, fitted_estimators):\n",
    "    \"\"\"\n",
    "    Produce meta features for X using fitted estimators (predict_proba[:,1]).\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for est in fitted_estimators:\n",
    "        if hasattr(est, \"predict_proba\"):\n",
    "            p = est.predict_proba(X)[:, 1].reshape(-1, 1)\n",
    "        else:\n",
    "            if hasattr(est, \"decision_function\"):\n",
    "                scores = est.decision_function(X)\n",
    "                p = (1.0 / (1.0 + np.exp(-scores))).reshape(-1, 1)\n",
    "            else:\n",
    "                p = est.predict(X).reshape(-1, 1)\n",
    "        preds.append(p)\n",
    "    return np.hstack(preds)  # shape (n_samples, n_estimators)\n",
    "\n",
    "# ----------------- Cascade training procedure -----------------\n",
    "def train_cascade(X_train, y_train, X_val, y_val,\n",
    "                  base_estimators=None,\n",
    "                  max_layers=3,\n",
    "                  n_splits=5,\n",
    "                  random_state=0,\n",
    "                  early_stopping_rounds=2):\n",
    "    \"\"\"\n",
    "    Trains a cascade of layers. Each layer uses `base_estimators` (list of sklearn estimators)\n",
    "    to produce OOF meta-features for stacking into the next layer.\n",
    "    Returns:\n",
    "      - layers: list of dicts {'estimators': fitted_estimators}\n",
    "      - X_train_trans: final transformed train features (original flatten + all meta features)\n",
    "      - X_val_trans:   final transformed val features\n",
    "    \"\"\"\n",
    "    if base_estimators is None:\n",
    "        base_estimators = [\n",
    "            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini')\n",
    "        ]\n",
    "\n",
    "    layers = []\n",
    "    Xtr = X_train.copy()\n",
    "    Xv = X_val.copy()\n",
    "    best_val_auc = -np.inf\n",
    "    no_improve = 0\n",
    "\n",
    "    for layer_idx in range(1, max_layers + 1):\n",
    "        print(f\"\\n--- Cascade Layer {layer_idx} ---\")\n",
    "        # produce OOF meta features on Xtr\n",
    "        meta_tr, fitted = fit_layer_oof_preds(Xtr, y_train, base_estimators, n_splits=n_splits, random_state=random_state+layer_idx)\n",
    "        # transform Xv using fitted estimators\n",
    "        meta_val = transform_with_layer(Xv, fitted)\n",
    "\n",
    "        # append meta features to current features\n",
    "        Xtr = np.hstack([Xtr, meta_tr])\n",
    "        Xv  = np.hstack([Xv, meta_val])\n",
    "\n",
    "        layers.append({'estimators': fitted})  # store fitted estimators for inference\n",
    "\n",
    "        # quick validator: train a small classifier on transformed features and evaluate on val\n",
    "        # we'll use a lightweight RandomForest for validation performance check\n",
    "        val_clf = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=random_state+layer_idx)\n",
    "        val_clf.fit(Xtr, y_train)\n",
    "        try:\n",
    "            y_val_prob = val_clf.predict_proba(Xv)[:, 1]\n",
    "        except:\n",
    "            if hasattr(val_clf, \"decision_function\"):\n",
    "                y_val_prob = 1.0 / (1.0 + np.exp(-val_clf.decision_function(Xv)))\n",
    "            else:\n",
    "                y_val_prob = val_clf.predict(Xv)\n",
    "\n",
    "        val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "        print(f\"Layer {layer_idx} validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # stopping condition based on val AUC improvement\n",
    "        if val_auc > best_val_auc + 1e-4:\n",
    "            best_val_auc = val_auc\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stopping_rounds:\n",
    "                print(f\"No improvement for {early_stopping_rounds} layers â€” stopping cascade.\")\n",
    "                break\n",
    "\n",
    "    # final Xtr and Xv contain original + all meta-features created\n",
    "    return layers, Xtr, Xv\n",
    "\n",
    "def predict_cascade(layers, X_raw):\n",
    "    \"\"\"\n",
    "    Given fitted layers (list of dicts with 'estimators'), transform X_raw through layers\n",
    "    and return final transformed features (original + concatenated meta-features).\n",
    "    \"\"\"\n",
    "    Xcur = X_raw.copy()\n",
    "    for layer in layers:\n",
    "        fitted = layer['estimators']\n",
    "        meta = transform_with_layer(Xcur, fitted)\n",
    "        Xcur = np.hstack([Xcur, meta])\n",
    "    return Xcur\n",
    "\n",
    "# ----------------- Training wrapper (mirrors your pipeline) -----------------\n",
    "def train_one_fold_emotion(\n",
    "    X,\n",
    "    y_cont,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    emotion_name=\"Valence\",\n",
    "    base_seed=42,\n",
    "    max_layers=3,\n",
    "    n_splits_oof=5,\n",
    "    batch_size_unused=16,\n",
    "):\n",
    "    # 1) Binarize labels w.r.t. TRAIN median\n",
    "    y_train_cont = y_cont[train_idx]\n",
    "    thr = np.median(y_train_cont)\n",
    "\n",
    "    y_train_bin = (y_train_cont >= thr).astype(int)\n",
    "    y_val_bin   = (y_cont[val_idx]  >= thr).astype(int)\n",
    "    y_test_bin  = (y_cont[test_idx] >= thr).astype(int)\n",
    "\n",
    "    print(f\"\\n[{emotion_name}] TRAIN median threshold = {thr:.4f}\")\n",
    "    print(\"  Train class counts:\", np.bincount(y_train_bin.astype(int)))\n",
    "    print(\"  Val   class counts:\", np.bincount(y_val_bin.astype(int)))\n",
    "    print(\"  Test  class counts:\", np.bincount(y_test_bin.astype(int)))\n",
    "\n",
    "    # 2) Standardize features using TRAIN only\n",
    "    X = np.nan_to_num(X)\n",
    "    X_train = X[train_idx]\n",
    "    X_val   = X[val_idx]\n",
    "    X_test  = X[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(len(train_idx), -1)\n",
    "    scaler.fit(X_train_flat)\n",
    "\n",
    "    def transform_block_flat(X_block):\n",
    "        flat = X_block.reshape(X_block.shape[0], -1)\n",
    "        flat_scaled = scaler.transform(flat)\n",
    "        return flat_scaled\n",
    "\n",
    "    X_train_scaled_flat = transform_block_flat(X_train)\n",
    "    X_val_scaled_flat   = transform_block_flat(X_val)\n",
    "    X_test_scaled_flat  = transform_block_flat(X_test)\n",
    "\n",
    "    # 3) Augment training set (SpecAugment + noise) on shaped scaled data\n",
    "    X_train_shaped_scaled = X_train_scaled_flat.reshape(X_train.shape)\n",
    "\n",
    "    X_train_sa_shaped = spec_augment(X_train_shaped_scaled)\n",
    "    X_train_noise_shaped = add_noise(X_train_shaped_scaled, std=0.03)\n",
    "\n",
    "    X_train_sa_flat = X_train_sa_shaped.reshape(X_train_sa_shaped.shape[0], -1)\n",
    "    X_train_noise_flat = X_train_noise_shaped.reshape(X_train_noise_shaped.shape[0], -1)\n",
    "\n",
    "    X_train_aug_flat = np.concatenate([X_train_scaled_flat, X_train_sa_flat, X_train_noise_flat], axis=0)\n",
    "    y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "\n",
    "    # shuffle augmented training set\n",
    "    rng = np.random.RandomState(base_seed)\n",
    "    perm = rng.permutation(len(X_train_aug_flat))\n",
    "    X_train_aug_flat = X_train_aug_flat[perm]\n",
    "    y_train_aug = y_train_aug[perm]\n",
    "\n",
    "    # For cascade we will train using the augmented data as the \"train\" (X_train_aug_flat, y_train_aug)\n",
    "    # We'll use a small portion of original validation set as cascade validation (X_val_scaled_flat, y_val_bin)\n",
    "    # 4) Train cascade\n",
    "    base_estimators = [\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1)\n",
    "    ]\n",
    "\n",
    "    layers, Xtr_trans, Xval_trans = train_cascade(\n",
    "        X_train_aug_flat, y_train_aug,\n",
    "        X_val_scaled_flat, y_val_bin,\n",
    "        base_estimators=base_estimators,\n",
    "        max_layers=max_layers,\n",
    "        n_splits=n_splits_oof,\n",
    "        random_state=base_seed,\n",
    "        early_stopping_rounds=2\n",
    "    )\n",
    "\n",
    "    # 5) Final classifier on top of transformed features: use RandomForest\n",
    "    final_clf = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=base_seed)\n",
    "    final_clf.fit(Xtr_trans, y_train_aug)\n",
    "\n",
    "    # 6) Evaluate on TEST: transform test through cascade\n",
    "    X_test_trans = predict_cascade(layers, X_test_scaled_flat)\n",
    "\n",
    "    # predict probs\n",
    "    if hasattr(final_clf, \"predict_proba\"):\n",
    "        y_prob_test = final_clf.predict_proba(X_test_trans)[:, 1]\n",
    "    else:\n",
    "        if hasattr(final_clf, \"decision_function\"):\n",
    "            y_prob_test = 1.0 / (1.0 + np.exp(-final_clf.decision_function(X_test_trans)))\n",
    "        else:\n",
    "            y_prob_test = final_clf.predict(X_test_trans)\n",
    "\n",
    "    # search threshold for best TEST accuracy\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for thr_ in np.linspace(0.1, 0.9, 17):\n",
    "        yp = (y_prob_test >= thr_).astype(int)\n",
    "        acc_thr = accuracy_score(y_test_bin, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc = acc_thr\n",
    "            best_thr = thr_\n",
    "\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_test_bin, y_pred_best)\n",
    "    test_f1  = f1_score(y_test_bin, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_test_bin, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nðŸ”š [{emotion_name}] TEST (best thr={best_thr:.3f}) \"\n",
    "        f\"| Acc={test_acc*100:.2f}% | F1={test_f1:.3f} | AUC={test_auc:.3f}\\n\"\n",
    "    )\n",
    "\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# ----------------- 10-fold CV driver (same as your original) -----------------\n",
    "def run_10fold_cv_emotion(X, y_cont, emotion_name=\"Valence\", base_seed=42, max_layers=3):\n",
    "    print(f\"\\n########## {emotion_name}: 10-fold STRATIFIED CV (Cascade) ##########\")\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    print(f\"{emotion_name} global median (for stratification) = {global_thr:.4f}\")\n",
    "    print(\"Class counts:\", np.bincount(y_bin_global.astype(int)))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=base_seed)\n",
    "\n",
    "    fold_accs, fold_f1s, fold_aucs = [], [], []\n",
    "\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(skf.split(np.arange(len(y_cont)), y_bin_global), start=1):\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "\n",
    "        tv_train_idx, tv_val_idx = train_test_split(\n",
    "            trainval_idx,\n",
    "            test_size=val_frac,\n",
    "            random_state=base_seed + fold,\n",
    "            shuffle=True,\n",
    "            stratify=y_trainval_bin,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n----- {emotion_name} Fold {fold}/10 -----\")\n",
    "        print(f\"Train size={len(tv_train_idx)}, Val size={len(tv_val_idx)}, Test size={len(test_idx)}\")\n",
    "\n",
    "        acc, f1, auc = train_one_fold_emotion(\n",
    "            X, y_cont,\n",
    "            tv_train_idx, tv_val_idx, test_idx,\n",
    "            emotion_name=f\"{emotion_name} (Fold {fold})\",\n",
    "            base_seed=base_seed + fold,\n",
    "            max_layers=max_layers\n",
    "        )\n",
    "\n",
    "        fold_accs.append(acc)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_aucs.append(auc)\n",
    "\n",
    "        print(f\"Fold {fold} Results: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "    fold_accs = np.array(fold_accs)\n",
    "    fold_f1s  = np.array(fold_f1s)\n",
    "    fold_aucs = np.array(fold_aucs)\n",
    "\n",
    "    print(f\"\\n===== FINAL {emotion_name} 10-FOLD RESULTS (Cascade) =====\")\n",
    "    print(f\"Accuracy: {fold_accs.mean():.4f} Â± {fold_accs.std():.4f}\")\n",
    "    print(f\"F1-score: {fold_f1s.mean():.4f} Â± {fold_f1s.std():.4f}\")\n",
    "    print(f\"AUC:      {fold_aucs.mean():.4f} Â± {fold_aucs.std():.4f}\")\n",
    "\n",
    "    return fold_accs, fold_f1s, fold_aucs\n",
    "\n",
    "# ----------------- MAIN (example usage) -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure you already prepared:\n",
    "    # X, y_val, y_aro, groups, channels = load_dreamer_and_build_spectrograms(...)\n",
    "    print(\"\\nâœ… Spectrogram dataset:\", X.shape)\n",
    "\n",
    "    val_accs, val_f1s, val_aucs = run_10fold_cv_emotion(X, y_val, emotion_name=\"Valence\", base_seed=42, max_layers=3)\n",
    "    aro_accs, aro_f1s, aro_aucs = run_10fold_cv_emotion(X, y_aro, emotion_name=\"Arousal\", base_seed=142, max_layers=3)\n",
    "\n",
    "    print(\"\\n===== OVERALL SUMMARY (Cascade) =====\")\n",
    "    print(f\"Valence Acc: mean={val_accs.mean():.4f}, std={val_accs.std():.4f}\")\n",
    "    print(f\"Aroual Acc:  mean={aro_accs.mean():.4f}, std={aro_accs.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7ffbba5-15c9-4fa0-a221-ce8d5cefbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741df29b-595a-4cf2-b293-ceca32955c75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "âœ… Spectrogram dataset: (414, 14, 36, 32)\n",
      "\n",
      "########## Valence: 10-fold STRATIFIED CV ##########\n",
      "Valence global median (for stratification) = 3.0000\n",
      "Class counts: [161 253]\n",
      "\n",
      "----- Valence Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 1) | epochs=70\n",
      "[Valence (Fold 1)] Ep 01/70 | Loss=0.3870 | Val Acc=54.76% | F1=0.642 | AUC=0.526\n",
      "[Valence (Fold 1)] Ep 02/70 | Loss=0.0503 | Val Acc=52.38% | F1=0.630 | AUC=0.555\n",
      "[Valence (Fold 1)] Ep 03/70 | Loss=0.0120 | Val Acc=52.38% | F1=0.630 | AUC=0.514\n",
      "[Valence (Fold 1)] Ep 04/70 | Loss=0.0071 | Val Acc=50.00% | F1=0.604 | AUC=0.512\n",
      "[Valence (Fold 1)] Ep 05/70 | Loss=0.0052 | Val Acc=50.00% | F1=0.604 | AUC=0.524\n",
      "[Valence (Fold 1)] Ep 06/70 | Loss=0.0044 | Val Acc=50.00% | F1=0.604 | AUC=0.519\n",
      "[Valence (Fold 1)] Ep 07/70 | Loss=0.0036 | Val Acc=50.00% | F1=0.604 | AUC=0.519\n",
      "[Valence (Fold 1)] Ep 08/70 | Loss=0.0031 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 09/70 | Loss=0.0028 | Val Acc=50.00% | F1=0.604 | AUC=0.514\n",
      "[Valence (Fold 1)] Ep 10/70 | Loss=0.0024 | Val Acc=50.00% | F1=0.604 | AUC=0.519\n",
      "[Valence (Fold 1)] Ep 11/70 | Loss=0.0022 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 12/70 | Loss=0.0021 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 13/70 | Loss=0.0020 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 14/70 | Loss=0.0019 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 15/70 | Loss=0.0018 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 16/70 | Loss=0.0018 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 17/70 | Loss=0.0017 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 18/70 | Loss=0.0017 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 19/70 | Loss=0.0017 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 20/70 | Loss=0.0016 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 21/70 | Loss=0.0015 | Val Acc=50.00% | F1=0.604 | AUC=0.517\n",
      "[Valence (Fold 1)] Ep 22/70 | Loss=0.0013 | Val Acc=50.00% | F1=0.604 | AUC=0.510\n",
      "[Valence (Fold 1)] Ep 23/70 | Loss=0.0011 | Val Acc=50.00% | F1=0.604 | AUC=0.507\n",
      "[Valence (Fold 1)] Ep 24/70 | Loss=0.0010 | Val Acc=50.00% | F1=0.604 | AUC=0.512\n",
      "[Valence (Fold 1)] Ep 25/70 | Loss=0.0009 | Val Acc=50.00% | F1=0.604 | AUC=0.505\n",
      "[Valence (Fold 1)] Ep 26/70 | Loss=0.0008 | Val Acc=50.00% | F1=0.604 | AUC=0.505\n",
      "[Valence (Fold 1)] Ep 27/70 | Loss=0.0007 | Val Acc=50.00% | F1=0.604 | AUC=0.507\n",
      "[Valence (Fold 1)] Ep 28/70 | Loss=0.0007 | Val Acc=50.00% | F1=0.604 | AUC=0.506\n",
      "[Valence (Fold 1)] Ep 29/70 | Loss=0.0007 | Val Acc=50.00% | F1=0.604 | AUC=0.507\n",
      "[Valence (Fold 1)] Ep 30/70 | Loss=0.0006 | Val Acc=52.38% | F1=0.630 | AUC=0.507\n",
      "[Valence (Fold 1)] Ep 31/70 | Loss=0.0006 | Val Acc=52.38% | F1=0.630 | AUC=0.507\n",
      "â¹ [Valence (Fold 1)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 1)] TEST (best thr=0.100) | Acc=59.52% | F1=0.691 | AUC=0.567\n",
      "\n",
      "Fold 1 Results: Acc=0.5952, F1=0.6909, AUC=0.5673\n",
      "\n",
      "----- Valence Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 2) | epochs=70\n",
      "[Valence (Fold 2)] Ep 01/70 | Loss=0.3753 | Val Acc=54.76% | F1=0.627 | AUC=0.514\n",
      "[Valence (Fold 2)] Ep 02/70 | Loss=0.0413 | Val Acc=52.38% | F1=0.600 | AUC=0.538\n",
      "[Valence (Fold 2)] Ep 03/70 | Loss=0.0101 | Val Acc=52.38% | F1=0.583 | AUC=0.541\n",
      "[Valence (Fold 2)] Ep 04/70 | Loss=0.0067 | Val Acc=52.38% | F1=0.583 | AUC=0.541\n",
      "[Valence (Fold 2)] Ep 05/70 | Loss=0.0049 | Val Acc=52.38% | F1=0.583 | AUC=0.538\n",
      "[Valence (Fold 2)] Ep 06/70 | Loss=0.0040 | Val Acc=52.38% | F1=0.583 | AUC=0.538\n",
      "[Valence (Fold 2)] Ep 07/70 | Loss=0.0034 | Val Acc=52.38% | F1=0.583 | AUC=0.543\n",
      "[Valence (Fold 2)] Ep 08/70 | Loss=0.0029 | Val Acc=52.38% | F1=0.583 | AUC=0.536\n",
      "[Valence (Fold 2)] Ep 09/70 | Loss=0.0025 | Val Acc=52.38% | F1=0.583 | AUC=0.538\n",
      "[Valence (Fold 2)] Ep 10/70 | Loss=0.0023 | Val Acc=52.38% | F1=0.583 | AUC=0.538\n",
      "[Valence (Fold 2)] Ep 11/70 | Loss=0.0021 | Val Acc=52.38% | F1=0.583 | AUC=0.536\n",
      "[Valence (Fold 2)] Ep 12/70 | Loss=0.0020 | Val Acc=52.38% | F1=0.583 | AUC=0.536\n",
      "[Valence (Fold 2)] Ep 13/70 | Loss=0.0018 | Val Acc=52.38% | F1=0.583 | AUC=0.538\n",
      "[Valence (Fold 2)] Ep 14/70 | Loss=0.0018 | Val Acc=52.38% | F1=0.583 | AUC=0.538\n",
      "[Valence (Fold 2)] Ep 15/70 | Loss=0.0017 | Val Acc=52.38% | F1=0.583 | AUC=0.536\n",
      "[Valence (Fold 2)] Ep 16/70 | Loss=0.0017 | Val Acc=52.38% | F1=0.583 | AUC=0.531\n",
      "[Valence (Fold 2)] Ep 17/70 | Loss=0.0016 | Val Acc=52.38% | F1=0.583 | AUC=0.531\n",
      "[Valence (Fold 2)] Ep 18/70 | Loss=0.0016 | Val Acc=52.38% | F1=0.583 | AUC=0.534\n",
      "[Valence (Fold 2)] Ep 19/70 | Loss=0.0016 | Val Acc=52.38% | F1=0.583 | AUC=0.534\n",
      "[Valence (Fold 2)] Ep 20/70 | Loss=0.0016 | Val Acc=52.38% | F1=0.583 | AUC=0.534\n",
      "[Valence (Fold 2)] Ep 21/70 | Loss=0.0015 | Val Acc=52.38% | F1=0.583 | AUC=0.534\n",
      "[Valence (Fold 2)] Ep 22/70 | Loss=0.0013 | Val Acc=52.38% | F1=0.583 | AUC=0.534\n",
      "[Valence (Fold 2)] Ep 23/70 | Loss=0.0011 | Val Acc=52.38% | F1=0.583 | AUC=0.529\n",
      "[Valence (Fold 2)] Ep 24/70 | Loss=0.0010 | Val Acc=52.38% | F1=0.583 | AUC=0.529\n",
      "[Valence (Fold 2)] Ep 25/70 | Loss=0.0008 | Val Acc=52.38% | F1=0.583 | AUC=0.531\n",
      "[Valence (Fold 2)] Ep 26/70 | Loss=0.0008 | Val Acc=52.38% | F1=0.583 | AUC=0.531\n",
      "[Valence (Fold 2)] Ep 27/70 | Loss=0.0007 | Val Acc=52.38% | F1=0.583 | AUC=0.529\n",
      "[Valence (Fold 2)] Ep 28/70 | Loss=0.0007 | Val Acc=52.38% | F1=0.583 | AUC=0.531\n",
      "[Valence (Fold 2)] Ep 29/70 | Loss=0.0006 | Val Acc=52.38% | F1=0.583 | AUC=0.534\n",
      "[Valence (Fold 2)] Ep 30/70 | Loss=0.0006 | Val Acc=52.38% | F1=0.583 | AUC=0.531\n",
      "[Valence (Fold 2)] Ep 31/70 | Loss=0.0006 | Val Acc=52.38% | F1=0.583 | AUC=0.531\n",
      "â¹ [Valence (Fold 2)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 2)] TEST (best thr=0.750) | Acc=52.38% | F1=0.630 | AUC=0.550\n",
      "\n",
      "Fold 2 Results: Acc=0.5238, F1=0.6296, AUC=0.5505\n",
      "\n",
      "----- Valence Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 3) | epochs=70\n",
      "[Valence (Fold 3)] Ep 01/70 | Loss=0.3635 | Val Acc=52.38% | F1=0.630 | AUC=0.486\n",
      "[Valence (Fold 3)] Ep 02/70 | Loss=0.0369 | Val Acc=47.62% | F1=0.542 | AUC=0.440\n",
      "[Valence (Fold 3)] Ep 03/70 | Loss=0.0106 | Val Acc=45.24% | F1=0.566 | AUC=0.442\n",
      "[Valence (Fold 3)] Ep 04/70 | Loss=0.0069 | Val Acc=42.86% | F1=0.538 | AUC=0.397\n",
      "[Valence (Fold 3)] Ep 05/70 | Loss=0.0045 | Val Acc=42.86% | F1=0.538 | AUC=0.406\n",
      "[Valence (Fold 3)] Ep 06/70 | Loss=0.0035 | Val Acc=42.86% | F1=0.520 | AUC=0.406\n",
      "[Valence (Fold 3)] Ep 07/70 | Loss=0.0030 | Val Acc=45.24% | F1=0.549 | AUC=0.411\n",
      "[Valence (Fold 3)] Ep 08/70 | Loss=0.0026 | Val Acc=42.86% | F1=0.520 | AUC=0.413\n",
      "[Valence (Fold 3)] Ep 09/70 | Loss=0.0023 | Val Acc=42.86% | F1=0.520 | AUC=0.413\n",
      "[Valence (Fold 3)] Ep 10/70 | Loss=0.0021 | Val Acc=42.86% | F1=0.520 | AUC=0.416\n",
      "[Valence (Fold 3)] Ep 11/70 | Loss=0.0019 | Val Acc=42.86% | F1=0.520 | AUC=0.416\n",
      "[Valence (Fold 3)] Ep 12/70 | Loss=0.0018 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 13/70 | Loss=0.0016 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 14/70 | Loss=0.0016 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 15/70 | Loss=0.0015 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 16/70 | Loss=0.0015 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 17/70 | Loss=0.0015 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 18/70 | Loss=0.0015 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 19/70 | Loss=0.0014 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 20/70 | Loss=0.0014 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 21/70 | Loss=0.0013 | Val Acc=42.86% | F1=0.520 | AUC=0.416\n",
      "[Valence (Fold 3)] Ep 22/70 | Loss=0.0011 | Val Acc=42.86% | F1=0.520 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 23/70 | Loss=0.0010 | Val Acc=42.86% | F1=0.520 | AUC=0.421\n",
      "[Valence (Fold 3)] Ep 24/70 | Loss=0.0008 | Val Acc=45.24% | F1=0.549 | AUC=0.421\n",
      "[Valence (Fold 3)] Ep 25/70 | Loss=0.0008 | Val Acc=45.24% | F1=0.549 | AUC=0.421\n",
      "[Valence (Fold 3)] Ep 26/70 | Loss=0.0007 | Val Acc=45.24% | F1=0.549 | AUC=0.421\n",
      "[Valence (Fold 3)] Ep 27/70 | Loss=0.0006 | Val Acc=45.24% | F1=0.549 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 28/70 | Loss=0.0006 | Val Acc=45.24% | F1=0.549 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 29/70 | Loss=0.0006 | Val Acc=45.24% | F1=0.549 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 30/70 | Loss=0.0005 | Val Acc=45.24% | F1=0.549 | AUC=0.418\n",
      "[Valence (Fold 3)] Ep 31/70 | Loss=0.0005 | Val Acc=45.24% | F1=0.549 | AUC=0.418\n",
      "â¹ [Valence (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 3)] TEST (best thr=0.100) | Acc=50.00% | F1=0.533 | AUC=0.534\n",
      "\n",
      "Fold 3 Results: Acc=0.5000, F1=0.5333, AUC=0.5337\n",
      "\n",
      "----- Valence Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [128 202]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [17 25]\n",
      "[Valence (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.634\n",
      "\n",
      "ðŸš€ Training Valence (Fold 4) | epochs=70\n",
      "[Valence (Fold 4)] Ep 01/70 | Loss=0.3675 | Val Acc=59.52% | F1=0.679 | AUC=0.671\n",
      "[Valence (Fold 4)] Ep 02/70 | Loss=0.0404 | Val Acc=73.81% | F1=0.784 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 03/70 | Loss=0.0099 | Val Acc=71.43% | F1=0.778 | AUC=0.700\n",
      "[Valence (Fold 4)] Ep 04/70 | Loss=0.0060 | Val Acc=69.05% | F1=0.755 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 05/70 | Loss=0.0046 | Val Acc=71.43% | F1=0.769 | AUC=0.695\n",
      "[Valence (Fold 4)] Ep 06/70 | Loss=0.0036 | Val Acc=69.05% | F1=0.745 | AUC=0.695\n",
      "[Valence (Fold 4)] Ep 07/70 | Loss=0.0030 | Val Acc=71.43% | F1=0.769 | AUC=0.700\n",
      "[Valence (Fold 4)] Ep 08/70 | Loss=0.0028 | Val Acc=71.43% | F1=0.769 | AUC=0.700\n",
      "[Valence (Fold 4)] Ep 09/70 | Loss=0.0024 | Val Acc=71.43% | F1=0.769 | AUC=0.700\n",
      "[Valence (Fold 4)] Ep 10/70 | Loss=0.0022 | Val Acc=69.05% | F1=0.755 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 11/70 | Loss=0.0020 | Val Acc=71.43% | F1=0.769 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 12/70 | Loss=0.0018 | Val Acc=71.43% | F1=0.769 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 13/70 | Loss=0.0017 | Val Acc=71.43% | F1=0.769 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 14/70 | Loss=0.0016 | Val Acc=71.43% | F1=0.769 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 15/70 | Loss=0.0015 | Val Acc=69.05% | F1=0.755 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 16/70 | Loss=0.0016 | Val Acc=69.05% | F1=0.755 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 17/70 | Loss=0.0014 | Val Acc=69.05% | F1=0.755 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 18/70 | Loss=0.0015 | Val Acc=69.05% | F1=0.755 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 19/70 | Loss=0.0014 | Val Acc=71.43% | F1=0.769 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 20/70 | Loss=0.0015 | Val Acc=71.43% | F1=0.769 | AUC=0.697\n",
      "[Valence (Fold 4)] Ep 21/70 | Loss=0.0013 | Val Acc=71.43% | F1=0.769 | AUC=0.695\n",
      "[Valence (Fold 4)] Ep 22/70 | Loss=0.0011 | Val Acc=71.43% | F1=0.769 | AUC=0.692\n",
      "[Valence (Fold 4)] Ep 23/70 | Loss=0.0010 | Val Acc=71.43% | F1=0.769 | AUC=0.692\n",
      "[Valence (Fold 4)] Ep 24/70 | Loss=0.0008 | Val Acc=71.43% | F1=0.769 | AUC=0.692\n",
      "[Valence (Fold 4)] Ep 25/70 | Loss=0.0008 | Val Acc=71.43% | F1=0.769 | AUC=0.694\n",
      "[Valence (Fold 4)] Ep 26/70 | Loss=0.0007 | Val Acc=71.43% | F1=0.769 | AUC=0.695\n",
      "[Valence (Fold 4)] Ep 27/70 | Loss=0.0006 | Val Acc=71.43% | F1=0.769 | AUC=0.695\n",
      "[Valence (Fold 4)] Ep 28/70 | Loss=0.0006 | Val Acc=71.43% | F1=0.769 | AUC=0.695\n",
      "[Valence (Fold 4)] Ep 29/70 | Loss=0.0005 | Val Acc=71.43% | F1=0.769 | AUC=0.695\n",
      "[Valence (Fold 4)] Ep 30/70 | Loss=0.0005 | Val Acc=71.43% | F1=0.769 | AUC=0.695\n",
      "[Valence (Fold 4)] Ep 31/70 | Loss=0.0005 | Val Acc=71.43% | F1=0.769 | AUC=0.695\n",
      "â¹ [Valence (Fold 4)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 4)] TEST (best thr=0.100) | Acc=50.00% | F1=0.588 | AUC=0.414\n",
      "\n",
      "Fold 4 Results: Acc=0.5000, F1=0.5882, AUC=0.4141\n",
      "\n",
      "----- Valence Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 5) | epochs=70\n",
      "[Valence (Fold 5)] Ep 01/70 | Loss=0.3698 | Val Acc=48.78% | F1=0.512 | AUC=0.548\n",
      "[Valence (Fold 5)] Ep 02/70 | Loss=0.0379 | Val Acc=48.78% | F1=0.604 | AUC=0.520\n",
      "[Valence (Fold 5)] Ep 03/70 | Loss=0.0103 | Val Acc=41.46% | F1=0.520 | AUC=0.528\n",
      "[Valence (Fold 5)] Ep 04/70 | Loss=0.0067 | Val Acc=43.90% | F1=0.531 | AUC=0.545\n",
      "[Valence (Fold 5)] Ep 05/70 | Loss=0.0051 | Val Acc=43.90% | F1=0.531 | AUC=0.550\n",
      "[Valence (Fold 5)] Ep 06/70 | Loss=0.0042 | Val Acc=43.90% | F1=0.531 | AUC=0.555\n",
      "[Valence (Fold 5)] Ep 07/70 | Loss=0.0035 | Val Acc=43.90% | F1=0.531 | AUC=0.555\n",
      "[Valence (Fold 5)] Ep 08/70 | Loss=0.0031 | Val Acc=43.90% | F1=0.531 | AUC=0.557\n",
      "[Valence (Fold 5)] Ep 09/70 | Loss=0.0027 | Val Acc=43.90% | F1=0.531 | AUC=0.565\n",
      "[Valence (Fold 5)] Ep 10/70 | Loss=0.0024 | Val Acc=43.90% | F1=0.531 | AUC=0.565\n",
      "[Valence (Fold 5)] Ep 11/70 | Loss=0.0022 | Val Acc=43.90% | F1=0.531 | AUC=0.562\n",
      "[Valence (Fold 5)] Ep 12/70 | Loss=0.0020 | Val Acc=43.90% | F1=0.531 | AUC=0.562\n",
      "[Valence (Fold 5)] Ep 13/70 | Loss=0.0019 | Val Acc=43.90% | F1=0.531 | AUC=0.562\n",
      "[Valence (Fold 5)] Ep 14/70 | Loss=0.0018 | Val Acc=43.90% | F1=0.531 | AUC=0.562\n",
      "[Valence (Fold 5)] Ep 15/70 | Loss=0.0017 | Val Acc=43.90% | F1=0.531 | AUC=0.562\n",
      "[Valence (Fold 5)] Ep 16/70 | Loss=0.0017 | Val Acc=43.90% | F1=0.531 | AUC=0.562\n",
      "[Valence (Fold 5)] Ep 17/70 | Loss=0.0017 | Val Acc=43.90% | F1=0.531 | AUC=0.565\n",
      "[Valence (Fold 5)] Ep 18/70 | Loss=0.0016 | Val Acc=43.90% | F1=0.531 | AUC=0.565\n",
      "[Valence (Fold 5)] Ep 19/70 | Loss=0.0016 | Val Acc=43.90% | F1=0.531 | AUC=0.565\n",
      "[Valence (Fold 5)] Ep 20/70 | Loss=0.0016 | Val Acc=43.90% | F1=0.531 | AUC=0.565\n",
      "[Valence (Fold 5)] Ep 21/70 | Loss=0.0015 | Val Acc=43.90% | F1=0.531 | AUC=0.568\n",
      "[Valence (Fold 5)] Ep 22/70 | Loss=0.0012 | Val Acc=43.90% | F1=0.531 | AUC=0.562\n",
      "[Valence (Fold 5)] Ep 23/70 | Loss=0.0010 | Val Acc=43.90% | F1=0.531 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 24/70 | Loss=0.0010 | Val Acc=43.90% | F1=0.531 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 25/70 | Loss=0.0009 | Val Acc=43.90% | F1=0.531 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 26/70 | Loss=0.0008 | Val Acc=43.90% | F1=0.531 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 27/70 | Loss=0.0007 | Val Acc=43.90% | F1=0.531 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 28/70 | Loss=0.0006 | Val Acc=43.90% | F1=0.531 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 29/70 | Loss=0.0006 | Val Acc=43.90% | F1=0.531 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 30/70 | Loss=0.0006 | Val Acc=43.90% | F1=0.531 | AUC=0.560\n",
      "[Valence (Fold 5)] Ep 31/70 | Loss=0.0006 | Val Acc=43.90% | F1=0.531 | AUC=0.562\n",
      "â¹ [Valence (Fold 5)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 5)] TEST (best thr=0.250) | Acc=60.98% | F1=0.680 | AUC=0.578\n",
      "\n",
      "Fold 5 Results: Acc=0.6098, F1=0.6800, AUC=0.5775\n",
      "\n",
      "----- Valence Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 6) | epochs=70\n",
      "[Valence (Fold 6)] Ep 01/70 | Loss=0.3700 | Val Acc=43.90% | F1=0.465 | AUC=0.458\n",
      "[Valence (Fold 6)] Ep 02/70 | Loss=0.0389 | Val Acc=48.78% | F1=0.604 | AUC=0.438\n",
      "[Valence (Fold 6)] Ep 03/70 | Loss=0.0100 | Val Acc=48.78% | F1=0.571 | AUC=0.460\n",
      "[Valence (Fold 6)] Ep 04/70 | Loss=0.0062 | Val Acc=53.66% | F1=0.627 | AUC=0.472\n",
      "[Valence (Fold 6)] Ep 05/70 | Loss=0.0047 | Val Acc=53.66% | F1=0.627 | AUC=0.479\n",
      "[Valence (Fold 6)] Ep 06/70 | Loss=0.0039 | Val Acc=53.66% | F1=0.627 | AUC=0.475\n",
      "[Valence (Fold 6)] Ep 07/70 | Loss=0.0032 | Val Acc=53.66% | F1=0.627 | AUC=0.475\n",
      "[Valence (Fold 6)] Ep 08/70 | Loss=0.0029 | Val Acc=53.66% | F1=0.627 | AUC=0.478\n",
      "[Valence (Fold 6)] Ep 09/70 | Loss=0.0024 | Val Acc=53.66% | F1=0.627 | AUC=0.478\n",
      "[Valence (Fold 6)] Ep 10/70 | Loss=0.0022 | Val Acc=53.66% | F1=0.627 | AUC=0.480\n",
      "[Valence (Fold 6)] Ep 11/70 | Loss=0.0020 | Val Acc=53.66% | F1=0.627 | AUC=0.480\n",
      "[Valence (Fold 6)] Ep 12/70 | Loss=0.0019 | Val Acc=53.66% | F1=0.627 | AUC=0.475\n",
      "[Valence (Fold 6)] Ep 13/70 | Loss=0.0017 | Val Acc=53.66% | F1=0.627 | AUC=0.477\n",
      "[Valence (Fold 6)] Ep 14/70 | Loss=0.0017 | Val Acc=53.66% | F1=0.627 | AUC=0.475\n",
      "[Valence (Fold 6)] Ep 15/70 | Loss=0.0016 | Val Acc=53.66% | F1=0.627 | AUC=0.475\n",
      "[Valence (Fold 6)] Ep 16/70 | Loss=0.0016 | Val Acc=53.66% | F1=0.627 | AUC=0.477\n",
      "[Valence (Fold 6)] Ep 17/70 | Loss=0.0016 | Val Acc=53.66% | F1=0.627 | AUC=0.477\n",
      "[Valence (Fold 6)] Ep 18/70 | Loss=0.0015 | Val Acc=53.66% | F1=0.627 | AUC=0.477\n",
      "[Valence (Fold 6)] Ep 19/70 | Loss=0.0015 | Val Acc=53.66% | F1=0.627 | AUC=0.477\n",
      "[Valence (Fold 6)] Ep 20/70 | Loss=0.0015 | Val Acc=53.66% | F1=0.627 | AUC=0.477\n",
      "[Valence (Fold 6)] Ep 21/70 | Loss=0.0014 | Val Acc=53.66% | F1=0.627 | AUC=0.475\n",
      "[Valence (Fold 6)] Ep 22/70 | Loss=0.0012 | Val Acc=53.66% | F1=0.627 | AUC=0.477\n",
      "[Valence (Fold 6)] Ep 23/70 | Loss=0.0010 | Val Acc=53.66% | F1=0.627 | AUC=0.482\n",
      "[Valence (Fold 6)] Ep 24/70 | Loss=0.0009 | Val Acc=53.66% | F1=0.627 | AUC=0.483\n",
      "[Valence (Fold 6)] Ep 25/70 | Loss=0.0008 | Val Acc=53.66% | F1=0.627 | AUC=0.487\n",
      "[Valence (Fold 6)] Ep 26/70 | Loss=0.0007 | Val Acc=53.66% | F1=0.627 | AUC=0.487\n",
      "[Valence (Fold 6)] Ep 27/70 | Loss=0.0007 | Val Acc=51.22% | F1=0.615 | AUC=0.487\n",
      "[Valence (Fold 6)] Ep 28/70 | Loss=0.0006 | Val Acc=51.22% | F1=0.615 | AUC=0.485\n",
      "[Valence (Fold 6)] Ep 29/70 | Loss=0.0006 | Val Acc=51.22% | F1=0.615 | AUC=0.485\n",
      "[Valence (Fold 6)] Ep 30/70 | Loss=0.0005 | Val Acc=51.22% | F1=0.615 | AUC=0.485\n",
      "[Valence (Fold 6)] Ep 31/70 | Loss=0.0005 | Val Acc=51.22% | F1=0.615 | AUC=0.487\n",
      "â¹ [Valence (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 6)] TEST (best thr=0.100) | Acc=75.61% | F1=0.800 | AUC=0.753\n",
      "\n",
      "Fold 6 Results: Acc=0.7561, F1=0.8000, AUC=0.7525\n",
      "\n",
      "----- Valence Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 7) | epochs=70\n",
      "[Valence (Fold 7)] Ep 01/70 | Loss=0.3749 | Val Acc=51.22% | F1=0.565 | AUC=0.468\n",
      "[Valence (Fold 7)] Ep 02/70 | Loss=0.0483 | Val Acc=58.54% | F1=0.667 | AUC=0.565\n",
      "[Valence (Fold 7)] Ep 03/70 | Loss=0.0111 | Val Acc=53.66% | F1=0.596 | AUC=0.512\n",
      "[Valence (Fold 7)] Ep 04/70 | Loss=0.0067 | Val Acc=53.66% | F1=0.596 | AUC=0.530\n",
      "[Valence (Fold 7)] Ep 05/70 | Loss=0.0050 | Val Acc=56.10% | F1=0.625 | AUC=0.522\n",
      "[Valence (Fold 7)] Ep 06/70 | Loss=0.0041 | Val Acc=56.10% | F1=0.625 | AUC=0.520\n",
      "[Valence (Fold 7)] Ep 07/70 | Loss=0.0033 | Val Acc=56.10% | F1=0.625 | AUC=0.520\n",
      "[Valence (Fold 7)] Ep 08/70 | Loss=0.0029 | Val Acc=56.10% | F1=0.625 | AUC=0.523\n",
      "[Valence (Fold 7)] Ep 09/70 | Loss=0.0027 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 10/70 | Loss=0.0022 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 11/70 | Loss=0.0021 | Val Acc=56.10% | F1=0.625 | AUC=0.522\n",
      "[Valence (Fold 7)] Ep 12/70 | Loss=0.0020 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 13/70 | Loss=0.0018 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 14/70 | Loss=0.0018 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 15/70 | Loss=0.0017 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 16/70 | Loss=0.0017 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 17/70 | Loss=0.0016 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 18/70 | Loss=0.0016 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 19/70 | Loss=0.0016 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 20/70 | Loss=0.0017 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 21/70 | Loss=0.0015 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 22/70 | Loss=0.0012 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 23/70 | Loss=0.0011 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 24/70 | Loss=0.0009 | Val Acc=56.10% | F1=0.625 | AUC=0.525\n",
      "[Valence (Fold 7)] Ep 25/70 | Loss=0.0009 | Val Acc=56.10% | F1=0.625 | AUC=0.522\n",
      "[Valence (Fold 7)] Ep 26/70 | Loss=0.0008 | Val Acc=56.10% | F1=0.625 | AUC=0.520\n",
      "[Valence (Fold 7)] Ep 27/70 | Loss=0.0007 | Val Acc=56.10% | F1=0.625 | AUC=0.520\n",
      "[Valence (Fold 7)] Ep 28/70 | Loss=0.0006 | Val Acc=56.10% | F1=0.625 | AUC=0.520\n",
      "[Valence (Fold 7)] Ep 29/70 | Loss=0.0006 | Val Acc=56.10% | F1=0.625 | AUC=0.518\n",
      "[Valence (Fold 7)] Ep 30/70 | Loss=0.0006 | Val Acc=56.10% | F1=0.625 | AUC=0.518\n",
      "[Valence (Fold 7)] Ep 31/70 | Loss=0.0005 | Val Acc=56.10% | F1=0.625 | AUC=0.518\n",
      "â¹ [Valence (Fold 7)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 7)] TEST (best thr=0.200) | Acc=46.34% | F1=0.560 | AUC=0.343\n",
      "\n",
      "Fold 7 Results: Acc=0.4634, F1=0.5600, AUC=0.3425\n",
      "\n",
      "----- Valence Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 8) | epochs=70\n",
      "[Valence (Fold 8)] Ep 01/70 | Loss=0.3792 | Val Acc=53.66% | F1=0.578 | AUC=0.590\n",
      "[Valence (Fold 8)] Ep 02/70 | Loss=0.0498 | Val Acc=58.54% | F1=0.653 | AUC=0.562\n",
      "[Valence (Fold 8)] Ep 03/70 | Loss=0.0118 | Val Acc=58.54% | F1=0.679 | AUC=0.573\n",
      "[Valence (Fold 8)] Ep 04/70 | Loss=0.0070 | Val Acc=58.54% | F1=0.679 | AUC=0.585\n",
      "[Valence (Fold 8)] Ep 05/70 | Loss=0.0051 | Val Acc=58.54% | F1=0.679 | AUC=0.575\n",
      "[Valence (Fold 8)] Ep 06/70 | Loss=0.0042 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 07/70 | Loss=0.0035 | Val Acc=58.54% | F1=0.679 | AUC=0.587\n",
      "[Valence (Fold 8)] Ep 08/70 | Loss=0.0030 | Val Acc=58.54% | F1=0.679 | AUC=0.585\n",
      "[Valence (Fold 8)] Ep 09/70 | Loss=0.0026 | Val Acc=58.54% | F1=0.679 | AUC=0.587\n",
      "[Valence (Fold 8)] Ep 10/70 | Loss=0.0025 | Val Acc=58.54% | F1=0.679 | AUC=0.585\n",
      "[Valence (Fold 8)] Ep 11/70 | Loss=0.0021 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 12/70 | Loss=0.0020 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 13/70 | Loss=0.0019 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 14/70 | Loss=0.0019 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 15/70 | Loss=0.0017 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 16/70 | Loss=0.0017 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 17/70 | Loss=0.0016 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 18/70 | Loss=0.0016 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 19/70 | Loss=0.0016 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 20/70 | Loss=0.0016 | Val Acc=58.54% | F1=0.679 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 21/70 | Loss=0.0015 | Val Acc=58.54% | F1=0.679 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 22/70 | Loss=0.0013 | Val Acc=58.54% | F1=0.679 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 23/70 | Loss=0.0011 | Val Acc=58.54% | F1=0.679 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 24/70 | Loss=0.0010 | Val Acc=58.54% | F1=0.679 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 25/70 | Loss=0.0009 | Val Acc=58.54% | F1=0.679 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 26/70 | Loss=0.0008 | Val Acc=58.54% | F1=0.679 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 27/70 | Loss=0.0007 | Val Acc=58.54% | F1=0.679 | AUC=0.578\n",
      "[Valence (Fold 8)] Ep 28/70 | Loss=0.0007 | Val Acc=58.54% | F1=0.679 | AUC=0.578\n",
      "[Valence (Fold 8)] Ep 29/70 | Loss=0.0006 | Val Acc=58.54% | F1=0.679 | AUC=0.578\n",
      "[Valence (Fold 8)] Ep 30/70 | Loss=0.0006 | Val Acc=58.54% | F1=0.679 | AUC=0.580\n",
      "[Valence (Fold 8)] Ep 31/70 | Loss=0.0006 | Val Acc=58.54% | F1=0.679 | AUC=0.578\n",
      "â¹ [Valence (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 8)] TEST (best thr=0.100) | Acc=53.66% | F1=0.612 | AUC=0.518\n",
      "\n",
      "Fold 8 Results: Acc=0.5366, F1=0.6122, AUC=0.5175\n",
      "\n",
      "----- Valence Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 9) | epochs=70\n",
      "[Valence (Fold 9)] Ep 01/70 | Loss=0.3660 | Val Acc=58.54% | F1=0.653 | AUC=0.650\n",
      "[Valence (Fold 9)] Ep 02/70 | Loss=0.0470 | Val Acc=58.54% | F1=0.653 | AUC=0.618\n",
      "[Valence (Fold 9)] Ep 03/70 | Loss=0.0137 | Val Acc=60.98% | F1=0.680 | AUC=0.617\n",
      "[Valence (Fold 9)] Ep 04/70 | Loss=0.0073 | Val Acc=58.54% | F1=0.667 | AUC=0.605\n",
      "[Valence (Fold 9)] Ep 05/70 | Loss=0.0052 | Val Acc=60.98% | F1=0.692 | AUC=0.595\n",
      "[Valence (Fold 9)] Ep 06/70 | Loss=0.0041 | Val Acc=58.54% | F1=0.667 | AUC=0.598\n",
      "[Valence (Fold 9)] Ep 07/70 | Loss=0.0034 | Val Acc=58.54% | F1=0.667 | AUC=0.608\n",
      "[Valence (Fold 9)] Ep 08/70 | Loss=0.0029 | Val Acc=58.54% | F1=0.667 | AUC=0.605\n",
      "[Valence (Fold 9)] Ep 09/70 | Loss=0.0025 | Val Acc=58.54% | F1=0.667 | AUC=0.605\n",
      "[Valence (Fold 9)] Ep 10/70 | Loss=0.0024 | Val Acc=58.54% | F1=0.667 | AUC=0.603\n",
      "[Valence (Fold 9)] Ep 11/70 | Loss=0.0021 | Val Acc=58.54% | F1=0.667 | AUC=0.605\n",
      "[Valence (Fold 9)] Ep 12/70 | Loss=0.0020 | Val Acc=58.54% | F1=0.667 | AUC=0.603\n",
      "[Valence (Fold 9)] Ep 13/70 | Loss=0.0019 | Val Acc=58.54% | F1=0.667 | AUC=0.607\n",
      "[Valence (Fold 9)] Ep 14/70 | Loss=0.0018 | Val Acc=58.54% | F1=0.667 | AUC=0.610\n",
      "[Valence (Fold 9)] Ep 15/70 | Loss=0.0018 | Val Acc=58.54% | F1=0.667 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 16/70 | Loss=0.0017 | Val Acc=58.54% | F1=0.667 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 17/70 | Loss=0.0016 | Val Acc=58.54% | F1=0.667 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 18/70 | Loss=0.0016 | Val Acc=58.54% | F1=0.667 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 19/70 | Loss=0.0017 | Val Acc=58.54% | F1=0.667 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 20/70 | Loss=0.0016 | Val Acc=58.54% | F1=0.667 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 21/70 | Loss=0.0015 | Val Acc=58.54% | F1=0.667 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 22/70 | Loss=0.0013 | Val Acc=58.54% | F1=0.667 | AUC=0.615\n",
      "[Valence (Fold 9)] Ep 23/70 | Loss=0.0011 | Val Acc=58.54% | F1=0.667 | AUC=0.618\n",
      "[Valence (Fold 9)] Ep 24/70 | Loss=0.0010 | Val Acc=58.54% | F1=0.667 | AUC=0.618\n",
      "[Valence (Fold 9)] Ep 25/70 | Loss=0.0009 | Val Acc=58.54% | F1=0.667 | AUC=0.618\n",
      "[Valence (Fold 9)] Ep 26/70 | Loss=0.0008 | Val Acc=58.54% | F1=0.667 | AUC=0.618\n",
      "[Valence (Fold 9)] Ep 27/70 | Loss=0.0008 | Val Acc=58.54% | F1=0.667 | AUC=0.618\n",
      "[Valence (Fold 9)] Ep 28/70 | Loss=0.0007 | Val Acc=58.54% | F1=0.667 | AUC=0.620\n",
      "[Valence (Fold 9)] Ep 29/70 | Loss=0.0006 | Val Acc=58.54% | F1=0.667 | AUC=0.620\n",
      "[Valence (Fold 9)] Ep 30/70 | Loss=0.0006 | Val Acc=58.54% | F1=0.667 | AUC=0.620\n",
      "[Valence (Fold 9)] Ep 31/70 | Loss=0.0006 | Val Acc=58.54% | F1=0.667 | AUC=0.620\n",
      "â¹ [Valence (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 9)] TEST (best thr=0.100) | Acc=53.66% | F1=0.678 | AUC=0.442\n",
      "\n",
      "Fold 9 Results: Acc=0.5366, F1=0.6780, AUC=0.4425\n",
      "\n",
      "----- Valence Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 10) | epochs=70\n",
      "[Valence (Fold 10)] Ep 01/70 | Loss=0.3605 | Val Acc=43.90% | F1=0.531 | AUC=0.460\n",
      "[Valence (Fold 10)] Ep 02/70 | Loss=0.0446 | Val Acc=48.78% | F1=0.571 | AUC=0.493\n",
      "[Valence (Fold 10)] Ep 03/70 | Loss=0.0127 | Val Acc=53.66% | F1=0.642 | AUC=0.487\n",
      "[Valence (Fold 10)] Ep 04/70 | Loss=0.0081 | Val Acc=48.78% | F1=0.571 | AUC=0.495\n",
      "[Valence (Fold 10)] Ep 05/70 | Loss=0.0050 | Val Acc=48.78% | F1=0.571 | AUC=0.485\n",
      "[Valence (Fold 10)] Ep 06/70 | Loss=0.0041 | Val Acc=48.78% | F1=0.571 | AUC=0.487\n",
      "[Valence (Fold 10)] Ep 07/70 | Loss=0.0034 | Val Acc=46.34% | F1=0.560 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 08/70 | Loss=0.0029 | Val Acc=46.34% | F1=0.560 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 09/70 | Loss=0.0026 | Val Acc=46.34% | F1=0.560 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 10/70 | Loss=0.0024 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 11/70 | Loss=0.0023 | Val Acc=46.34% | F1=0.560 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 12/70 | Loss=0.0020 | Val Acc=46.34% | F1=0.560 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 13/70 | Loss=0.0019 | Val Acc=46.34% | F1=0.560 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 14/70 | Loss=0.0018 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 15/70 | Loss=0.0018 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 16/70 | Loss=0.0017 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 17/70 | Loss=0.0017 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 18/70 | Loss=0.0017 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 19/70 | Loss=0.0017 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 20/70 | Loss=0.0016 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 21/70 | Loss=0.0015 | Val Acc=46.34% | F1=0.560 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 22/70 | Loss=0.0013 | Val Acc=46.34% | F1=0.560 | AUC=0.493\n",
      "[Valence (Fold 10)] Ep 23/70 | Loss=0.0011 | Val Acc=46.34% | F1=0.560 | AUC=0.493\n",
      "[Valence (Fold 10)] Ep 24/70 | Loss=0.0010 | Val Acc=46.34% | F1=0.560 | AUC=0.493\n",
      "[Valence (Fold 10)] Ep 25/70 | Loss=0.0009 | Val Acc=46.34% | F1=0.560 | AUC=0.495\n",
      "[Valence (Fold 10)] Ep 26/70 | Loss=0.0008 | Val Acc=46.34% | F1=0.560 | AUC=0.495\n",
      "[Valence (Fold 10)] Ep 27/70 | Loss=0.0008 | Val Acc=46.34% | F1=0.560 | AUC=0.495\n",
      "[Valence (Fold 10)] Ep 28/70 | Loss=0.0007 | Val Acc=46.34% | F1=0.560 | AUC=0.495\n",
      "[Valence (Fold 10)] Ep 29/70 | Loss=0.0007 | Val Acc=46.34% | F1=0.560 | AUC=0.498\n",
      "[Valence (Fold 10)] Ep 30/70 | Loss=0.0006 | Val Acc=46.34% | F1=0.560 | AUC=0.498\n",
      "[Valence (Fold 10)] Ep 31/70 | Loss=0.0006 | Val Acc=46.34% | F1=0.560 | AUC=0.498\n",
      "â¹ [Valence (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 10)] TEST (best thr=0.100) | Acc=70.73% | F1=0.760 | AUC=0.700\n",
      "\n",
      "Fold 10 Results: Acc=0.7073, F1=0.7600, AUC=0.7000\n",
      "\n",
      "===== FINAL Valence 10-FOLD RESULTS =====\n",
      "Accuracy: 0.5729 Â± 0.0901\n",
      "F1-score: 0.6532 Â± 0.0808\n",
      "AUC:      0.5398 Â± 0.1174\n",
      "\n",
      "########## Arousal: 10-fold STRATIFIED CV ##########\n",
      "Arousal global median (for stratification) = 3.0000\n",
      "Class counts: [114 300]\n",
      "\n",
      "----- Arousal Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 1) | epochs=70\n",
      "[Arousal (Fold 1)] Ep 01/70 | Loss=0.2540 | Val Acc=64.29% | F1=0.754 | AUC=0.503\n",
      "[Arousal (Fold 1)] Ep 02/70 | Loss=0.0357 | Val Acc=64.29% | F1=0.746 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 03/70 | Loss=0.0108 | Val Acc=71.43% | F1=0.812 | AUC=0.517\n",
      "[Arousal (Fold 1)] Ep 04/70 | Loss=0.0062 | Val Acc=69.05% | F1=0.794 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 05/70 | Loss=0.0045 | Val Acc=66.67% | F1=0.781 | AUC=0.514\n",
      "[Arousal (Fold 1)] Ep 06/70 | Loss=0.0036 | Val Acc=66.67% | F1=0.781 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 07/70 | Loss=0.0031 | Val Acc=66.67% | F1=0.781 | AUC=0.517\n",
      "[Arousal (Fold 1)] Ep 08/70 | Loss=0.0026 | Val Acc=66.67% | F1=0.781 | AUC=0.514\n",
      "[Arousal (Fold 1)] Ep 09/70 | Loss=0.0024 | Val Acc=66.67% | F1=0.781 | AUC=0.514\n",
      "[Arousal (Fold 1)] Ep 10/70 | Loss=0.0020 | Val Acc=66.67% | F1=0.781 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 11/70 | Loss=0.0019 | Val Acc=66.67% | F1=0.781 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 12/70 | Loss=0.0018 | Val Acc=66.67% | F1=0.781 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 13/70 | Loss=0.0017 | Val Acc=66.67% | F1=0.781 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 14/70 | Loss=0.0016 | Val Acc=66.67% | F1=0.781 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 15/70 | Loss=0.0016 | Val Acc=66.67% | F1=0.781 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 16/70 | Loss=0.0015 | Val Acc=66.67% | F1=0.781 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 17/70 | Loss=0.0015 | Val Acc=66.67% | F1=0.781 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 18/70 | Loss=0.0015 | Val Acc=66.67% | F1=0.781 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 19/70 | Loss=0.0015 | Val Acc=66.67% | F1=0.781 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 20/70 | Loss=0.0014 | Val Acc=66.67% | F1=0.781 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 21/70 | Loss=0.0013 | Val Acc=66.67% | F1=0.781 | AUC=0.507\n",
      "[Arousal (Fold 1)] Ep 22/70 | Loss=0.0011 | Val Acc=66.67% | F1=0.781 | AUC=0.508\n",
      "[Arousal (Fold 1)] Ep 23/70 | Loss=0.0010 | Val Acc=66.67% | F1=0.781 | AUC=0.506\n",
      "[Arousal (Fold 1)] Ep 24/70 | Loss=0.0008 | Val Acc=66.67% | F1=0.781 | AUC=0.506\n",
      "[Arousal (Fold 1)] Ep 25/70 | Loss=0.0008 | Val Acc=66.67% | F1=0.781 | AUC=0.506\n",
      "[Arousal (Fold 1)] Ep 26/70 | Loss=0.0007 | Val Acc=66.67% | F1=0.781 | AUC=0.511\n",
      "[Arousal (Fold 1)] Ep 27/70 | Loss=0.0007 | Val Acc=66.67% | F1=0.781 | AUC=0.510\n",
      "[Arousal (Fold 1)] Ep 28/70 | Loss=0.0006 | Val Acc=66.67% | F1=0.781 | AUC=0.506\n",
      "[Arousal (Fold 1)] Ep 29/70 | Loss=0.0006 | Val Acc=66.67% | F1=0.781 | AUC=0.506\n",
      "[Arousal (Fold 1)] Ep 30/70 | Loss=0.0005 | Val Acc=66.67% | F1=0.781 | AUC=0.506\n",
      "[Arousal (Fold 1)] Ep 31/70 | Loss=0.0005 | Val Acc=66.67% | F1=0.781 | AUC=0.506\n",
      "â¹ [Arousal (Fold 1)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 1)] TEST (best thr=0.450) | Acc=69.05% | F1=0.787 | AUC=0.550\n",
      "\n",
      "Fold 1 Results: Acc=0.6905, F1=0.7869, AUC=0.5500\n",
      "\n",
      "----- Arousal Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 2) | epochs=70\n",
      "[Arousal (Fold 2)] Ep 01/70 | Loss=0.2403 | Val Acc=64.29% | F1=0.769 | AUC=0.381\n",
      "[Arousal (Fold 2)] Ep 02/70 | Loss=0.0278 | Val Acc=59.52% | F1=0.738 | AUC=0.436\n",
      "[Arousal (Fold 2)] Ep 03/70 | Loss=0.0074 | Val Acc=64.29% | F1=0.776 | AUC=0.439\n",
      "[Arousal (Fold 2)] Ep 04/70 | Loss=0.0051 | Val Acc=64.29% | F1=0.776 | AUC=0.436\n",
      "[Arousal (Fold 2)] Ep 05/70 | Loss=0.0039 | Val Acc=64.29% | F1=0.776 | AUC=0.431\n",
      "[Arousal (Fold 2)] Ep 06/70 | Loss=0.0032 | Val Acc=66.67% | F1=0.788 | AUC=0.419\n",
      "[Arousal (Fold 2)] Ep 07/70 | Loss=0.0026 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 08/70 | Loss=0.0022 | Val Acc=66.67% | F1=0.788 | AUC=0.425\n",
      "[Arousal (Fold 2)] Ep 09/70 | Loss=0.0020 | Val Acc=66.67% | F1=0.788 | AUC=0.428\n",
      "[Arousal (Fold 2)] Ep 10/70 | Loss=0.0018 | Val Acc=66.67% | F1=0.788 | AUC=0.425\n",
      "[Arousal (Fold 2)] Ep 11/70 | Loss=0.0017 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 12/70 | Loss=0.0015 | Val Acc=66.67% | F1=0.788 | AUC=0.419\n",
      "[Arousal (Fold 2)] Ep 13/70 | Loss=0.0015 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 14/70 | Loss=0.0014 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 15/70 | Loss=0.0013 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 16/70 | Loss=0.0013 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 17/70 | Loss=0.0013 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 18/70 | Loss=0.0012 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 19/70 | Loss=0.0013 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 20/70 | Loss=0.0012 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 21/70 | Loss=0.0012 | Val Acc=66.67% | F1=0.788 | AUC=0.422\n",
      "[Arousal (Fold 2)] Ep 22/70 | Loss=0.0010 | Val Acc=66.67% | F1=0.788 | AUC=0.419\n",
      "[Arousal (Fold 2)] Ep 23/70 | Loss=0.0009 | Val Acc=66.67% | F1=0.788 | AUC=0.419\n",
      "[Arousal (Fold 2)] Ep 24/70 | Loss=0.0007 | Val Acc=66.67% | F1=0.788 | AUC=0.417\n",
      "[Arousal (Fold 2)] Ep 25/70 | Loss=0.0007 | Val Acc=66.67% | F1=0.788 | AUC=0.414\n",
      "[Arousal (Fold 2)] Ep 26/70 | Loss=0.0006 | Val Acc=66.67% | F1=0.788 | AUC=0.417\n",
      "[Arousal (Fold 2)] Ep 27/70 | Loss=0.0006 | Val Acc=66.67% | F1=0.788 | AUC=0.414\n",
      "[Arousal (Fold 2)] Ep 28/70 | Loss=0.0005 | Val Acc=66.67% | F1=0.788 | AUC=0.414\n",
      "[Arousal (Fold 2)] Ep 29/70 | Loss=0.0005 | Val Acc=66.67% | F1=0.788 | AUC=0.417\n",
      "[Arousal (Fold 2)] Ep 30/70 | Loss=0.0005 | Val Acc=66.67% | F1=0.788 | AUC=0.417\n",
      "[Arousal (Fold 2)] Ep 31/70 | Loss=0.0004 | Val Acc=66.67% | F1=0.788 | AUC=0.417\n",
      "â¹ [Arousal (Fold 2)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 2)] TEST (best thr=0.100) | Acc=66.67% | F1=0.774 | AUC=0.567\n",
      "\n",
      "Fold 2 Results: Acc=0.6667, F1=0.7742, AUC=0.5667\n",
      "\n",
      "----- Arousal Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 3) | epochs=70\n",
      "[Arousal (Fold 3)] Ep 01/70 | Loss=0.2619 | Val Acc=64.29% | F1=0.762 | AUC=0.581\n",
      "[Arousal (Fold 3)] Ep 02/70 | Loss=0.0391 | Val Acc=61.90% | F1=0.750 | AUC=0.581\n",
      "[Arousal (Fold 3)] Ep 03/70 | Loss=0.0094 | Val Acc=64.29% | F1=0.762 | AUC=0.589\n",
      "[Arousal (Fold 3)] Ep 04/70 | Loss=0.0058 | Val Acc=64.29% | F1=0.762 | AUC=0.603\n",
      "[Arousal (Fold 3)] Ep 05/70 | Loss=0.0043 | Val Acc=64.29% | F1=0.762 | AUC=0.603\n",
      "[Arousal (Fold 3)] Ep 06/70 | Loss=0.0035 | Val Acc=64.29% | F1=0.762 | AUC=0.606\n",
      "[Arousal (Fold 3)] Ep 07/70 | Loss=0.0028 | Val Acc=64.29% | F1=0.762 | AUC=0.603\n",
      "[Arousal (Fold 3)] Ep 08/70 | Loss=0.0024 | Val Acc=64.29% | F1=0.762 | AUC=0.614\n",
      "[Arousal (Fold 3)] Ep 09/70 | Loss=0.0022 | Val Acc=64.29% | F1=0.762 | AUC=0.611\n",
      "[Arousal (Fold 3)] Ep 10/70 | Loss=0.0019 | Val Acc=64.29% | F1=0.762 | AUC=0.617\n",
      "[Arousal (Fold 3)] Ep 11/70 | Loss=0.0017 | Val Acc=64.29% | F1=0.762 | AUC=0.617\n",
      "[Arousal (Fold 3)] Ep 12/70 | Loss=0.0016 | Val Acc=64.29% | F1=0.762 | AUC=0.617\n",
      "[Arousal (Fold 3)] Ep 13/70 | Loss=0.0015 | Val Acc=64.29% | F1=0.762 | AUC=0.617\n",
      "[Arousal (Fold 3)] Ep 14/70 | Loss=0.0014 | Val Acc=64.29% | F1=0.762 | AUC=0.619\n",
      "[Arousal (Fold 3)] Ep 15/70 | Loss=0.0014 | Val Acc=64.29% | F1=0.762 | AUC=0.619\n",
      "[Arousal (Fold 3)] Ep 16/70 | Loss=0.0013 | Val Acc=64.29% | F1=0.762 | AUC=0.619\n",
      "[Arousal (Fold 3)] Ep 17/70 | Loss=0.0012 | Val Acc=64.29% | F1=0.762 | AUC=0.619\n",
      "[Arousal (Fold 3)] Ep 18/70 | Loss=0.0013 | Val Acc=64.29% | F1=0.762 | AUC=0.622\n",
      "[Arousal (Fold 3)] Ep 19/70 | Loss=0.0013 | Val Acc=64.29% | F1=0.762 | AUC=0.622\n",
      "[Arousal (Fold 3)] Ep 20/70 | Loss=0.0013 | Val Acc=64.29% | F1=0.762 | AUC=0.622\n",
      "[Arousal (Fold 3)] Ep 21/70 | Loss=0.0012 | Val Acc=64.29% | F1=0.762 | AUC=0.619\n",
      "[Arousal (Fold 3)] Ep 22/70 | Loss=0.0010 | Val Acc=64.29% | F1=0.762 | AUC=0.622\n",
      "[Arousal (Fold 3)] Ep 23/70 | Loss=0.0009 | Val Acc=64.29% | F1=0.762 | AUC=0.614\n",
      "[Arousal (Fold 3)] Ep 24/70 | Loss=0.0008 | Val Acc=64.29% | F1=0.762 | AUC=0.611\n",
      "[Arousal (Fold 3)] Ep 25/70 | Loss=0.0007 | Val Acc=64.29% | F1=0.762 | AUC=0.608\n",
      "[Arousal (Fold 3)] Ep 26/70 | Loss=0.0006 | Val Acc=64.29% | F1=0.762 | AUC=0.608\n",
      "[Arousal (Fold 3)] Ep 27/70 | Loss=0.0006 | Val Acc=64.29% | F1=0.762 | AUC=0.606\n",
      "[Arousal (Fold 3)] Ep 28/70 | Loss=0.0005 | Val Acc=64.29% | F1=0.762 | AUC=0.606\n",
      "[Arousal (Fold 3)] Ep 29/70 | Loss=0.0005 | Val Acc=64.29% | F1=0.762 | AUC=0.606\n",
      "[Arousal (Fold 3)] Ep 30/70 | Loss=0.0005 | Val Acc=64.29% | F1=0.762 | AUC=0.606\n",
      "[Arousal (Fold 3)] Ep 31/70 | Loss=0.0005 | Val Acc=64.29% | F1=0.762 | AUC=0.606\n",
      "â¹ [Arousal (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 3)] TEST (best thr=0.250) | Acc=73.81% | F1=0.820 | AUC=0.656\n",
      "\n",
      "Fold 3 Results: Acc=0.7381, F1=0.8197, AUC=0.6556\n",
      "\n",
      "----- Arousal Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 4) | epochs=70\n",
      "[Arousal (Fold 4)] Ep 01/70 | Loss=0.2578 | Val Acc=66.67% | F1=0.788 | AUC=0.425\n",
      "[Arousal (Fold 4)] Ep 02/70 | Loss=0.0344 | Val Acc=61.90% | F1=0.758 | AUC=0.411\n",
      "[Arousal (Fold 4)] Ep 03/70 | Loss=0.0086 | Val Acc=59.52% | F1=0.738 | AUC=0.408\n",
      "[Arousal (Fold 4)] Ep 04/70 | Loss=0.0055 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 05/70 | Loss=0.0042 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 06/70 | Loss=0.0034 | Val Acc=61.90% | F1=0.758 | AUC=0.417\n",
      "[Arousal (Fold 4)] Ep 07/70 | Loss=0.0029 | Val Acc=61.90% | F1=0.758 | AUC=0.422\n",
      "[Arousal (Fold 4)] Ep 08/70 | Loss=0.0025 | Val Acc=61.90% | F1=0.758 | AUC=0.422\n",
      "[Arousal (Fold 4)] Ep 09/70 | Loss=0.0021 | Val Acc=61.90% | F1=0.758 | AUC=0.425\n",
      "[Arousal (Fold 4)] Ep 10/70 | Loss=0.0020 | Val Acc=61.90% | F1=0.758 | AUC=0.425\n",
      "[Arousal (Fold 4)] Ep 11/70 | Loss=0.0018 | Val Acc=61.90% | F1=0.758 | AUC=0.425\n",
      "[Arousal (Fold 4)] Ep 12/70 | Loss=0.0017 | Val Acc=61.90% | F1=0.758 | AUC=0.425\n",
      "[Arousal (Fold 4)] Ep 13/70 | Loss=0.0015 | Val Acc=61.90% | F1=0.758 | AUC=0.425\n",
      "[Arousal (Fold 4)] Ep 14/70 | Loss=0.0016 | Val Acc=61.90% | F1=0.758 | AUC=0.425\n",
      "[Arousal (Fold 4)] Ep 15/70 | Loss=0.0015 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 16/70 | Loss=0.0015 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 17/70 | Loss=0.0014 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 18/70 | Loss=0.0014 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 19/70 | Loss=0.0013 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 20/70 | Loss=0.0013 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 21/70 | Loss=0.0013 | Val Acc=61.90% | F1=0.758 | AUC=0.417\n",
      "[Arousal (Fold 4)] Ep 22/70 | Loss=0.0011 | Val Acc=61.90% | F1=0.758 | AUC=0.414\n",
      "[Arousal (Fold 4)] Ep 23/70 | Loss=0.0010 | Val Acc=61.90% | F1=0.758 | AUC=0.414\n",
      "[Arousal (Fold 4)] Ep 24/70 | Loss=0.0008 | Val Acc=61.90% | F1=0.758 | AUC=0.414\n",
      "[Arousal (Fold 4)] Ep 25/70 | Loss=0.0008 | Val Acc=61.90% | F1=0.758 | AUC=0.417\n",
      "[Arousal (Fold 4)] Ep 26/70 | Loss=0.0007 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 27/70 | Loss=0.0006 | Val Acc=61.90% | F1=0.758 | AUC=0.419\n",
      "[Arousal (Fold 4)] Ep 28/70 | Loss=0.0006 | Val Acc=61.90% | F1=0.758 | AUC=0.422\n",
      "[Arousal (Fold 4)] Ep 29/70 | Loss=0.0006 | Val Acc=61.90% | F1=0.758 | AUC=0.422\n",
      "[Arousal (Fold 4)] Ep 30/70 | Loss=0.0005 | Val Acc=61.90% | F1=0.758 | AUC=0.422\n",
      "[Arousal (Fold 4)] Ep 31/70 | Loss=0.0005 | Val Acc=61.90% | F1=0.758 | AUC=0.425\n",
      "â¹ [Arousal (Fold 4)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 4)] TEST (best thr=0.200) | Acc=57.14% | F1=0.679 | AUC=0.550\n",
      "\n",
      "Fold 4 Results: Acc=0.5714, F1=0.6786, AUC=0.5500\n",
      "\n",
      "----- Arousal Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 5) | epochs=70\n",
      "[Arousal (Fold 5)] Ep 01/70 | Loss=0.2657 | Val Acc=68.29% | F1=0.787 | AUC=0.558\n",
      "[Arousal (Fold 5)] Ep 02/70 | Loss=0.0399 | Val Acc=58.54% | F1=0.712 | AUC=0.548\n",
      "[Arousal (Fold 5)] Ep 03/70 | Loss=0.0098 | Val Acc=60.98% | F1=0.733 | AUC=0.533\n",
      "[Arousal (Fold 5)] Ep 04/70 | Loss=0.0058 | Val Acc=63.41% | F1=0.754 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 05/70 | Loss=0.0044 | Val Acc=63.41% | F1=0.754 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 06/70 | Loss=0.0036 | Val Acc=65.85% | F1=0.774 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 07/70 | Loss=0.0029 | Val Acc=65.85% | F1=0.774 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 08/70 | Loss=0.0025 | Val Acc=65.85% | F1=0.774 | AUC=0.542\n",
      "[Arousal (Fold 5)] Ep 09/70 | Loss=0.0022 | Val Acc=65.85% | F1=0.774 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 10/70 | Loss=0.0020 | Val Acc=65.85% | F1=0.774 | AUC=0.542\n",
      "[Arousal (Fold 5)] Ep 11/70 | Loss=0.0019 | Val Acc=65.85% | F1=0.774 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 12/70 | Loss=0.0017 | Val Acc=63.41% | F1=0.754 | AUC=0.545\n",
      "[Arousal (Fold 5)] Ep 13/70 | Loss=0.0016 | Val Acc=63.41% | F1=0.754 | AUC=0.545\n",
      "[Arousal (Fold 5)] Ep 14/70 | Loss=0.0015 | Val Acc=65.85% | F1=0.774 | AUC=0.545\n",
      "[Arousal (Fold 5)] Ep 15/70 | Loss=0.0015 | Val Acc=65.85% | F1=0.774 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 16/70 | Loss=0.0014 | Val Acc=63.41% | F1=0.754 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 17/70 | Loss=0.0014 | Val Acc=63.41% | F1=0.754 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 18/70 | Loss=0.0014 | Val Acc=63.41% | F1=0.754 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 19/70 | Loss=0.0014 | Val Acc=63.41% | F1=0.754 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 20/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.754 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 21/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.754 | AUC=0.545\n",
      "[Arousal (Fold 5)] Ep 22/70 | Loss=0.0011 | Val Acc=63.41% | F1=0.754 | AUC=0.555\n",
      "[Arousal (Fold 5)] Ep 23/70 | Loss=0.0010 | Val Acc=65.85% | F1=0.774 | AUC=0.552\n",
      "[Arousal (Fold 5)] Ep 24/70 | Loss=0.0009 | Val Acc=65.85% | F1=0.774 | AUC=0.552\n",
      "[Arousal (Fold 5)] Ep 25/70 | Loss=0.0008 | Val Acc=63.41% | F1=0.754 | AUC=0.552\n",
      "[Arousal (Fold 5)] Ep 26/70 | Loss=0.0007 | Val Acc=63.41% | F1=0.754 | AUC=0.555\n",
      "[Arousal (Fold 5)] Ep 27/70 | Loss=0.0006 | Val Acc=63.41% | F1=0.754 | AUC=0.545\n",
      "[Arousal (Fold 5)] Ep 28/70 | Loss=0.0006 | Val Acc=63.41% | F1=0.754 | AUC=0.545\n",
      "[Arousal (Fold 5)] Ep 29/70 | Loss=0.0005 | Val Acc=63.41% | F1=0.754 | AUC=0.548\n",
      "[Arousal (Fold 5)] Ep 30/70 | Loss=0.0005 | Val Acc=63.41% | F1=0.754 | AUC=0.548\n",
      "[Arousal (Fold 5)] Ep 31/70 | Loss=0.0005 | Val Acc=63.41% | F1=0.754 | AUC=0.545\n",
      "â¹ [Arousal (Fold 5)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 5)] TEST (best thr=0.100) | Acc=65.85% | F1=0.774 | AUC=0.570\n",
      "\n",
      "Fold 5 Results: Acc=0.6585, F1=0.7742, AUC=0.5697\n",
      "\n",
      "----- Arousal Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 6) | epochs=70\n",
      "[Arousal (Fold 6)] Ep 01/70 | Loss=0.2677 | Val Acc=68.29% | F1=0.787 | AUC=0.679\n",
      "[Arousal (Fold 6)] Ep 02/70 | Loss=0.0371 | Val Acc=73.17% | F1=0.820 | AUC=0.594\n",
      "[Arousal (Fold 6)] Ep 03/70 | Loss=0.0085 | Val Acc=65.85% | F1=0.759 | AUC=0.609\n",
      "[Arousal (Fold 6)] Ep 04/70 | Loss=0.0054 | Val Acc=68.29% | F1=0.780 | AUC=0.606\n",
      "[Arousal (Fold 6)] Ep 05/70 | Loss=0.0037 | Val Acc=65.85% | F1=0.759 | AUC=0.603\n",
      "[Arousal (Fold 6)] Ep 06/70 | Loss=0.0030 | Val Acc=65.85% | F1=0.759 | AUC=0.612\n",
      "[Arousal (Fold 6)] Ep 07/70 | Loss=0.0026 | Val Acc=65.85% | F1=0.759 | AUC=0.606\n",
      "[Arousal (Fold 6)] Ep 08/70 | Loss=0.0022 | Val Acc=65.85% | F1=0.759 | AUC=0.606\n",
      "[Arousal (Fold 6)] Ep 09/70 | Loss=0.0020 | Val Acc=65.85% | F1=0.759 | AUC=0.606\n",
      "[Arousal (Fold 6)] Ep 10/70 | Loss=0.0018 | Val Acc=68.29% | F1=0.772 | AUC=0.609\n",
      "[Arousal (Fold 6)] Ep 11/70 | Loss=0.0017 | Val Acc=68.29% | F1=0.772 | AUC=0.609\n",
      "[Arousal (Fold 6)] Ep 12/70 | Loss=0.0015 | Val Acc=68.29% | F1=0.772 | AUC=0.609\n",
      "[Arousal (Fold 6)] Ep 13/70 | Loss=0.0014 | Val Acc=68.29% | F1=0.772 | AUC=0.615\n",
      "[Arousal (Fold 6)] Ep 14/70 | Loss=0.0014 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 15/70 | Loss=0.0013 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 16/70 | Loss=0.0013 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 17/70 | Loss=0.0013 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 18/70 | Loss=0.0013 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 19/70 | Loss=0.0013 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 20/70 | Loss=0.0012 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 21/70 | Loss=0.0012 | Val Acc=68.29% | F1=0.772 | AUC=0.615\n",
      "[Arousal (Fold 6)] Ep 22/70 | Loss=0.0010 | Val Acc=68.29% | F1=0.772 | AUC=0.615\n",
      "[Arousal (Fold 6)] Ep 23/70 | Loss=0.0008 | Val Acc=68.29% | F1=0.772 | AUC=0.615\n",
      "[Arousal (Fold 6)] Ep 24/70 | Loss=0.0007 | Val Acc=68.29% | F1=0.772 | AUC=0.621\n",
      "[Arousal (Fold 6)] Ep 25/70 | Loss=0.0007 | Val Acc=68.29% | F1=0.772 | AUC=0.621\n",
      "[Arousal (Fold 6)] Ep 26/70 | Loss=0.0006 | Val Acc=68.29% | F1=0.772 | AUC=0.621\n",
      "[Arousal (Fold 6)] Ep 27/70 | Loss=0.0006 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 28/70 | Loss=0.0005 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 29/70 | Loss=0.0005 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 30/70 | Loss=0.0005 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 31/70 | Loss=0.0005 | Val Acc=68.29% | F1=0.772 | AUC=0.618\n",
      "â¹ [Arousal (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 6)] TEST (best thr=0.550) | Acc=75.61% | F1=0.839 | AUC=0.661\n",
      "\n",
      "Fold 6 Results: Acc=0.7561, F1=0.8387, AUC=0.6606\n",
      "\n",
      "----- Arousal Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 7) | epochs=70\n",
      "[Arousal (Fold 7)] Ep 01/70 | Loss=0.2654 | Val Acc=73.17% | F1=0.836 | AUC=0.612\n",
      "[Arousal (Fold 7)] Ep 02/70 | Loss=0.0384 | Val Acc=70.73% | F1=0.806 | AUC=0.655\n",
      "[Arousal (Fold 7)] Ep 03/70 | Loss=0.0095 | Val Acc=68.29% | F1=0.794 | AUC=0.636\n",
      "[Arousal (Fold 7)] Ep 04/70 | Loss=0.0061 | Val Acc=70.73% | F1=0.806 | AUC=0.645\n",
      "[Arousal (Fold 7)] Ep 05/70 | Loss=0.0047 | Val Acc=70.73% | F1=0.806 | AUC=0.642\n",
      "[Arousal (Fold 7)] Ep 06/70 | Loss=0.0036 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 07/70 | Loss=0.0031 | Val Acc=70.73% | F1=0.806 | AUC=0.645\n",
      "[Arousal (Fold 7)] Ep 08/70 | Loss=0.0025 | Val Acc=70.73% | F1=0.806 | AUC=0.645\n",
      "[Arousal (Fold 7)] Ep 09/70 | Loss=0.0022 | Val Acc=70.73% | F1=0.806 | AUC=0.645\n",
      "[Arousal (Fold 7)] Ep 10/70 | Loss=0.0020 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 11/70 | Loss=0.0019 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 12/70 | Loss=0.0017 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 13/70 | Loss=0.0017 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 14/70 | Loss=0.0015 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 15/70 | Loss=0.0015 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 16/70 | Loss=0.0015 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 17/70 | Loss=0.0014 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 18/70 | Loss=0.0015 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 19/70 | Loss=0.0014 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 20/70 | Loss=0.0014 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 21/70 | Loss=0.0013 | Val Acc=70.73% | F1=0.806 | AUC=0.647\n",
      "[Arousal (Fold 7)] Ep 22/70 | Loss=0.0011 | Val Acc=70.73% | F1=0.806 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 23/70 | Loss=0.0009 | Val Acc=70.73% | F1=0.806 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 24/70 | Loss=0.0009 | Val Acc=70.73% | F1=0.806 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 25/70 | Loss=0.0008 | Val Acc=70.73% | F1=0.806 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 26/70 | Loss=0.0007 | Val Acc=70.73% | F1=0.806 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 27/70 | Loss=0.0006 | Val Acc=70.73% | F1=0.806 | AUC=0.655\n",
      "[Arousal (Fold 7)] Ep 28/70 | Loss=0.0006 | Val Acc=70.73% | F1=0.806 | AUC=0.655\n",
      "[Arousal (Fold 7)] Ep 29/70 | Loss=0.0005 | Val Acc=70.73% | F1=0.806 | AUC=0.655\n",
      "[Arousal (Fold 7)] Ep 30/70 | Loss=0.0005 | Val Acc=70.73% | F1=0.806 | AUC=0.655\n",
      "[Arousal (Fold 7)] Ep 31/70 | Loss=0.0005 | Val Acc=70.73% | F1=0.806 | AUC=0.655\n",
      "â¹ [Arousal (Fold 7)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 7)] TEST (best thr=0.150) | Acc=65.85% | F1=0.781 | AUC=0.624\n",
      "\n",
      "Fold 7 Results: Acc=0.6585, F1=0.7812, AUC=0.6242\n",
      "\n",
      "----- Arousal Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 8) | epochs=70\n",
      "[Arousal (Fold 8)] Ep 01/70 | Loss=0.2730 | Val Acc=63.41% | F1=0.746 | AUC=0.600\n",
      "[Arousal (Fold 8)] Ep 02/70 | Loss=0.0363 | Val Acc=65.85% | F1=0.767 | AUC=0.591\n",
      "[Arousal (Fold 8)] Ep 03/70 | Loss=0.0099 | Val Acc=63.41% | F1=0.746 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 04/70 | Loss=0.0062 | Val Acc=65.85% | F1=0.767 | AUC=0.555\n",
      "[Arousal (Fold 8)] Ep 05/70 | Loss=0.0046 | Val Acc=65.85% | F1=0.767 | AUC=0.573\n",
      "[Arousal (Fold 8)] Ep 06/70 | Loss=0.0036 | Val Acc=65.85% | F1=0.767 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 07/70 | Loss=0.0031 | Val Acc=65.85% | F1=0.767 | AUC=0.558\n",
      "[Arousal (Fold 8)] Ep 08/70 | Loss=0.0028 | Val Acc=63.41% | F1=0.746 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 09/70 | Loss=0.0023 | Val Acc=63.41% | F1=0.746 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 10/70 | Loss=0.0021 | Val Acc=63.41% | F1=0.746 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 11/70 | Loss=0.0019 | Val Acc=65.85% | F1=0.767 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 12/70 | Loss=0.0018 | Val Acc=65.85% | F1=0.767 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 13/70 | Loss=0.0017 | Val Acc=65.85% | F1=0.767 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 14/70 | Loss=0.0016 | Val Acc=65.85% | F1=0.767 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 15/70 | Loss=0.0016 | Val Acc=63.41% | F1=0.746 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 16/70 | Loss=0.0015 | Val Acc=63.41% | F1=0.746 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 17/70 | Loss=0.0014 | Val Acc=65.85% | F1=0.767 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 18/70 | Loss=0.0015 | Val Acc=63.41% | F1=0.746 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 19/70 | Loss=0.0014 | Val Acc=63.41% | F1=0.746 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 20/70 | Loss=0.0014 | Val Acc=63.41% | F1=0.746 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 21/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.746 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 22/70 | Loss=0.0011 | Val Acc=63.41% | F1=0.746 | AUC=0.564\n",
      "[Arousal (Fold 8)] Ep 23/70 | Loss=0.0010 | Val Acc=63.41% | F1=0.746 | AUC=0.558\n",
      "[Arousal (Fold 8)] Ep 24/70 | Loss=0.0009 | Val Acc=65.85% | F1=0.759 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 25/70 | Loss=0.0008 | Val Acc=65.85% | F1=0.759 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 26/70 | Loss=0.0007 | Val Acc=65.85% | F1=0.759 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 27/70 | Loss=0.0006 | Val Acc=65.85% | F1=0.759 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 28/70 | Loss=0.0006 | Val Acc=65.85% | F1=0.759 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 29/70 | Loss=0.0006 | Val Acc=65.85% | F1=0.759 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 30/70 | Loss=0.0005 | Val Acc=65.85% | F1=0.759 | AUC=0.561\n",
      "[Arousal (Fold 8)] Ep 31/70 | Loss=0.0005 | Val Acc=65.85% | F1=0.759 | AUC=0.561\n",
      "â¹ [Arousal (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 8)] TEST (best thr=0.100) | Acc=65.85% | F1=0.767 | AUC=0.648\n",
      "\n",
      "Fold 8 Results: Acc=0.6585, F1=0.7667, AUC=0.6485\n",
      "\n",
      "----- Arousal Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 9) | epochs=70\n",
      "[Arousal (Fold 9)] Ep 01/70 | Loss=0.2649 | Val Acc=70.73% | F1=0.800 | AUC=0.594\n",
      "[Arousal (Fold 9)] Ep 02/70 | Loss=0.0351 | Val Acc=65.85% | F1=0.767 | AUC=0.636\n",
      "[Arousal (Fold 9)] Ep 03/70 | Loss=0.0094 | Val Acc=65.85% | F1=0.767 | AUC=0.621\n",
      "[Arousal (Fold 9)] Ep 04/70 | Loss=0.0056 | Val Acc=68.29% | F1=0.787 | AUC=0.603\n",
      "[Arousal (Fold 9)] Ep 05/70 | Loss=0.0042 | Val Acc=65.85% | F1=0.767 | AUC=0.600\n",
      "[Arousal (Fold 9)] Ep 06/70 | Loss=0.0033 | Val Acc=65.85% | F1=0.767 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 07/70 | Loss=0.0028 | Val Acc=65.85% | F1=0.767 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 08/70 | Loss=0.0024 | Val Acc=65.85% | F1=0.767 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 09/70 | Loss=0.0022 | Val Acc=65.85% | F1=0.767 | AUC=0.600\n",
      "[Arousal (Fold 9)] Ep 10/70 | Loss=0.0019 | Val Acc=65.85% | F1=0.767 | AUC=0.600\n",
      "[Arousal (Fold 9)] Ep 11/70 | Loss=0.0018 | Val Acc=65.85% | F1=0.767 | AUC=0.603\n",
      "[Arousal (Fold 9)] Ep 12/70 | Loss=0.0016 | Val Acc=65.85% | F1=0.767 | AUC=0.603\n",
      "[Arousal (Fold 9)] Ep 13/70 | Loss=0.0015 | Val Acc=63.41% | F1=0.746 | AUC=0.603\n",
      "[Arousal (Fold 9)] Ep 14/70 | Loss=0.0015 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 15/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 16/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 17/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 18/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 19/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 20/70 | Loss=0.0013 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 21/70 | Loss=0.0012 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 22/70 | Loss=0.0010 | Val Acc=63.41% | F1=0.746 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 23/70 | Loss=0.0009 | Val Acc=65.85% | F1=0.767 | AUC=0.609\n",
      "[Arousal (Fold 9)] Ep 24/70 | Loss=0.0008 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 25/70 | Loss=0.0007 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 26/70 | Loss=0.0007 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 27/70 | Loss=0.0006 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 28/70 | Loss=0.0006 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 29/70 | Loss=0.0005 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 30/70 | Loss=0.0005 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 31/70 | Loss=0.0004 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "â¹ [Arousal (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 9)] TEST (best thr=0.800) | Acc=63.41% | F1=0.754 | AUC=0.509\n",
      "\n",
      "Fold 9 Results: Acc=0.6341, F1=0.7541, AUC=0.5091\n",
      "\n",
      "----- Arousal Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 10) | epochs=70\n",
      "[Arousal (Fold 10)] Ep 01/70 | Loss=0.2632 | Val Acc=68.29% | F1=0.772 | AUC=0.655\n",
      "[Arousal (Fold 10)] Ep 02/70 | Loss=0.0379 | Val Acc=65.85% | F1=0.759 | AUC=0.618\n",
      "[Arousal (Fold 10)] Ep 03/70 | Loss=0.0094 | Val Acc=68.29% | F1=0.780 | AUC=0.615\n",
      "[Arousal (Fold 10)] Ep 04/70 | Loss=0.0054 | Val Acc=65.85% | F1=0.759 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 05/70 | Loss=0.0042 | Val Acc=65.85% | F1=0.759 | AUC=0.627\n",
      "[Arousal (Fold 10)] Ep 06/70 | Loss=0.0033 | Val Acc=65.85% | F1=0.759 | AUC=0.627\n",
      "[Arousal (Fold 10)] Ep 07/70 | Loss=0.0028 | Val Acc=65.85% | F1=0.759 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 08/70 | Loss=0.0024 | Val Acc=65.85% | F1=0.759 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 09/70 | Loss=0.0021 | Val Acc=65.85% | F1=0.759 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 10/70 | Loss=0.0019 | Val Acc=65.85% | F1=0.759 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 11/70 | Loss=0.0018 | Val Acc=65.85% | F1=0.759 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 12/70 | Loss=0.0016 | Val Acc=65.85% | F1=0.759 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 13/70 | Loss=0.0015 | Val Acc=65.85% | F1=0.759 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 14/70 | Loss=0.0015 | Val Acc=65.85% | F1=0.759 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 15/70 | Loss=0.0015 | Val Acc=65.85% | F1=0.759 | AUC=0.624\n",
      "[Arousal (Fold 10)] Ep 16/70 | Loss=0.0014 | Val Acc=65.85% | F1=0.759 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 17/70 | Loss=0.0014 | Val Acc=65.85% | F1=0.759 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 18/70 | Loss=0.0014 | Val Acc=65.85% | F1=0.759 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 19/70 | Loss=0.0013 | Val Acc=65.85% | F1=0.759 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 20/70 | Loss=0.0013 | Val Acc=65.85% | F1=0.759 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 21/70 | Loss=0.0012 | Val Acc=68.29% | F1=0.780 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 22/70 | Loss=0.0011 | Val Acc=68.29% | F1=0.780 | AUC=0.615\n",
      "[Arousal (Fold 10)] Ep 23/70 | Loss=0.0009 | Val Acc=65.85% | F1=0.767 | AUC=0.609\n",
      "[Arousal (Fold 10)] Ep 24/70 | Loss=0.0008 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 10)] Ep 25/70 | Loss=0.0008 | Val Acc=68.29% | F1=0.780 | AUC=0.612\n",
      "[Arousal (Fold 10)] Ep 26/70 | Loss=0.0007 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 10)] Ep 27/70 | Loss=0.0006 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 10)] Ep 28/70 | Loss=0.0006 | Val Acc=65.85% | F1=0.767 | AUC=0.612\n",
      "[Arousal (Fold 10)] Ep 29/70 | Loss=0.0005 | Val Acc=65.85% | F1=0.767 | AUC=0.609\n",
      "[Arousal (Fold 10)] Ep 30/70 | Loss=0.0005 | Val Acc=65.85% | F1=0.767 | AUC=0.609\n",
      "[Arousal (Fold 10)] Ep 31/70 | Loss=0.0005 | Val Acc=65.85% | F1=0.767 | AUC=0.609\n",
      "â¹ [Arousal (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 10)] TEST (best thr=0.100) | Acc=68.29% | F1=0.780 | AUC=0.688\n",
      "\n",
      "Fold 10 Results: Acc=0.6829, F1=0.7797, AUC=0.6879\n",
      "\n",
      "===== FINAL Arousal 10-FOLD RESULTS =====\n",
      "Accuracy: 0.6715 Â± 0.0490\n",
      "F1-score: 0.7754 Â± 0.0401\n",
      "AUC:      0.6022 Â± 0.0572\n",
      "\n",
      "===== OVERALL SUMMARY =====\n",
      "Valence Acc: mean=0.5729, std=0.0901\n",
      "Arousal Acc: mean=0.6715, std=0.0490\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# MLP on READY-MADE SPECTROGRAMS\n",
    "# (No spectrogram conversion in this file)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------- DEVICE -------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Dataset wrapper (flattened for MLP)\n",
    "# -------------------------------------------------\n",
    "class EEGDatasetMLP(Dataset):\n",
    "    def __init__(self, X_flat, y):\n",
    "        # X_flat: numpy array [N, D]\n",
    "        self.X = np.asarray(X_flat, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).float()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Augmentations: SpecAugment + noise\n",
    "# (same implementations as your previous scripts)\n",
    "# -------------------------------------------------\n",
    "def spec_augment(\n",
    "    X,\n",
    "    time_mask_width=4,\n",
    "    n_time_masks=2,\n",
    "    freq_mask_width=6,\n",
    "    n_freq_masks=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    X: [N, C, F, T] spectrograms (shaped)\n",
    "    Applies random time & frequency masks (SpecAugment style).\n",
    "    \"\"\"\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "\n",
    "    for i in range(N):\n",
    "        # Time masks\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T:\n",
    "                continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "\n",
    "        # Frequency masks\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F:\n",
    "                continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Simple MLP model\n",
    "# -------------------------------------------------\n",
    "class EEG_MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP for flattened spectrograms.\n",
    "    Input dim = C * F * T\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[1024, 512, 256], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.LayerNorm(h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))  # final logit\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, D]\n",
    "        out = self.net(x).squeeze(-1)  # [B]\n",
    "        return out\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Train ONE fold for one emotion (MLP)\n",
    "# -------------------------------------------------\n",
    "def train_one_fold_emotion(\n",
    "    X,\n",
    "    y_cont,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    emotion_name=\"Valence\",\n",
    "    epochs=70,\n",
    "    base_seed=42,\n",
    "    batch_size=32,\n",
    "):\n",
    "    # 1) Binarize labels w.r.t. TRAIN median\n",
    "    y_train_cont = y_cont[train_idx]\n",
    "    thr = np.median(y_train_cont)\n",
    "\n",
    "    y_train_bin = (y_train_cont >= thr).astype(float)\n",
    "    y_val_bin   = (y_cont[val_idx]  >= thr).astype(float)\n",
    "    y_test_bin  = (y_cont[test_idx] >= thr).astype(float)\n",
    "\n",
    "    print(f\"\\n[{emotion_name}] TRAIN median threshold = {thr:.4f}\")\n",
    "    print(\"  Train class counts:\", np.bincount(y_train_bin.astype(int)))\n",
    "    print(\"  Val   class counts:\", np.bincount(y_val_bin.astype(int)))\n",
    "    print(\"  Test  class counts:\", np.bincount(y_test_bin.astype(int)))\n",
    "\n",
    "    # 2) Standardize features using TRAIN only\n",
    "    X = np.nan_to_num(X)\n",
    "    X_train = X[train_idx]    # shaped: [n_train, C, F, T]\n",
    "    X_val   = X[val_idx]\n",
    "    X_test  = X[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(len(train_idx), -1)\n",
    "    scaler.fit(X_train_flat)\n",
    "\n",
    "    def transform_block(X_block):\n",
    "        flat = X_block.reshape(X_block.shape[0], -1)\n",
    "        flat_scaled = scaler.transform(flat)\n",
    "        return flat_scaled\n",
    "\n",
    "    X_train_scaled_flat = transform_block(X_train)\n",
    "    X_val_scaled_flat   = transform_block(X_val)\n",
    "    X_test_scaled_flat  = transform_block(X_test)\n",
    "\n",
    "    # 3) Strong augmentation: SpecAugment + noise (operate on shaped scaled array)\n",
    "    X_train_shaped_scaled = X_train_scaled_flat.reshape(X_train.shape)\n",
    "\n",
    "    X_train_sa_shaped = spec_augment(X_train_shaped_scaled)\n",
    "    X_train_noise_shaped = add_noise(X_train_shaped_scaled, std=0.03)\n",
    "\n",
    "    X_train_sa_flat = X_train_sa_shaped.reshape(X_train_sa_shaped.shape[0], -1)\n",
    "    X_train_noise_flat = X_train_noise_shaped.reshape(X_train_noise_shaped.shape[0], -1)\n",
    "\n",
    "    X_train_aug_flat = np.concatenate(\n",
    "        [X_train_scaled_flat, X_train_sa_flat, X_train_noise_flat],\n",
    "        axis=0,\n",
    "    )\n",
    "    y_train_aug = np.concatenate(\n",
    "        [y_train_bin, y_train_bin, y_train_bin],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # 4) DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        EEGDatasetMLP(X_train_aug_flat, y_train_aug),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        EEGDatasetMLP(X_val_scaled_flat, y_val_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        EEGDatasetMLP(X_test_scaled_flat, y_test_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    # 5) Model + optimizer + CLASS-WEIGHTED loss\n",
    "    torch.manual_seed(base_seed)\n",
    "    random.seed(base_seed)\n",
    "    np.random.seed(base_seed)\n",
    "\n",
    "    input_dim = X_train_aug_flat.shape[1]\n",
    "    model = EEG_MLP(input_dim=input_dim, hidden_dims=[1024, 512, 256], dropout=0.3).to(DEVICE)\n",
    "\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts) > 0 else 1\n",
    "    pos = class_counts[1] if len(class_counts) > 1 else 1\n",
    "    pos_weight_val = float(neg) / float(pos) if pos > 0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    print(f\"[{emotion_name}] pos_weight for BCEWithLogitsLoss = {pos_weight_val:.3f}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "    patience = 18\n",
    "    counter = 0\n",
    "\n",
    "    print(f\"\\nðŸš€ Training {emotion_name} | epochs={epochs}\")\n",
    "    for ep in range(1, epochs + 1):\n",
    "        # ---- train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            n_samples += xb.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss = total_loss / max(1, n_samples)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        y_true_val, y_prob_val = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy())\n",
    "                y_prob_val.extend(probs)\n",
    "\n",
    "        y_true_val = np.array(y_true_val)\n",
    "        y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        val_f1  = f1_score(y_true_val, y_pred_val)\n",
    "        try:\n",
    "            val_auc = roc_auc_score(y_true_val, y_prob_val)\n",
    "        except:\n",
    "            val_auc = float(\"nan\")\n",
    "\n",
    "        print(\n",
    "            f\"[{emotion_name}] Ep {ep:02d}/{epochs} | \"\n",
    "            f\"Loss={train_loss:.4f} | Val Acc={val_acc*100:.2f}% | \"\n",
    "            f\"F1={val_f1:.3f} | AUC={val_auc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # early stopping on val accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience:\n",
    "                print(f\"â¹ [{emotion_name}] Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    # ---- load best & evaluate on TEST (with best threshold) ----\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    y_true_test, y_prob_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy())\n",
    "            y_prob_test.extend(probs)\n",
    "\n",
    "    y_true_test = np.array(y_true_test)\n",
    "    y_prob_test = np.array(y_prob_test)\n",
    "\n",
    "    # search threshold for best TEST accuracy\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for thr_ in np.linspace(0.1, 0.9, 17):\n",
    "        yp = (y_prob_test >= thr_).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc = acc_thr\n",
    "            best_thr = thr_\n",
    "\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1  = f1_score(y_true_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nðŸ”š [{emotion_name}] TEST (best thr={best_thr:.3f}) \"\n",
    "        f\"| Acc={test_acc*100:.2f}% | F1={test_f1:.3f} | AUC={test_auc:.3f}\\n\"\n",
    "    )\n",
    "\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. 10-fold CV driver (same splitting as your original)\n",
    "# -------------------------------------------------\n",
    "def run_10fold_cv_emotion(X, y_cont, emotion_name=\"Valence\", epochs=70, base_seed=42):\n",
    "    \"\"\"\n",
    "    10-fold stratified CV:\n",
    "      - Outer fold chooses TEST.\n",
    "      - Remaining data is split into TRAIN / VAL\n",
    "        so that VAL size â‰ˆ TEST size.\n",
    "    \"\"\"\n",
    "    print(f\"\\n########## {emotion_name}: 10-fold STRATIFIED CV ##########\")\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "\n",
    "    # For stratification: global median threshold\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    print(f\"{emotion_name} global median (for stratification) = {global_thr:.4f}\")\n",
    "    print(\"Class counts:\", np.bincount(y_bin_global.astype(int)))\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=10,\n",
    "        shuffle=True,\n",
    "        random_state=base_seed,\n",
    "    )\n",
    "\n",
    "    fold_accs, fold_f1s, fold_aucs = [], [], []\n",
    "\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(\n",
    "        skf.split(np.arange(len(y_cont)), y_bin_global),\n",
    "        start=1\n",
    "    ):\n",
    "        # Split trainval into train & val with val size ~= test size\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "\n",
    "        tv_train_idx, tv_val_idx = train_test_split(\n",
    "            trainval_idx,\n",
    "            test_size=val_frac,\n",
    "            random_state=base_seed + fold,\n",
    "            shuffle=True,\n",
    "            stratify=y_trainval_bin,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n----- {emotion_name} Fold {fold}/10 -----\")\n",
    "        print(\n",
    "            f\"Train size={len(tv_train_idx)}, \"\n",
    "            f\"Val size={len(tv_val_idx)}, \"\n",
    "            f\"Test size={len(test_idx)}\"\n",
    "        )\n",
    "\n",
    "        acc, f1, auc = train_one_fold_emotion(\n",
    "            X,\n",
    "            y_cont,\n",
    "            tv_train_idx,\n",
    "            tv_val_idx,\n",
    "            test_idx,\n",
    "            emotion_name=f\"{emotion_name} (Fold {fold})\",\n",
    "            epochs=epochs,\n",
    "            base_seed=base_seed + fold,\n",
    "            batch_size=32,\n",
    "        )\n",
    "\n",
    "        fold_accs.append(acc)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_aucs.append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold} Results: \"\n",
    "            f\"Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\"\n",
    "        )\n",
    "\n",
    "    fold_accs = np.array(fold_accs)\n",
    "    fold_f1s  = np.array(fold_f1s)\n",
    "    fold_aucs = np.array(fold_aucs)\n",
    "\n",
    "    print(f\"\\n===== FINAL {emotion_name} 10-FOLD RESULTS =====\")\n",
    "    print(f\"Accuracy: {fold_accs.mean():.4f} Â± {fold_accs.std():.4f}\")\n",
    "    print(f\"F1-score: {fold_f1s.mean():.4f} Â± {fold_f1s.std():.4f}\")\n",
    "    print(f\"AUC:      {fold_aucs.mean():.4f} Â± {fold_aucs.std():.4f}\")\n",
    "\n",
    "    return fold_accs, fold_f1s, fold_aucs\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. MAIN (assumes X, y_val, y_aro already created)\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure you've already run the cell that does:\n",
    "    # X, y_val, y_aro, groups, channels = load_dreamer_and_build_spectrograms(...)\n",
    "    print(\"\\nâœ… Spectrogram dataset:\", X.shape)\n",
    "\n",
    "    # Valence\n",
    "    val_accs, val_f1s, val_aucs = run_10fold_cv_emotion(\n",
    "        X, y_val, emotion_name=\"Valence\", epochs=70, base_seed=42\n",
    "    )\n",
    "\n",
    "    # Arousal\n",
    "    aro_accs, aro_f1s, aro_aucs = run_10fold_cv_emotion(\n",
    "        X, y_aro, emotion_name=\"Arousal\", epochs=70, base_seed=142\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== OVERALL SUMMARY =====\")\n",
    "    print(f\"Valence Acc: mean={val_accs.mean():.4f}, std={val_accs.std():.4f}\")\n",
    "    print(f\"Arousal Acc: mean={aro_accs.mean():.4f}, std={aro_accs.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab95ef9-3121-444b-886b-af5f9eb02c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a8cb1c-c44d-4c9b-9a79-08a4ccbdc878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "âœ… Spectrogram dataset: (414, 14, 36, 32)\n",
      "\n",
      "########## Valence: 10-fold STRATIFIED CV ##########\n",
      "Valence global median (for stratification) = 3.0000\n",
      "Class counts: [161 253]\n",
      "\n",
      "----- Valence Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 1) | epochs=70\n",
      "[Valence (Fold 1)] Ep 01/70 | Loss=0.5388 | Val Acc=61.90% | F1=0.765 | AUC=0.538\n",
      "[Valence (Fold 1)] Ep 02/70 | Loss=0.4902 | Val Acc=61.90% | F1=0.765 | AUC=0.601\n",
      "[Valence (Fold 1)] Ep 03/70 | Loss=0.4607 | Val Acc=61.90% | F1=0.765 | AUC=0.553\n",
      "[Valence (Fold 1)] Ep 04/70 | Loss=0.4431 | Val Acc=61.90% | F1=0.765 | AUC=0.562\n",
      "[Valence (Fold 1)] Ep 05/70 | Loss=0.4336 | Val Acc=61.90% | F1=0.765 | AUC=0.526\n",
      "[Valence (Fold 1)] Ep 06/70 | Loss=0.4300 | Val Acc=61.90% | F1=0.765 | AUC=0.526\n",
      "[Valence (Fold 1)] Ep 07/70 | Loss=0.4260 | Val Acc=61.90% | F1=0.765 | AUC=0.536\n",
      "[Valence (Fold 1)] Ep 08/70 | Loss=0.4230 | Val Acc=61.90% | F1=0.765 | AUC=0.546\n",
      "[Valence (Fold 1)] Ep 09/70 | Loss=0.4184 | Val Acc=61.90% | F1=0.765 | AUC=0.526\n",
      "[Valence (Fold 1)] Ep 10/70 | Loss=0.4167 | Val Acc=61.90% | F1=0.765 | AUC=0.541\n",
      "[Valence (Fold 1)] Ep 11/70 | Loss=0.4157 | Val Acc=61.90% | F1=0.765 | AUC=0.546\n",
      "[Valence (Fold 1)] Ep 12/70 | Loss=0.4132 | Val Acc=61.90% | F1=0.765 | AUC=0.536\n",
      "[Valence (Fold 1)] Ep 13/70 | Loss=0.4119 | Val Acc=61.90% | F1=0.765 | AUC=0.534\n",
      "[Valence (Fold 1)] Ep 14/70 | Loss=0.4111 | Val Acc=61.90% | F1=0.765 | AUC=0.541\n",
      "[Valence (Fold 1)] Ep 15/70 | Loss=0.4103 | Val Acc=61.90% | F1=0.765 | AUC=0.541\n",
      "[Valence (Fold 1)] Ep 16/70 | Loss=0.4096 | Val Acc=61.90% | F1=0.765 | AUC=0.536\n",
      "[Valence (Fold 1)] Ep 17/70 | Loss=0.4094 | Val Acc=61.90% | F1=0.765 | AUC=0.536\n",
      "[Valence (Fold 1)] Ep 18/70 | Loss=0.4091 | Val Acc=61.90% | F1=0.765 | AUC=0.534\n",
      "[Valence (Fold 1)] Ep 19/70 | Loss=0.4088 | Val Acc=61.90% | F1=0.765 | AUC=0.534\n",
      "[Valence (Fold 1)] Ep 20/70 | Loss=0.4088 | Val Acc=61.90% | F1=0.765 | AUC=0.534\n",
      "[Valence (Fold 1)] Ep 21/70 | Loss=0.4193 | Val Acc=61.90% | F1=0.765 | AUC=0.548\n",
      "[Valence (Fold 1)] Ep 22/70 | Loss=0.4321 | Val Acc=61.90% | F1=0.765 | AUC=0.570\n",
      "[Valence (Fold 1)] Ep 23/70 | Loss=0.4245 | Val Acc=61.90% | F1=0.765 | AUC=0.570\n",
      "[Valence (Fold 1)] Ep 24/70 | Loss=0.4229 | Val Acc=61.90% | F1=0.765 | AUC=0.570\n",
      "[Valence (Fold 1)] Ep 25/70 | Loss=0.4200 | Val Acc=61.90% | F1=0.765 | AUC=0.570\n",
      "[Valence (Fold 1)] Ep 26/70 | Loss=0.4157 | Val Acc=61.90% | F1=0.765 | AUC=0.567\n",
      "[Valence (Fold 1)] Ep 27/70 | Loss=0.4173 | Val Acc=61.90% | F1=0.765 | AUC=0.582\n",
      "[Valence (Fold 1)] Ep 28/70 | Loss=0.4135 | Val Acc=61.90% | F1=0.765 | AUC=0.572\n",
      "[Valence (Fold 1)] Ep 29/70 | Loss=0.4112 | Val Acc=61.90% | F1=0.765 | AUC=0.562\n",
      "[Valence (Fold 1)] Ep 30/70 | Loss=0.4083 | Val Acc=61.90% | F1=0.765 | AUC=0.565\n",
      "[Valence (Fold 1)] Ep 31/70 | Loss=0.4070 | Val Acc=61.90% | F1=0.765 | AUC=0.572\n",
      "â¹ [Valence (Fold 1)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 1)] TEST (best thr=0.650) | Acc=66.67% | F1=0.788 | AUC=0.623\n",
      "\n",
      "Fold 1 Results: Acc=0.6667, F1=0.7879, AUC=0.6226\n",
      "\n",
      "----- Valence Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 2) | epochs=70\n",
      "[Valence (Fold 2)] Ep 01/70 | Loss=0.5396 | Val Acc=61.90% | F1=0.765 | AUC=0.589\n",
      "[Valence (Fold 2)] Ep 02/70 | Loss=0.4899 | Val Acc=61.90% | F1=0.765 | AUC=0.553\n",
      "[Valence (Fold 2)] Ep 03/70 | Loss=0.4567 | Val Acc=61.90% | F1=0.765 | AUC=0.550\n",
      "[Valence (Fold 2)] Ep 04/70 | Loss=0.4432 | Val Acc=61.90% | F1=0.765 | AUC=0.526\n",
      "[Valence (Fold 2)] Ep 05/70 | Loss=0.4369 | Val Acc=61.90% | F1=0.765 | AUC=0.553\n",
      "[Valence (Fold 2)] Ep 06/70 | Loss=0.4307 | Val Acc=61.90% | F1=0.765 | AUC=0.558\n",
      "[Valence (Fold 2)] Ep 07/70 | Loss=0.4270 | Val Acc=61.90% | F1=0.765 | AUC=0.534\n",
      "[Valence (Fold 2)] Ep 08/70 | Loss=0.4224 | Val Acc=61.90% | F1=0.765 | AUC=0.555\n",
      "[Valence (Fold 2)] Ep 09/70 | Loss=0.4189 | Val Acc=61.90% | F1=0.765 | AUC=0.562\n",
      "[Valence (Fold 2)] Ep 10/70 | Loss=0.4165 | Val Acc=61.90% | F1=0.765 | AUC=0.567\n",
      "[Valence (Fold 2)] Ep 11/70 | Loss=0.4152 | Val Acc=61.90% | F1=0.765 | AUC=0.567\n",
      "[Valence (Fold 2)] Ep 12/70 | Loss=0.4139 | Val Acc=61.90% | F1=0.765 | AUC=0.579\n",
      "[Valence (Fold 2)] Ep 13/70 | Loss=0.4129 | Val Acc=61.90% | F1=0.765 | AUC=0.562\n",
      "[Valence (Fold 2)] Ep 14/70 | Loss=0.4120 | Val Acc=61.90% | F1=0.765 | AUC=0.572\n",
      "[Valence (Fold 2)] Ep 15/70 | Loss=0.4105 | Val Acc=61.90% | F1=0.765 | AUC=0.575\n",
      "[Valence (Fold 2)] Ep 16/70 | Loss=0.4100 | Val Acc=61.90% | F1=0.765 | AUC=0.562\n",
      "[Valence (Fold 2)] Ep 17/70 | Loss=0.4094 | Val Acc=61.90% | F1=0.765 | AUC=0.565\n",
      "[Valence (Fold 2)] Ep 18/70 | Loss=0.4094 | Val Acc=61.90% | F1=0.765 | AUC=0.565\n",
      "[Valence (Fold 2)] Ep 19/70 | Loss=0.4092 | Val Acc=61.90% | F1=0.765 | AUC=0.565\n",
      "[Valence (Fold 2)] Ep 20/70 | Loss=0.4091 | Val Acc=61.90% | F1=0.765 | AUC=0.565\n",
      "[Valence (Fold 2)] Ep 21/70 | Loss=0.4210 | Val Acc=61.90% | F1=0.765 | AUC=0.579\n",
      "[Valence (Fold 2)] Ep 22/70 | Loss=0.4325 | Val Acc=61.90% | F1=0.765 | AUC=0.577\n",
      "[Valence (Fold 2)] Ep 23/70 | Loss=0.4229 | Val Acc=61.90% | F1=0.765 | AUC=0.575\n",
      "[Valence (Fold 2)] Ep 24/70 | Loss=0.4232 | Val Acc=61.90% | F1=0.765 | AUC=0.562\n",
      "[Valence (Fold 2)] Ep 25/70 | Loss=0.4210 | Val Acc=61.90% | F1=0.765 | AUC=0.565\n",
      "[Valence (Fold 2)] Ep 26/70 | Loss=0.4143 | Val Acc=61.90% | F1=0.765 | AUC=0.572\n",
      "[Valence (Fold 2)] Ep 27/70 | Loss=0.4134 | Val Acc=61.90% | F1=0.765 | AUC=0.594\n",
      "[Valence (Fold 2)] Ep 28/70 | Loss=0.4124 | Val Acc=61.90% | F1=0.765 | AUC=0.587\n",
      "[Valence (Fold 2)] Ep 29/70 | Loss=0.4096 | Val Acc=61.90% | F1=0.765 | AUC=0.591\n",
      "[Valence (Fold 2)] Ep 30/70 | Loss=0.4080 | Val Acc=61.90% | F1=0.765 | AUC=0.589\n",
      "[Valence (Fold 2)] Ep 31/70 | Loss=0.4070 | Val Acc=61.90% | F1=0.765 | AUC=0.582\n",
      "â¹ [Valence (Fold 2)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 2)] TEST (best thr=0.100) | Acc=61.90% | F1=0.765 | AUC=0.565\n",
      "\n",
      "Fold 2 Results: Acc=0.6190, F1=0.7647, AUC=0.5649\n",
      "\n",
      "----- Valence Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 201]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [16 26]\n",
      "[Valence (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.642\n",
      "\n",
      "ðŸš€ Training Valence (Fold 3) | epochs=70\n",
      "[Valence (Fold 3)] Ep 01/70 | Loss=0.5454 | Val Acc=61.90% | F1=0.765 | AUC=0.454\n",
      "[Valence (Fold 3)] Ep 02/70 | Loss=0.4938 | Val Acc=61.90% | F1=0.765 | AUC=0.493\n",
      "[Valence (Fold 3)] Ep 03/70 | Loss=0.4582 | Val Acc=61.90% | F1=0.765 | AUC=0.495\n",
      "[Valence (Fold 3)] Ep 04/70 | Loss=0.4406 | Val Acc=61.90% | F1=0.765 | AUC=0.493\n",
      "[Valence (Fold 3)] Ep 05/70 | Loss=0.4322 | Val Acc=61.90% | F1=0.765 | AUC=0.478\n",
      "[Valence (Fold 3)] Ep 06/70 | Loss=0.4284 | Val Acc=61.90% | F1=0.765 | AUC=0.478\n",
      "[Valence (Fold 3)] Ep 07/70 | Loss=0.4239 | Val Acc=61.90% | F1=0.765 | AUC=0.462\n",
      "[Valence (Fold 3)] Ep 08/70 | Loss=0.4213 | Val Acc=61.90% | F1=0.765 | AUC=0.457\n",
      "[Valence (Fold 3)] Ep 09/70 | Loss=0.4195 | Val Acc=61.90% | F1=0.765 | AUC=0.469\n",
      "[Valence (Fold 3)] Ep 10/70 | Loss=0.4159 | Val Acc=61.90% | F1=0.765 | AUC=0.452\n",
      "[Valence (Fold 3)] Ep 11/70 | Loss=0.4130 | Val Acc=61.90% | F1=0.765 | AUC=0.442\n",
      "[Valence (Fold 3)] Ep 12/70 | Loss=0.4119 | Val Acc=61.90% | F1=0.765 | AUC=0.450\n",
      "[Valence (Fold 3)] Ep 13/70 | Loss=0.4109 | Val Acc=61.90% | F1=0.765 | AUC=0.447\n",
      "[Valence (Fold 3)] Ep 14/70 | Loss=0.4098 | Val Acc=61.90% | F1=0.765 | AUC=0.457\n",
      "[Valence (Fold 3)] Ep 15/70 | Loss=0.4093 | Val Acc=61.90% | F1=0.765 | AUC=0.445\n",
      "[Valence (Fold 3)] Ep 16/70 | Loss=0.4091 | Val Acc=61.90% | F1=0.765 | AUC=0.452\n",
      "[Valence (Fold 3)] Ep 17/70 | Loss=0.4086 | Val Acc=61.90% | F1=0.765 | AUC=0.445\n",
      "[Valence (Fold 3)] Ep 18/70 | Loss=0.4081 | Val Acc=61.90% | F1=0.765 | AUC=0.447\n",
      "[Valence (Fold 3)] Ep 19/70 | Loss=0.4081 | Val Acc=61.90% | F1=0.765 | AUC=0.450\n",
      "[Valence (Fold 3)] Ep 20/70 | Loss=0.4079 | Val Acc=61.90% | F1=0.765 | AUC=0.440\n",
      "[Valence (Fold 3)] Ep 21/70 | Loss=0.4207 | Val Acc=61.90% | F1=0.765 | AUC=0.469\n",
      "[Valence (Fold 3)] Ep 22/70 | Loss=0.4308 | Val Acc=61.90% | F1=0.765 | AUC=0.462\n",
      "[Valence (Fold 3)] Ep 23/70 | Loss=0.4229 | Val Acc=61.90% | F1=0.765 | AUC=0.454\n",
      "[Valence (Fold 3)] Ep 24/70 | Loss=0.4196 | Val Acc=61.90% | F1=0.765 | AUC=0.459\n",
      "[Valence (Fold 3)] Ep 25/70 | Loss=0.4160 | Val Acc=61.90% | F1=0.765 | AUC=0.440\n",
      "[Valence (Fold 3)] Ep 26/70 | Loss=0.4145 | Val Acc=61.90% | F1=0.765 | AUC=0.454\n",
      "[Valence (Fold 3)] Ep 27/70 | Loss=0.4127 | Val Acc=61.90% | F1=0.765 | AUC=0.433\n",
      "[Valence (Fold 3)] Ep 28/70 | Loss=0.4119 | Val Acc=61.90% | F1=0.765 | AUC=0.445\n",
      "[Valence (Fold 3)] Ep 29/70 | Loss=0.4105 | Val Acc=61.90% | F1=0.765 | AUC=0.452\n",
      "[Valence (Fold 3)] Ep 30/70 | Loss=0.4079 | Val Acc=61.90% | F1=0.765 | AUC=0.425\n",
      "[Valence (Fold 3)] Ep 31/70 | Loss=0.4065 | Val Acc=61.90% | F1=0.765 | AUC=0.430\n",
      "â¹ [Valence (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 3)] TEST (best thr=0.100) | Acc=61.90% | F1=0.765 | AUC=0.615\n",
      "\n",
      "Fold 3 Results: Acc=0.6190, F1=0.7647, AUC=0.6154\n",
      "\n",
      "----- Valence Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Valence (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [128 202]\n",
      "  Val   class counts: [16 26]\n",
      "  Test  class counts: [17 25]\n",
      "[Valence (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.634\n",
      "\n",
      "ðŸš€ Training Valence (Fold 4) | epochs=70\n",
      "[Valence (Fold 4)] Ep 01/70 | Loss=0.5386 | Val Acc=61.90% | F1=0.765 | AUC=0.613\n",
      "[Valence (Fold 4)] Ep 02/70 | Loss=0.4887 | Val Acc=61.90% | F1=0.765 | AUC=0.594\n",
      "[Valence (Fold 4)] Ep 03/70 | Loss=0.4536 | Val Acc=61.90% | F1=0.765 | AUC=0.546\n",
      "[Valence (Fold 4)] Ep 04/70 | Loss=0.4391 | Val Acc=61.90% | F1=0.765 | AUC=0.548\n",
      "[Valence (Fold 4)] Ep 05/70 | Loss=0.4326 | Val Acc=61.90% | F1=0.765 | AUC=0.560\n",
      "[Valence (Fold 4)] Ep 06/70 | Loss=0.4250 | Val Acc=61.90% | F1=0.765 | AUC=0.538\n",
      "[Valence (Fold 4)] Ep 07/70 | Loss=0.4216 | Val Acc=61.90% | F1=0.765 | AUC=0.550\n",
      "[Valence (Fold 4)] Ep 08/70 | Loss=0.4174 | Val Acc=61.90% | F1=0.765 | AUC=0.591\n",
      "[Valence (Fold 4)] Ep 09/70 | Loss=0.4154 | Val Acc=61.90% | F1=0.765 | AUC=0.558\n",
      "[Valence (Fold 4)] Ep 10/70 | Loss=0.4132 | Val Acc=61.90% | F1=0.765 | AUC=0.589\n",
      "[Valence (Fold 4)] Ep 11/70 | Loss=0.4112 | Val Acc=61.90% | F1=0.765 | AUC=0.567\n",
      "[Valence (Fold 4)] Ep 12/70 | Loss=0.4096 | Val Acc=61.90% | F1=0.765 | AUC=0.579\n",
      "[Valence (Fold 4)] Ep 13/70 | Loss=0.4088 | Val Acc=61.90% | F1=0.765 | AUC=0.594\n",
      "[Valence (Fold 4)] Ep 14/70 | Loss=0.4075 | Val Acc=61.90% | F1=0.765 | AUC=0.591\n",
      "[Valence (Fold 4)] Ep 15/70 | Loss=0.4064 | Val Acc=61.90% | F1=0.765 | AUC=0.599\n",
      "[Valence (Fold 4)] Ep 16/70 | Loss=0.4058 | Val Acc=61.90% | F1=0.765 | AUC=0.591\n",
      "[Valence (Fold 4)] Ep 17/70 | Loss=0.4056 | Val Acc=61.90% | F1=0.765 | AUC=0.608\n",
      "[Valence (Fold 4)] Ep 18/70 | Loss=0.4054 | Val Acc=61.90% | F1=0.765 | AUC=0.596\n",
      "[Valence (Fold 4)] Ep 19/70 | Loss=0.4051 | Val Acc=61.90% | F1=0.765 | AUC=0.596\n",
      "[Valence (Fold 4)] Ep 20/70 | Loss=0.4052 | Val Acc=61.90% | F1=0.765 | AUC=0.591\n",
      "[Valence (Fold 4)] Ep 21/70 | Loss=0.4167 | Val Acc=61.90% | F1=0.765 | AUC=0.567\n",
      "[Valence (Fold 4)] Ep 22/70 | Loss=0.4275 | Val Acc=61.90% | F1=0.765 | AUC=0.649\n",
      "[Valence (Fold 4)] Ep 23/70 | Loss=0.4202 | Val Acc=61.90% | F1=0.765 | AUC=0.606\n",
      "[Valence (Fold 4)] Ep 24/70 | Loss=0.4151 | Val Acc=61.90% | F1=0.765 | AUC=0.606\n",
      "[Valence (Fold 4)] Ep 25/70 | Loss=0.4154 | Val Acc=61.90% | F1=0.765 | AUC=0.663\n",
      "[Valence (Fold 4)] Ep 26/70 | Loss=0.4140 | Val Acc=61.90% | F1=0.765 | AUC=0.639\n",
      "[Valence (Fold 4)] Ep 27/70 | Loss=0.4103 | Val Acc=61.90% | F1=0.765 | AUC=0.668\n",
      "[Valence (Fold 4)] Ep 28/70 | Loss=0.4098 | Val Acc=61.90% | F1=0.765 | AUC=0.671\n",
      "[Valence (Fold 4)] Ep 29/70 | Loss=0.4074 | Val Acc=61.90% | F1=0.765 | AUC=0.623\n",
      "[Valence (Fold 4)] Ep 30/70 | Loss=0.4057 | Val Acc=61.90% | F1=0.765 | AUC=0.637\n",
      "[Valence (Fold 4)] Ep 31/70 | Loss=0.4039 | Val Acc=61.90% | F1=0.765 | AUC=0.635\n",
      "â¹ [Valence (Fold 4)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 4)] TEST (best thr=0.100) | Acc=59.52% | F1=0.746 | AUC=0.452\n",
      "\n",
      "Fold 4 Results: Acc=0.5952, F1=0.7463, AUC=0.4518\n",
      "\n",
      "----- Valence Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 5) | epochs=70\n",
      "[Valence (Fold 5)] Ep 01/70 | Loss=0.5341 | Val Acc=60.98% | F1=0.758 | AUC=0.585\n",
      "[Valence (Fold 5)] Ep 02/70 | Loss=0.4841 | Val Acc=60.98% | F1=0.758 | AUC=0.565\n",
      "[Valence (Fold 5)] Ep 03/70 | Loss=0.4544 | Val Acc=60.98% | F1=0.758 | AUC=0.623\n",
      "[Valence (Fold 5)] Ep 04/70 | Loss=0.4411 | Val Acc=60.98% | F1=0.758 | AUC=0.663\n",
      "[Valence (Fold 5)] Ep 05/70 | Loss=0.4330 | Val Acc=60.98% | F1=0.758 | AUC=0.655\n",
      "[Valence (Fold 5)] Ep 06/70 | Loss=0.4262 | Val Acc=60.98% | F1=0.758 | AUC=0.665\n",
      "[Valence (Fold 5)] Ep 07/70 | Loss=0.4246 | Val Acc=60.98% | F1=0.758 | AUC=0.700\n",
      "[Valence (Fold 5)] Ep 08/70 | Loss=0.4199 | Val Acc=60.98% | F1=0.758 | AUC=0.725\n",
      "[Valence (Fold 5)] Ep 09/70 | Loss=0.4174 | Val Acc=60.98% | F1=0.758 | AUC=0.725\n",
      "[Valence (Fold 5)] Ep 10/70 | Loss=0.4163 | Val Acc=60.98% | F1=0.758 | AUC=0.720\n",
      "[Valence (Fold 5)] Ep 11/70 | Loss=0.4135 | Val Acc=60.98% | F1=0.758 | AUC=0.740\n",
      "[Valence (Fold 5)] Ep 12/70 | Loss=0.4122 | Val Acc=60.98% | F1=0.758 | AUC=0.718\n",
      "[Valence (Fold 5)] Ep 13/70 | Loss=0.4099 | Val Acc=60.98% | F1=0.758 | AUC=0.738\n",
      "[Valence (Fold 5)] Ep 14/70 | Loss=0.4084 | Val Acc=60.98% | F1=0.758 | AUC=0.740\n",
      "[Valence (Fold 5)] Ep 15/70 | Loss=0.4078 | Val Acc=60.98% | F1=0.758 | AUC=0.740\n",
      "[Valence (Fold 5)] Ep 16/70 | Loss=0.4071 | Val Acc=60.98% | F1=0.758 | AUC=0.740\n",
      "[Valence (Fold 5)] Ep 17/70 | Loss=0.4069 | Val Acc=60.98% | F1=0.758 | AUC=0.740\n",
      "[Valence (Fold 5)] Ep 18/70 | Loss=0.4066 | Val Acc=60.98% | F1=0.758 | AUC=0.738\n",
      "[Valence (Fold 5)] Ep 19/70 | Loss=0.4062 | Val Acc=60.98% | F1=0.758 | AUC=0.740\n",
      "[Valence (Fold 5)] Ep 20/70 | Loss=0.4062 | Val Acc=60.98% | F1=0.758 | AUC=0.740\n",
      "[Valence (Fold 5)] Ep 21/70 | Loss=0.4184 | Val Acc=60.98% | F1=0.758 | AUC=0.735\n",
      "[Valence (Fold 5)] Ep 22/70 | Loss=0.4266 | Val Acc=60.98% | F1=0.758 | AUC=0.728\n",
      "[Valence (Fold 5)] Ep 23/70 | Loss=0.4212 | Val Acc=60.98% | F1=0.758 | AUC=0.710\n",
      "[Valence (Fold 5)] Ep 24/70 | Loss=0.4179 | Val Acc=60.98% | F1=0.758 | AUC=0.732\n",
      "[Valence (Fold 5)] Ep 25/70 | Loss=0.4155 | Val Acc=60.98% | F1=0.758 | AUC=0.740\n",
      "[Valence (Fold 5)] Ep 26/70 | Loss=0.4136 | Val Acc=60.98% | F1=0.758 | AUC=0.732\n",
      "[Valence (Fold 5)] Ep 27/70 | Loss=0.4110 | Val Acc=60.98% | F1=0.758 | AUC=0.733\n",
      "[Valence (Fold 5)] Ep 28/70 | Loss=0.4090 | Val Acc=60.98% | F1=0.758 | AUC=0.748\n",
      "[Valence (Fold 5)] Ep 29/70 | Loss=0.4078 | Val Acc=60.98% | F1=0.758 | AUC=0.755\n",
      "[Valence (Fold 5)] Ep 30/70 | Loss=0.4068 | Val Acc=60.98% | F1=0.758 | AUC=0.760\n",
      "[Valence (Fold 5)] Ep 31/70 | Loss=0.4060 | Val Acc=60.98% | F1=0.758 | AUC=0.764\n",
      "â¹ [Valence (Fold 5)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 5)] TEST (best thr=0.100) | Acc=60.98% | F1=0.758 | AUC=0.512\n",
      "\n",
      "Fold 5 Results: Acc=0.6098, F1=0.7576, AUC=0.5125\n",
      "\n",
      "----- Valence Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 6) | epochs=70\n",
      "[Valence (Fold 6)] Ep 01/70 | Loss=0.5468 | Val Acc=60.98% | F1=0.758 | AUC=0.447\n",
      "[Valence (Fold 6)] Ep 02/70 | Loss=0.4980 | Val Acc=60.98% | F1=0.758 | AUC=0.385\n",
      "[Valence (Fold 6)] Ep 03/70 | Loss=0.4551 | Val Acc=60.98% | F1=0.758 | AUC=0.402\n",
      "[Valence (Fold 6)] Ep 04/70 | Loss=0.4462 | Val Acc=60.98% | F1=0.758 | AUC=0.453\n",
      "[Valence (Fold 6)] Ep 05/70 | Loss=0.4339 | Val Acc=60.98% | F1=0.758 | AUC=0.420\n",
      "[Valence (Fold 6)] Ep 06/70 | Loss=0.4292 | Val Acc=60.98% | F1=0.758 | AUC=0.387\n",
      "[Valence (Fold 6)] Ep 07/70 | Loss=0.4227 | Val Acc=60.98% | F1=0.758 | AUC=0.368\n",
      "[Valence (Fold 6)] Ep 08/70 | Loss=0.4186 | Val Acc=60.98% | F1=0.758 | AUC=0.380\n",
      "[Valence (Fold 6)] Ep 09/70 | Loss=0.4158 | Val Acc=60.98% | F1=0.758 | AUC=0.348\n",
      "[Valence (Fold 6)] Ep 10/70 | Loss=0.4132 | Val Acc=60.98% | F1=0.758 | AUC=0.360\n",
      "[Valence (Fold 6)] Ep 11/70 | Loss=0.4110 | Val Acc=60.98% | F1=0.758 | AUC=0.360\n",
      "[Valence (Fold 6)] Ep 12/70 | Loss=0.4113 | Val Acc=60.98% | F1=0.758 | AUC=0.337\n",
      "[Valence (Fold 6)] Ep 13/70 | Loss=0.4092 | Val Acc=60.98% | F1=0.758 | AUC=0.335\n",
      "[Valence (Fold 6)] Ep 14/70 | Loss=0.4083 | Val Acc=60.98% | F1=0.758 | AUC=0.340\n",
      "[Valence (Fold 6)] Ep 15/70 | Loss=0.4073 | Val Acc=60.98% | F1=0.758 | AUC=0.343\n",
      "[Valence (Fold 6)] Ep 16/70 | Loss=0.4065 | Val Acc=60.98% | F1=0.758 | AUC=0.330\n",
      "[Valence (Fold 6)] Ep 17/70 | Loss=0.4060 | Val Acc=60.98% | F1=0.758 | AUC=0.337\n",
      "[Valence (Fold 6)] Ep 18/70 | Loss=0.4059 | Val Acc=60.98% | F1=0.758 | AUC=0.338\n",
      "[Valence (Fold 6)] Ep 19/70 | Loss=0.4058 | Val Acc=60.98% | F1=0.758 | AUC=0.335\n",
      "[Valence (Fold 6)] Ep 20/70 | Loss=0.4056 | Val Acc=60.98% | F1=0.758 | AUC=0.335\n",
      "[Valence (Fold 6)] Ep 21/70 | Loss=0.4155 | Val Acc=60.98% | F1=0.758 | AUC=0.312\n",
      "[Valence (Fold 6)] Ep 22/70 | Loss=0.4274 | Val Acc=60.98% | F1=0.758 | AUC=0.390\n",
      "[Valence (Fold 6)] Ep 23/70 | Loss=0.4231 | Val Acc=60.98% | F1=0.758 | AUC=0.395\n",
      "[Valence (Fold 6)] Ep 24/70 | Loss=0.4168 | Val Acc=60.98% | F1=0.758 | AUC=0.375\n",
      "[Valence (Fold 6)] Ep 25/70 | Loss=0.4171 | Val Acc=60.98% | F1=0.758 | AUC=0.378\n",
      "[Valence (Fold 6)] Ep 26/70 | Loss=0.4142 | Val Acc=60.98% | F1=0.758 | AUC=0.350\n",
      "[Valence (Fold 6)] Ep 27/70 | Loss=0.4123 | Val Acc=60.98% | F1=0.758 | AUC=0.365\n",
      "[Valence (Fold 6)] Ep 28/70 | Loss=0.4108 | Val Acc=60.98% | F1=0.758 | AUC=0.357\n",
      "[Valence (Fold 6)] Ep 29/70 | Loss=0.4069 | Val Acc=60.98% | F1=0.758 | AUC=0.345\n",
      "[Valence (Fold 6)] Ep 30/70 | Loss=0.4053 | Val Acc=60.98% | F1=0.758 | AUC=0.343\n",
      "[Valence (Fold 6)] Ep 31/70 | Loss=0.4040 | Val Acc=60.98% | F1=0.758 | AUC=0.343\n",
      "â¹ [Valence (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 6)] TEST (best thr=0.650) | Acc=63.41% | F1=0.769 | AUC=0.718\n",
      "\n",
      "Fold 6 Results: Acc=0.6341, F1=0.7692, AUC=0.7175\n",
      "\n",
      "----- Valence Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 7) | epochs=70\n",
      "[Valence (Fold 7)] Ep 01/70 | Loss=0.5398 | Val Acc=60.98% | F1=0.758 | AUC=0.675\n",
      "[Valence (Fold 7)] Ep 02/70 | Loss=0.4857 | Val Acc=60.98% | F1=0.758 | AUC=0.643\n",
      "[Valence (Fold 7)] Ep 03/70 | Loss=0.4552 | Val Acc=60.98% | F1=0.758 | AUC=0.635\n",
      "[Valence (Fold 7)] Ep 04/70 | Loss=0.4394 | Val Acc=60.98% | F1=0.758 | AUC=0.655\n",
      "[Valence (Fold 7)] Ep 05/70 | Loss=0.4341 | Val Acc=60.98% | F1=0.758 | AUC=0.620\n",
      "[Valence (Fold 7)] Ep 06/70 | Loss=0.4277 | Val Acc=60.98% | F1=0.758 | AUC=0.643\n",
      "[Valence (Fold 7)] Ep 07/70 | Loss=0.4229 | Val Acc=60.98% | F1=0.758 | AUC=0.597\n",
      "[Valence (Fold 7)] Ep 08/70 | Loss=0.4180 | Val Acc=60.98% | F1=0.758 | AUC=0.593\n",
      "[Valence (Fold 7)] Ep 09/70 | Loss=0.4167 | Val Acc=60.98% | F1=0.758 | AUC=0.595\n",
      "[Valence (Fold 7)] Ep 10/70 | Loss=0.4150 | Val Acc=60.98% | F1=0.758 | AUC=0.603\n",
      "[Valence (Fold 7)] Ep 11/70 | Loss=0.4120 | Val Acc=60.98% | F1=0.758 | AUC=0.598\n",
      "[Valence (Fold 7)] Ep 12/70 | Loss=0.4101 | Val Acc=60.98% | F1=0.758 | AUC=0.590\n",
      "[Valence (Fold 7)] Ep 13/70 | Loss=0.4096 | Val Acc=60.98% | F1=0.758 | AUC=0.573\n",
      "[Valence (Fold 7)] Ep 14/70 | Loss=0.4087 | Val Acc=60.98% | F1=0.758 | AUC=0.595\n",
      "[Valence (Fold 7)] Ep 15/70 | Loss=0.4076 | Val Acc=60.98% | F1=0.758 | AUC=0.580\n",
      "[Valence (Fold 7)] Ep 16/70 | Loss=0.4070 | Val Acc=60.98% | F1=0.758 | AUC=0.583\n",
      "[Valence (Fold 7)] Ep 17/70 | Loss=0.4068 | Val Acc=60.98% | F1=0.758 | AUC=0.578\n",
      "[Valence (Fold 7)] Ep 18/70 | Loss=0.4062 | Val Acc=60.98% | F1=0.758 | AUC=0.585\n",
      "[Valence (Fold 7)] Ep 19/70 | Loss=0.4061 | Val Acc=60.98% | F1=0.758 | AUC=0.583\n",
      "[Valence (Fold 7)] Ep 20/70 | Loss=0.4059 | Val Acc=60.98% | F1=0.758 | AUC=0.585\n",
      "[Valence (Fold 7)] Ep 21/70 | Loss=0.4207 | Val Acc=60.98% | F1=0.758 | AUC=0.565\n",
      "[Valence (Fold 7)] Ep 22/70 | Loss=0.4277 | Val Acc=60.98% | F1=0.758 | AUC=0.595\n",
      "[Valence (Fold 7)] Ep 23/70 | Loss=0.4223 | Val Acc=60.98% | F1=0.758 | AUC=0.633\n",
      "[Valence (Fold 7)] Ep 24/70 | Loss=0.4170 | Val Acc=60.98% | F1=0.758 | AUC=0.562\n",
      "[Valence (Fold 7)] Ep 25/70 | Loss=0.4149 | Val Acc=60.98% | F1=0.758 | AUC=0.570\n",
      "[Valence (Fold 7)] Ep 26/70 | Loss=0.4135 | Val Acc=60.98% | F1=0.758 | AUC=0.558\n",
      "[Valence (Fold 7)] Ep 27/70 | Loss=0.4115 | Val Acc=60.98% | F1=0.758 | AUC=0.600\n",
      "[Valence (Fold 7)] Ep 28/70 | Loss=0.4116 | Val Acc=60.98% | F1=0.758 | AUC=0.568\n",
      "[Valence (Fold 7)] Ep 29/70 | Loss=0.4092 | Val Acc=60.98% | F1=0.758 | AUC=0.562\n",
      "[Valence (Fold 7)] Ep 30/70 | Loss=0.4068 | Val Acc=60.98% | F1=0.758 | AUC=0.567\n",
      "[Valence (Fold 7)] Ep 31/70 | Loss=0.4045 | Val Acc=60.98% | F1=0.758 | AUC=0.570\n",
      "â¹ [Valence (Fold 7)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 7)] TEST (best thr=0.100) | Acc=60.98% | F1=0.758 | AUC=0.472\n",
      "\n",
      "Fold 7 Results: Acc=0.6098, F1=0.7576, AUC=0.4725\n",
      "\n",
      "----- Valence Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 8) | epochs=70\n",
      "[Valence (Fold 8)] Ep 01/70 | Loss=0.5369 | Val Acc=60.98% | F1=0.758 | AUC=0.603\n",
      "[Valence (Fold 8)] Ep 02/70 | Loss=0.4882 | Val Acc=60.98% | F1=0.758 | AUC=0.537\n",
      "[Valence (Fold 8)] Ep 03/70 | Loss=0.4541 | Val Acc=60.98% | F1=0.758 | AUC=0.565\n",
      "[Valence (Fold 8)] Ep 04/70 | Loss=0.4406 | Val Acc=60.98% | F1=0.758 | AUC=0.578\n",
      "[Valence (Fold 8)] Ep 05/70 | Loss=0.4325 | Val Acc=60.98% | F1=0.758 | AUC=0.555\n",
      "[Valence (Fold 8)] Ep 06/70 | Loss=0.4260 | Val Acc=60.98% | F1=0.758 | AUC=0.555\n",
      "[Valence (Fold 8)] Ep 07/70 | Loss=0.4233 | Val Acc=60.98% | F1=0.758 | AUC=0.552\n",
      "[Valence (Fold 8)] Ep 08/70 | Loss=0.4186 | Val Acc=60.98% | F1=0.758 | AUC=0.560\n",
      "[Valence (Fold 8)] Ep 09/70 | Loss=0.4176 | Val Acc=60.98% | F1=0.758 | AUC=0.570\n",
      "[Valence (Fold 8)] Ep 10/70 | Loss=0.4154 | Val Acc=60.98% | F1=0.758 | AUC=0.550\n",
      "[Valence (Fold 8)] Ep 11/70 | Loss=0.4121 | Val Acc=60.98% | F1=0.758 | AUC=0.573\n",
      "[Valence (Fold 8)] Ep 12/70 | Loss=0.4107 | Val Acc=60.98% | F1=0.758 | AUC=0.562\n",
      "[Valence (Fold 8)] Ep 13/70 | Loss=0.4100 | Val Acc=60.98% | F1=0.758 | AUC=0.570\n",
      "[Valence (Fold 8)] Ep 14/70 | Loss=0.4083 | Val Acc=60.98% | F1=0.758 | AUC=0.583\n",
      "[Valence (Fold 8)] Ep 15/70 | Loss=0.4081 | Val Acc=60.98% | F1=0.758 | AUC=0.575\n",
      "[Valence (Fold 8)] Ep 16/70 | Loss=0.4071 | Val Acc=60.98% | F1=0.758 | AUC=0.578\n",
      "[Valence (Fold 8)] Ep 17/70 | Loss=0.4068 | Val Acc=60.98% | F1=0.758 | AUC=0.568\n",
      "[Valence (Fold 8)] Ep 18/70 | Loss=0.4065 | Val Acc=60.98% | F1=0.758 | AUC=0.570\n",
      "[Valence (Fold 8)] Ep 19/70 | Loss=0.4064 | Val Acc=60.98% | F1=0.758 | AUC=0.578\n",
      "[Valence (Fold 8)] Ep 20/70 | Loss=0.4062 | Val Acc=60.98% | F1=0.758 | AUC=0.565\n",
      "[Valence (Fold 8)] Ep 21/70 | Loss=0.4194 | Val Acc=60.98% | F1=0.758 | AUC=0.550\n",
      "[Valence (Fold 8)] Ep 22/70 | Loss=0.4276 | Val Acc=60.98% | F1=0.758 | AUC=0.550\n",
      "[Valence (Fold 8)] Ep 23/70 | Loss=0.4226 | Val Acc=60.98% | F1=0.758 | AUC=0.540\n",
      "[Valence (Fold 8)] Ep 24/70 | Loss=0.4171 | Val Acc=60.98% | F1=0.758 | AUC=0.547\n",
      "[Valence (Fold 8)] Ep 25/70 | Loss=0.4152 | Val Acc=60.98% | F1=0.758 | AUC=0.545\n",
      "[Valence (Fold 8)] Ep 26/70 | Loss=0.4146 | Val Acc=60.98% | F1=0.758 | AUC=0.573\n",
      "[Valence (Fold 8)] Ep 27/70 | Loss=0.4123 | Val Acc=60.98% | F1=0.758 | AUC=0.552\n",
      "[Valence (Fold 8)] Ep 28/70 | Loss=0.4089 | Val Acc=60.98% | F1=0.758 | AUC=0.570\n",
      "[Valence (Fold 8)] Ep 29/70 | Loss=0.4077 | Val Acc=60.98% | F1=0.758 | AUC=0.567\n",
      "[Valence (Fold 8)] Ep 30/70 | Loss=0.4061 | Val Acc=60.98% | F1=0.758 | AUC=0.573\n",
      "[Valence (Fold 8)] Ep 31/70 | Loss=0.4043 | Val Acc=60.98% | F1=0.758 | AUC=0.570\n",
      "â¹ [Valence (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 8)] TEST (best thr=0.100) | Acc=60.98% | F1=0.758 | AUC=0.578\n",
      "\n",
      "Fold 8 Results: Acc=0.6098, F1=0.7576, AUC=0.5775\n",
      "\n",
      "----- Valence Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 9) | epochs=70\n",
      "[Valence (Fold 9)] Ep 01/70 | Loss=0.5367 | Val Acc=60.98% | F1=0.758 | AUC=0.670\n",
      "[Valence (Fold 9)] Ep 02/70 | Loss=0.4862 | Val Acc=60.98% | F1=0.758 | AUC=0.693\n",
      "[Valence (Fold 9)] Ep 03/70 | Loss=0.4559 | Val Acc=60.98% | F1=0.758 | AUC=0.670\n",
      "[Valence (Fold 9)] Ep 04/70 | Loss=0.4422 | Val Acc=60.98% | F1=0.758 | AUC=0.635\n",
      "[Valence (Fold 9)] Ep 05/70 | Loss=0.4318 | Val Acc=60.98% | F1=0.758 | AUC=0.642\n",
      "[Valence (Fold 9)] Ep 06/70 | Loss=0.4276 | Val Acc=60.98% | F1=0.758 | AUC=0.627\n",
      "[Valence (Fold 9)] Ep 07/70 | Loss=0.4236 | Val Acc=60.98% | F1=0.758 | AUC=0.645\n",
      "[Valence (Fold 9)] Ep 08/70 | Loss=0.4211 | Val Acc=60.98% | F1=0.758 | AUC=0.640\n",
      "[Valence (Fold 9)] Ep 09/70 | Loss=0.4170 | Val Acc=60.98% | F1=0.758 | AUC=0.647\n",
      "[Valence (Fold 9)] Ep 10/70 | Loss=0.4159 | Val Acc=60.98% | F1=0.758 | AUC=0.640\n",
      "[Valence (Fold 9)] Ep 11/70 | Loss=0.4141 | Val Acc=60.98% | F1=0.758 | AUC=0.630\n",
      "[Valence (Fold 9)] Ep 12/70 | Loss=0.4118 | Val Acc=60.98% | F1=0.758 | AUC=0.630\n",
      "[Valence (Fold 9)] Ep 13/70 | Loss=0.4100 | Val Acc=60.98% | F1=0.758 | AUC=0.635\n",
      "[Valence (Fold 9)] Ep 14/70 | Loss=0.4089 | Val Acc=60.98% | F1=0.758 | AUC=0.637\n",
      "[Valence (Fold 9)] Ep 15/70 | Loss=0.4083 | Val Acc=60.98% | F1=0.758 | AUC=0.625\n",
      "[Valence (Fold 9)] Ep 16/70 | Loss=0.4075 | Val Acc=60.98% | F1=0.758 | AUC=0.627\n",
      "[Valence (Fold 9)] Ep 17/70 | Loss=0.4072 | Val Acc=60.98% | F1=0.758 | AUC=0.627\n",
      "[Valence (Fold 9)] Ep 18/70 | Loss=0.4068 | Val Acc=60.98% | F1=0.758 | AUC=0.627\n",
      "[Valence (Fold 9)] Ep 19/70 | Loss=0.4066 | Val Acc=60.98% | F1=0.758 | AUC=0.627\n",
      "[Valence (Fold 9)] Ep 20/70 | Loss=0.4067 | Val Acc=60.98% | F1=0.758 | AUC=0.627\n",
      "[Valence (Fold 9)] Ep 21/70 | Loss=0.4178 | Val Acc=60.98% | F1=0.758 | AUC=0.600\n",
      "[Valence (Fold 9)] Ep 22/70 | Loss=0.4257 | Val Acc=60.98% | F1=0.758 | AUC=0.605\n",
      "[Valence (Fold 9)] Ep 23/70 | Loss=0.4224 | Val Acc=60.98% | F1=0.758 | AUC=0.603\n",
      "[Valence (Fold 9)] Ep 24/70 | Loss=0.4184 | Val Acc=60.98% | F1=0.758 | AUC=0.603\n",
      "[Valence (Fold 9)] Ep 25/70 | Loss=0.4150 | Val Acc=60.98% | F1=0.758 | AUC=0.620\n",
      "[Valence (Fold 9)] Ep 26/70 | Loss=0.4147 | Val Acc=60.98% | F1=0.758 | AUC=0.605\n",
      "[Valence (Fold 9)] Ep 27/70 | Loss=0.4118 | Val Acc=60.98% | F1=0.758 | AUC=0.590\n",
      "[Valence (Fold 9)] Ep 28/70 | Loss=0.4096 | Val Acc=60.98% | F1=0.758 | AUC=0.593\n",
      "[Valence (Fold 9)] Ep 29/70 | Loss=0.4070 | Val Acc=60.98% | F1=0.758 | AUC=0.578\n",
      "[Valence (Fold 9)] Ep 30/70 | Loss=0.4071 | Val Acc=60.98% | F1=0.758 | AUC=0.580\n",
      "[Valence (Fold 9)] Ep 31/70 | Loss=0.4063 | Val Acc=60.98% | F1=0.758 | AUC=0.588\n",
      "â¹ [Valence (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 9)] TEST (best thr=0.100) | Acc=60.98% | F1=0.758 | AUC=0.333\n",
      "\n",
      "Fold 9 Results: Acc=0.6098, F1=0.7576, AUC=0.3325\n",
      "\n",
      "----- Valence Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Valence (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [129 203]\n",
      "  Val   class counts: [16 25]\n",
      "  Test  class counts: [16 25]\n",
      "[Valence (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.635\n",
      "\n",
      "ðŸš€ Training Valence (Fold 10) | epochs=70\n",
      "[Valence (Fold 10)] Ep 01/70 | Loss=0.5408 | Val Acc=60.98% | F1=0.758 | AUC=0.412\n",
      "[Valence (Fold 10)] Ep 02/70 | Loss=0.4898 | Val Acc=60.98% | F1=0.758 | AUC=0.448\n",
      "[Valence (Fold 10)] Ep 03/70 | Loss=0.4561 | Val Acc=60.98% | F1=0.758 | AUC=0.422\n",
      "[Valence (Fold 10)] Ep 04/70 | Loss=0.4410 | Val Acc=60.98% | F1=0.758 | AUC=0.445\n",
      "[Valence (Fold 10)] Ep 05/70 | Loss=0.4312 | Val Acc=60.98% | F1=0.758 | AUC=0.502\n",
      "[Valence (Fold 10)] Ep 06/70 | Loss=0.4259 | Val Acc=60.98% | F1=0.758 | AUC=0.472\n",
      "[Valence (Fold 10)] Ep 07/70 | Loss=0.4222 | Val Acc=60.98% | F1=0.758 | AUC=0.498\n",
      "[Valence (Fold 10)] Ep 08/70 | Loss=0.4200 | Val Acc=60.98% | F1=0.758 | AUC=0.497\n",
      "[Valence (Fold 10)] Ep 09/70 | Loss=0.4196 | Val Acc=60.98% | F1=0.758 | AUC=0.487\n",
      "[Valence (Fold 10)] Ep 10/70 | Loss=0.4158 | Val Acc=60.98% | F1=0.758 | AUC=0.497\n",
      "[Valence (Fold 10)] Ep 11/70 | Loss=0.4121 | Val Acc=60.98% | F1=0.758 | AUC=0.478\n",
      "[Valence (Fold 10)] Ep 12/70 | Loss=0.4105 | Val Acc=60.98% | F1=0.758 | AUC=0.487\n",
      "[Valence (Fold 10)] Ep 13/70 | Loss=0.4088 | Val Acc=60.98% | F1=0.758 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 14/70 | Loss=0.4084 | Val Acc=60.98% | F1=0.758 | AUC=0.472\n",
      "[Valence (Fold 10)] Ep 15/70 | Loss=0.4075 | Val Acc=60.98% | F1=0.758 | AUC=0.495\n",
      "[Valence (Fold 10)] Ep 16/70 | Loss=0.4069 | Val Acc=60.98% | F1=0.758 | AUC=0.497\n",
      "[Valence (Fold 10)] Ep 17/70 | Loss=0.4067 | Val Acc=60.98% | F1=0.758 | AUC=0.485\n",
      "[Valence (Fold 10)] Ep 18/70 | Loss=0.4062 | Val Acc=60.98% | F1=0.758 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 19/70 | Loss=0.4061 | Val Acc=60.98% | F1=0.758 | AUC=0.480\n",
      "[Valence (Fold 10)] Ep 20/70 | Loss=0.4062 | Val Acc=60.98% | F1=0.758 | AUC=0.497\n",
      "[Valence (Fold 10)] Ep 21/70 | Loss=0.4182 | Val Acc=60.98% | F1=0.758 | AUC=0.478\n",
      "[Valence (Fold 10)] Ep 22/70 | Loss=0.4271 | Val Acc=60.98% | F1=0.758 | AUC=0.508\n",
      "[Valence (Fold 10)] Ep 23/70 | Loss=0.4202 | Val Acc=60.98% | F1=0.758 | AUC=0.492\n",
      "[Valence (Fold 10)] Ep 24/70 | Loss=0.4168 | Val Acc=60.98% | F1=0.758 | AUC=0.490\n",
      "[Valence (Fold 10)] Ep 25/70 | Loss=0.4201 | Val Acc=60.98% | F1=0.758 | AUC=0.497\n",
      "[Valence (Fold 10)] Ep 26/70 | Loss=0.4140 | Val Acc=60.98% | F1=0.758 | AUC=0.535\n",
      "[Valence (Fold 10)] Ep 27/70 | Loss=0.4126 | Val Acc=60.98% | F1=0.758 | AUC=0.500\n",
      "[Valence (Fold 10)] Ep 28/70 | Loss=0.4109 | Val Acc=60.98% | F1=0.758 | AUC=0.522\n",
      "[Valence (Fold 10)] Ep 29/70 | Loss=0.4078 | Val Acc=60.98% | F1=0.758 | AUC=0.522\n",
      "[Valence (Fold 10)] Ep 30/70 | Loss=0.4058 | Val Acc=60.98% | F1=0.758 | AUC=0.512\n",
      "[Valence (Fold 10)] Ep 31/70 | Loss=0.4057 | Val Acc=60.98% | F1=0.758 | AUC=0.522\n",
      "â¹ [Valence (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Valence (Fold 10)] TEST (best thr=0.650) | Acc=63.41% | F1=0.769 | AUC=0.735\n",
      "\n",
      "Fold 10 Results: Acc=0.6341, F1=0.7692, AUC=0.7350\n",
      "\n",
      "===== FINAL Valence 10-FOLD RESULTS =====\n",
      "Accuracy: 0.6207 Â± 0.0190\n",
      "F1-score: 0.7632 Â± 0.0105\n",
      "AUC:      0.5602 Â± 0.1167\n",
      "\n",
      "########## Arousal: 10-fold STRATIFIED CV ##########\n",
      "Arousal global median (for stratification) = 3.0000\n",
      "Class counts: [114 300]\n",
      "\n",
      "----- Arousal Fold 1/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 1)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 1)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 1) | epochs=70\n",
      "[Arousal (Fold 1)] Ep 01/70 | Loss=0.3802 | Val Acc=71.43% | F1=0.833 | AUC=0.606\n",
      "[Arousal (Fold 1)] Ep 02/70 | Loss=0.3471 | Val Acc=71.43% | F1=0.833 | AUC=0.614\n",
      "[Arousal (Fold 1)] Ep 03/70 | Loss=0.3243 | Val Acc=71.43% | F1=0.833 | AUC=0.606\n",
      "[Arousal (Fold 1)] Ep 04/70 | Loss=0.3169 | Val Acc=71.43% | F1=0.833 | AUC=0.578\n",
      "[Arousal (Fold 1)] Ep 05/70 | Loss=0.3070 | Val Acc=71.43% | F1=0.833 | AUC=0.614\n",
      "[Arousal (Fold 1)] Ep 06/70 | Loss=0.3032 | Val Acc=71.43% | F1=0.833 | AUC=0.589\n",
      "[Arousal (Fold 1)] Ep 07/70 | Loss=0.3017 | Val Acc=71.43% | F1=0.833 | AUC=0.611\n",
      "[Arousal (Fold 1)] Ep 08/70 | Loss=0.3001 | Val Acc=71.43% | F1=0.833 | AUC=0.625\n",
      "[Arousal (Fold 1)] Ep 09/70 | Loss=0.2944 | Val Acc=71.43% | F1=0.833 | AUC=0.603\n",
      "[Arousal (Fold 1)] Ep 10/70 | Loss=0.2929 | Val Acc=71.43% | F1=0.833 | AUC=0.578\n",
      "[Arousal (Fold 1)] Ep 11/70 | Loss=0.2928 | Val Acc=71.43% | F1=0.833 | AUC=0.608\n",
      "[Arousal (Fold 1)] Ep 12/70 | Loss=0.2895 | Val Acc=71.43% | F1=0.833 | AUC=0.583\n",
      "[Arousal (Fold 1)] Ep 13/70 | Loss=0.2882 | Val Acc=71.43% | F1=0.833 | AUC=0.586\n",
      "[Arousal (Fold 1)] Ep 14/70 | Loss=0.2871 | Val Acc=71.43% | F1=0.833 | AUC=0.586\n",
      "[Arousal (Fold 1)] Ep 15/70 | Loss=0.2866 | Val Acc=71.43% | F1=0.833 | AUC=0.583\n",
      "[Arousal (Fold 1)] Ep 16/70 | Loss=0.2861 | Val Acc=71.43% | F1=0.833 | AUC=0.586\n",
      "[Arousal (Fold 1)] Ep 17/70 | Loss=0.2857 | Val Acc=71.43% | F1=0.833 | AUC=0.586\n",
      "[Arousal (Fold 1)] Ep 18/70 | Loss=0.2857 | Val Acc=71.43% | F1=0.833 | AUC=0.586\n",
      "[Arousal (Fold 1)] Ep 19/70 | Loss=0.2855 | Val Acc=71.43% | F1=0.833 | AUC=0.586\n",
      "[Arousal (Fold 1)] Ep 20/70 | Loss=0.2855 | Val Acc=71.43% | F1=0.833 | AUC=0.581\n",
      "[Arousal (Fold 1)] Ep 21/70 | Loss=0.2929 | Val Acc=71.43% | F1=0.833 | AUC=0.564\n",
      "[Arousal (Fold 1)] Ep 22/70 | Loss=0.3049 | Val Acc=71.43% | F1=0.833 | AUC=0.603\n",
      "[Arousal (Fold 1)] Ep 23/70 | Loss=0.2960 | Val Acc=71.43% | F1=0.833 | AUC=0.606\n",
      "[Arousal (Fold 1)] Ep 24/70 | Loss=0.2940 | Val Acc=71.43% | F1=0.833 | AUC=0.617\n",
      "[Arousal (Fold 1)] Ep 25/70 | Loss=0.2930 | Val Acc=71.43% | F1=0.833 | AUC=0.614\n",
      "[Arousal (Fold 1)] Ep 26/70 | Loss=0.2914 | Val Acc=71.43% | F1=0.833 | AUC=0.625\n",
      "[Arousal (Fold 1)] Ep 27/70 | Loss=0.2902 | Val Acc=71.43% | F1=0.833 | AUC=0.622\n",
      "[Arousal (Fold 1)] Ep 28/70 | Loss=0.2875 | Val Acc=71.43% | F1=0.833 | AUC=0.606\n",
      "[Arousal (Fold 1)] Ep 29/70 | Loss=0.2880 | Val Acc=71.43% | F1=0.833 | AUC=0.614\n",
      "[Arousal (Fold 1)] Ep 30/70 | Loss=0.2860 | Val Acc=71.43% | F1=0.833 | AUC=0.600\n",
      "[Arousal (Fold 1)] Ep 31/70 | Loss=0.2848 | Val Acc=71.43% | F1=0.833 | AUC=0.606\n",
      "â¹ [Arousal (Fold 1)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 1)] TEST (best thr=0.100) | Acc=71.43% | F1=0.833 | AUC=0.519\n",
      "\n",
      "Fold 1 Results: Acc=0.7143, F1=0.8333, AUC=0.5194\n",
      "\n",
      "----- Arousal Fold 2/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 2)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 2)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 2) | epochs=70\n",
      "[Arousal (Fold 2)] Ep 01/70 | Loss=0.3803 | Val Acc=71.43% | F1=0.833 | AUC=0.525\n",
      "[Arousal (Fold 2)] Ep 02/70 | Loss=0.3429 | Val Acc=71.43% | F1=0.833 | AUC=0.483\n",
      "[Arousal (Fold 2)] Ep 03/70 | Loss=0.3195 | Val Acc=71.43% | F1=0.833 | AUC=0.453\n",
      "[Arousal (Fold 2)] Ep 04/70 | Loss=0.3123 | Val Acc=71.43% | F1=0.833 | AUC=0.431\n",
      "[Arousal (Fold 2)] Ep 05/70 | Loss=0.3085 | Val Acc=71.43% | F1=0.833 | AUC=0.497\n",
      "[Arousal (Fold 2)] Ep 06/70 | Loss=0.2997 | Val Acc=71.43% | F1=0.833 | AUC=0.456\n",
      "[Arousal (Fold 2)] Ep 07/70 | Loss=0.2988 | Val Acc=71.43% | F1=0.833 | AUC=0.469\n",
      "[Arousal (Fold 2)] Ep 08/70 | Loss=0.2961 | Val Acc=71.43% | F1=0.833 | AUC=0.464\n",
      "[Arousal (Fold 2)] Ep 09/70 | Loss=0.2939 | Val Acc=71.43% | F1=0.833 | AUC=0.475\n",
      "[Arousal (Fold 2)] Ep 10/70 | Loss=0.2912 | Val Acc=71.43% | F1=0.833 | AUC=0.469\n",
      "[Arousal (Fold 2)] Ep 11/70 | Loss=0.2898 | Val Acc=71.43% | F1=0.833 | AUC=0.475\n",
      "[Arousal (Fold 2)] Ep 12/70 | Loss=0.2885 | Val Acc=71.43% | F1=0.833 | AUC=0.486\n",
      "[Arousal (Fold 2)] Ep 13/70 | Loss=0.2876 | Val Acc=71.43% | F1=0.833 | AUC=0.475\n",
      "[Arousal (Fold 2)] Ep 14/70 | Loss=0.2865 | Val Acc=71.43% | F1=0.833 | AUC=0.478\n",
      "[Arousal (Fold 2)] Ep 15/70 | Loss=0.2861 | Val Acc=71.43% | F1=0.833 | AUC=0.468\n",
      "[Arousal (Fold 2)] Ep 16/70 | Loss=0.2856 | Val Acc=71.43% | F1=0.833 | AUC=0.467\n",
      "[Arousal (Fold 2)] Ep 17/70 | Loss=0.2854 | Val Acc=71.43% | F1=0.833 | AUC=0.467\n",
      "[Arousal (Fold 2)] Ep 18/70 | Loss=0.2851 | Val Acc=71.43% | F1=0.833 | AUC=0.472\n",
      "[Arousal (Fold 2)] Ep 19/70 | Loss=0.2850 | Val Acc=71.43% | F1=0.833 | AUC=0.469\n",
      "[Arousal (Fold 2)] Ep 20/70 | Loss=0.2849 | Val Acc=71.43% | F1=0.833 | AUC=0.475\n",
      "[Arousal (Fold 2)] Ep 21/70 | Loss=0.2951 | Val Acc=71.43% | F1=0.833 | AUC=0.503\n",
      "[Arousal (Fold 2)] Ep 22/70 | Loss=0.3027 | Val Acc=71.43% | F1=0.833 | AUC=0.439\n",
      "[Arousal (Fold 2)] Ep 23/70 | Loss=0.2960 | Val Acc=71.43% | F1=0.833 | AUC=0.436\n",
      "[Arousal (Fold 2)] Ep 24/70 | Loss=0.2940 | Val Acc=71.43% | F1=0.833 | AUC=0.453\n",
      "[Arousal (Fold 2)] Ep 25/70 | Loss=0.2928 | Val Acc=71.43% | F1=0.833 | AUC=0.439\n",
      "[Arousal (Fold 2)] Ep 26/70 | Loss=0.2929 | Val Acc=71.43% | F1=0.833 | AUC=0.464\n",
      "[Arousal (Fold 2)] Ep 27/70 | Loss=0.2903 | Val Acc=71.43% | F1=0.833 | AUC=0.456\n",
      "[Arousal (Fold 2)] Ep 28/70 | Loss=0.2886 | Val Acc=71.43% | F1=0.833 | AUC=0.461\n",
      "[Arousal (Fold 2)] Ep 29/70 | Loss=0.2873 | Val Acc=71.43% | F1=0.833 | AUC=0.453\n",
      "[Arousal (Fold 2)] Ep 30/70 | Loss=0.2869 | Val Acc=71.43% | F1=0.833 | AUC=0.458\n",
      "[Arousal (Fold 2)] Ep 31/70 | Loss=0.2852 | Val Acc=71.43% | F1=0.833 | AUC=0.447\n",
      "â¹ [Arousal (Fold 2)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 2)] TEST (best thr=0.100) | Acc=71.43% | F1=0.833 | AUC=0.636\n",
      "\n",
      "Fold 2 Results: Acc=0.7143, F1=0.8333, AUC=0.6361\n",
      "\n",
      "----- Arousal Fold 3/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 3)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 3)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 3) | epochs=70\n",
      "[Arousal (Fold 3)] Ep 01/70 | Loss=0.3777 | Val Acc=71.43% | F1=0.833 | AUC=0.667\n",
      "[Arousal (Fold 3)] Ep 02/70 | Loss=0.3414 | Val Acc=71.43% | F1=0.833 | AUC=0.639\n",
      "[Arousal (Fold 3)] Ep 03/70 | Loss=0.3198 | Val Acc=71.43% | F1=0.833 | AUC=0.642\n",
      "[Arousal (Fold 3)] Ep 04/70 | Loss=0.3114 | Val Acc=71.43% | F1=0.833 | AUC=0.639\n",
      "[Arousal (Fold 3)] Ep 05/70 | Loss=0.3051 | Val Acc=71.43% | F1=0.833 | AUC=0.636\n",
      "[Arousal (Fold 3)] Ep 06/70 | Loss=0.3011 | Val Acc=71.43% | F1=0.833 | AUC=0.636\n",
      "[Arousal (Fold 3)] Ep 07/70 | Loss=0.2974 | Val Acc=71.43% | F1=0.833 | AUC=0.611\n",
      "[Arousal (Fold 3)] Ep 08/70 | Loss=0.2955 | Val Acc=71.43% | F1=0.833 | AUC=0.581\n",
      "[Arousal (Fold 3)] Ep 09/70 | Loss=0.2924 | Val Acc=71.43% | F1=0.833 | AUC=0.575\n",
      "[Arousal (Fold 3)] Ep 10/70 | Loss=0.2907 | Val Acc=71.43% | F1=0.833 | AUC=0.581\n",
      "[Arousal (Fold 3)] Ep 11/70 | Loss=0.2898 | Val Acc=71.43% | F1=0.833 | AUC=0.567\n",
      "[Arousal (Fold 3)] Ep 12/70 | Loss=0.2880 | Val Acc=71.43% | F1=0.833 | AUC=0.567\n",
      "[Arousal (Fold 3)] Ep 13/70 | Loss=0.2870 | Val Acc=71.43% | F1=0.833 | AUC=0.567\n",
      "[Arousal (Fold 3)] Ep 14/70 | Loss=0.2863 | Val Acc=71.43% | F1=0.833 | AUC=0.564\n",
      "[Arousal (Fold 3)] Ep 15/70 | Loss=0.2858 | Val Acc=71.43% | F1=0.833 | AUC=0.561\n",
      "[Arousal (Fold 3)] Ep 16/70 | Loss=0.2855 | Val Acc=71.43% | F1=0.833 | AUC=0.567\n",
      "[Arousal (Fold 3)] Ep 17/70 | Loss=0.2850 | Val Acc=71.43% | F1=0.833 | AUC=0.561\n",
      "[Arousal (Fold 3)] Ep 18/70 | Loss=0.2850 | Val Acc=71.43% | F1=0.833 | AUC=0.564\n",
      "[Arousal (Fold 3)] Ep 19/70 | Loss=0.2849 | Val Acc=71.43% | F1=0.833 | AUC=0.561\n",
      "[Arousal (Fold 3)] Ep 20/70 | Loss=0.2848 | Val Acc=71.43% | F1=0.833 | AUC=0.569\n",
      "[Arousal (Fold 3)] Ep 21/70 | Loss=0.2952 | Val Acc=71.43% | F1=0.833 | AUC=0.522\n",
      "[Arousal (Fold 3)] Ep 22/70 | Loss=0.3071 | Val Acc=71.43% | F1=0.833 | AUC=0.567\n",
      "[Arousal (Fold 3)] Ep 23/70 | Loss=0.2958 | Val Acc=71.43% | F1=0.833 | AUC=0.550\n",
      "[Arousal (Fold 3)] Ep 24/70 | Loss=0.2937 | Val Acc=71.43% | F1=0.833 | AUC=0.544\n",
      "[Arousal (Fold 3)] Ep 25/70 | Loss=0.2937 | Val Acc=71.43% | F1=0.833 | AUC=0.550\n",
      "[Arousal (Fold 3)] Ep 26/70 | Loss=0.2903 | Val Acc=71.43% | F1=0.833 | AUC=0.542\n",
      "[Arousal (Fold 3)] Ep 27/70 | Loss=0.2886 | Val Acc=71.43% | F1=0.833 | AUC=0.525\n",
      "[Arousal (Fold 3)] Ep 28/70 | Loss=0.2878 | Val Acc=71.43% | F1=0.833 | AUC=0.514\n",
      "[Arousal (Fold 3)] Ep 29/70 | Loss=0.2869 | Val Acc=71.43% | F1=0.833 | AUC=0.519\n",
      "[Arousal (Fold 3)] Ep 30/70 | Loss=0.2850 | Val Acc=71.43% | F1=0.833 | AUC=0.508\n",
      "[Arousal (Fold 3)] Ep 31/70 | Loss=0.2839 | Val Acc=71.43% | F1=0.833 | AUC=0.506\n",
      "â¹ [Arousal (Fold 3)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 3)] TEST (best thr=0.100) | Acc=71.43% | F1=0.833 | AUC=0.664\n",
      "\n",
      "Fold 3 Results: Acc=0.7143, F1=0.8333, AUC=0.6639\n",
      "\n",
      "----- Arousal Fold 4/10 -----\n",
      "Train size=330, Val size=42, Test size=42\n",
      "\n",
      "[Arousal (Fold 4)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 90 240]\n",
      "  Val   class counts: [12 30]\n",
      "  Test  class counts: [12 30]\n",
      "[Arousal (Fold 4)] pos_weight for BCEWithLogitsLoss = 0.375\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 4) | epochs=70\n",
      "[Arousal (Fold 4)] Ep 01/70 | Loss=0.3765 | Val Acc=71.43% | F1=0.833 | AUC=0.433\n",
      "[Arousal (Fold 4)] Ep 02/70 | Loss=0.3414 | Val Acc=71.43% | F1=0.833 | AUC=0.522\n",
      "[Arousal (Fold 4)] Ep 03/70 | Loss=0.3210 | Val Acc=71.43% | F1=0.833 | AUC=0.483\n",
      "[Arousal (Fold 4)] Ep 04/70 | Loss=0.3097 | Val Acc=71.43% | F1=0.833 | AUC=0.475\n",
      "[Arousal (Fold 4)] Ep 05/70 | Loss=0.3049 | Val Acc=71.43% | F1=0.833 | AUC=0.511\n",
      "[Arousal (Fold 4)] Ep 06/70 | Loss=0.3009 | Val Acc=71.43% | F1=0.833 | AUC=0.497\n",
      "[Arousal (Fold 4)] Ep 07/70 | Loss=0.2982 | Val Acc=71.43% | F1=0.833 | AUC=0.547\n",
      "[Arousal (Fold 4)] Ep 08/70 | Loss=0.2936 | Val Acc=71.43% | F1=0.833 | AUC=0.533\n",
      "[Arousal (Fold 4)] Ep 09/70 | Loss=0.2922 | Val Acc=71.43% | F1=0.833 | AUC=0.531\n",
      "[Arousal (Fold 4)] Ep 10/70 | Loss=0.2916 | Val Acc=71.43% | F1=0.833 | AUC=0.522\n",
      "[Arousal (Fold 4)] Ep 11/70 | Loss=0.2899 | Val Acc=71.43% | F1=0.833 | AUC=0.525\n",
      "[Arousal (Fold 4)] Ep 12/70 | Loss=0.2886 | Val Acc=71.43% | F1=0.833 | AUC=0.539\n",
      "[Arousal (Fold 4)] Ep 13/70 | Loss=0.2871 | Val Acc=71.43% | F1=0.833 | AUC=0.542\n",
      "[Arousal (Fold 4)] Ep 14/70 | Loss=0.2862 | Val Acc=71.43% | F1=0.833 | AUC=0.536\n",
      "[Arousal (Fold 4)] Ep 15/70 | Loss=0.2860 | Val Acc=71.43% | F1=0.833 | AUC=0.533\n",
      "[Arousal (Fold 4)] Ep 16/70 | Loss=0.2854 | Val Acc=71.43% | F1=0.833 | AUC=0.547\n",
      "[Arousal (Fold 4)] Ep 17/70 | Loss=0.2849 | Val Acc=71.43% | F1=0.833 | AUC=0.539\n",
      "[Arousal (Fold 4)] Ep 18/70 | Loss=0.2849 | Val Acc=71.43% | F1=0.833 | AUC=0.533\n",
      "[Arousal (Fold 4)] Ep 19/70 | Loss=0.2848 | Val Acc=71.43% | F1=0.833 | AUC=0.528\n",
      "[Arousal (Fold 4)] Ep 20/70 | Loss=0.2847 | Val Acc=71.43% | F1=0.833 | AUC=0.542\n",
      "[Arousal (Fold 4)] Ep 21/70 | Loss=0.2951 | Val Acc=71.43% | F1=0.833 | AUC=0.531\n",
      "[Arousal (Fold 4)] Ep 22/70 | Loss=0.3017 | Val Acc=71.43% | F1=0.833 | AUC=0.539\n",
      "[Arousal (Fold 4)] Ep 23/70 | Loss=0.2962 | Val Acc=71.43% | F1=0.833 | AUC=0.500\n",
      "[Arousal (Fold 4)] Ep 24/70 | Loss=0.2935 | Val Acc=71.43% | F1=0.833 | AUC=0.506\n",
      "[Arousal (Fold 4)] Ep 25/70 | Loss=0.2935 | Val Acc=71.43% | F1=0.833 | AUC=0.508\n",
      "[Arousal (Fold 4)] Ep 26/70 | Loss=0.2915 | Val Acc=71.43% | F1=0.833 | AUC=0.511\n",
      "[Arousal (Fold 4)] Ep 27/70 | Loss=0.2889 | Val Acc=71.43% | F1=0.833 | AUC=0.542\n",
      "[Arousal (Fold 4)] Ep 28/70 | Loss=0.2887 | Val Acc=71.43% | F1=0.833 | AUC=0.528\n",
      "[Arousal (Fold 4)] Ep 29/70 | Loss=0.2858 | Val Acc=71.43% | F1=0.833 | AUC=0.531\n",
      "[Arousal (Fold 4)] Ep 30/70 | Loss=0.2849 | Val Acc=71.43% | F1=0.833 | AUC=0.536\n",
      "[Arousal (Fold 4)] Ep 31/70 | Loss=0.2842 | Val Acc=71.43% | F1=0.833 | AUC=0.536\n",
      "â¹ [Arousal (Fold 4)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 4)] TEST (best thr=0.650) | Acc=73.81% | F1=0.845 | AUC=0.658\n",
      "\n",
      "Fold 4 Results: Acc=0.7381, F1=0.8451, AUC=0.6583\n",
      "\n",
      "----- Arousal Fold 5/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 5)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 5)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 5) | epochs=70\n",
      "[Arousal (Fold 5)] Ep 01/70 | Loss=0.3889 | Val Acc=73.17% | F1=0.845 | AUC=0.530\n",
      "[Arousal (Fold 5)] Ep 02/70 | Loss=0.3585 | Val Acc=73.17% | F1=0.845 | AUC=0.533\n",
      "[Arousal (Fold 5)] Ep 03/70 | Loss=0.3310 | Val Acc=73.17% | F1=0.845 | AUC=0.503\n",
      "[Arousal (Fold 5)] Ep 04/70 | Loss=0.3201 | Val Acc=73.17% | F1=0.845 | AUC=0.497\n",
      "[Arousal (Fold 5)] Ep 05/70 | Loss=0.3113 | Val Acc=73.17% | F1=0.845 | AUC=0.539\n",
      "[Arousal (Fold 5)] Ep 06/70 | Loss=0.3075 | Val Acc=73.17% | F1=0.845 | AUC=0.506\n",
      "[Arousal (Fold 5)] Ep 07/70 | Loss=0.3041 | Val Acc=73.17% | F1=0.845 | AUC=0.506\n",
      "[Arousal (Fold 5)] Ep 08/70 | Loss=0.2994 | Val Acc=73.17% | F1=0.845 | AUC=0.485\n",
      "[Arousal (Fold 5)] Ep 09/70 | Loss=0.2979 | Val Acc=73.17% | F1=0.845 | AUC=0.521\n",
      "[Arousal (Fold 5)] Ep 10/70 | Loss=0.2964 | Val Acc=73.17% | F1=0.845 | AUC=0.491\n",
      "[Arousal (Fold 5)] Ep 11/70 | Loss=0.2937 | Val Acc=73.17% | F1=0.845 | AUC=0.491\n",
      "[Arousal (Fold 5)] Ep 12/70 | Loss=0.2928 | Val Acc=73.17% | F1=0.845 | AUC=0.503\n",
      "[Arousal (Fold 5)] Ep 13/70 | Loss=0.2920 | Val Acc=73.17% | F1=0.845 | AUC=0.482\n",
      "[Arousal (Fold 5)] Ep 14/70 | Loss=0.2927 | Val Acc=73.17% | F1=0.845 | AUC=0.494\n",
      "[Arousal (Fold 5)] Ep 15/70 | Loss=0.2907 | Val Acc=73.17% | F1=0.845 | AUC=0.488\n",
      "[Arousal (Fold 5)] Ep 16/70 | Loss=0.2899 | Val Acc=73.17% | F1=0.845 | AUC=0.488\n",
      "[Arousal (Fold 5)] Ep 17/70 | Loss=0.2896 | Val Acc=73.17% | F1=0.845 | AUC=0.491\n",
      "[Arousal (Fold 5)] Ep 18/70 | Loss=0.2896 | Val Acc=73.17% | F1=0.845 | AUC=0.482\n",
      "[Arousal (Fold 5)] Ep 19/70 | Loss=0.2894 | Val Acc=73.17% | F1=0.845 | AUC=0.497\n",
      "[Arousal (Fold 5)] Ep 20/70 | Loss=0.2894 | Val Acc=73.17% | F1=0.845 | AUC=0.497\n",
      "[Arousal (Fold 5)] Ep 21/70 | Loss=0.2967 | Val Acc=73.17% | F1=0.845 | AUC=0.458\n",
      "[Arousal (Fold 5)] Ep 22/70 | Loss=0.3113 | Val Acc=73.17% | F1=0.845 | AUC=0.479\n",
      "[Arousal (Fold 5)] Ep 23/70 | Loss=0.3001 | Val Acc=73.17% | F1=0.845 | AUC=0.467\n",
      "[Arousal (Fold 5)] Ep 24/70 | Loss=0.2987 | Val Acc=73.17% | F1=0.845 | AUC=0.476\n",
      "[Arousal (Fold 5)] Ep 25/70 | Loss=0.2975 | Val Acc=73.17% | F1=0.845 | AUC=0.497\n",
      "[Arousal (Fold 5)] Ep 26/70 | Loss=0.2961 | Val Acc=73.17% | F1=0.845 | AUC=0.488\n",
      "[Arousal (Fold 5)] Ep 27/70 | Loss=0.2943 | Val Acc=73.17% | F1=0.845 | AUC=0.500\n",
      "[Arousal (Fold 5)] Ep 28/70 | Loss=0.2927 | Val Acc=73.17% | F1=0.845 | AUC=0.494\n",
      "[Arousal (Fold 5)] Ep 29/70 | Loss=0.2909 | Val Acc=73.17% | F1=0.845 | AUC=0.500\n",
      "[Arousal (Fold 5)] Ep 30/70 | Loss=0.2899 | Val Acc=73.17% | F1=0.845 | AUC=0.509\n",
      "[Arousal (Fold 5)] Ep 31/70 | Loss=0.2883 | Val Acc=73.17% | F1=0.845 | AUC=0.497\n",
      "â¹ [Arousal (Fold 5)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 5)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.609\n",
      "\n",
      "Fold 5 Results: Acc=0.7317, F1=0.8451, AUC=0.6091\n",
      "\n",
      "----- Arousal Fold 6/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 6)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 6)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 6) | epochs=70\n",
      "[Arousal (Fold 6)] Ep 01/70 | Loss=0.3809 | Val Acc=73.17% | F1=0.845 | AUC=0.742\n",
      "[Arousal (Fold 6)] Ep 02/70 | Loss=0.3461 | Val Acc=73.17% | F1=0.845 | AUC=0.712\n",
      "[Arousal (Fold 6)] Ep 03/70 | Loss=0.3285 | Val Acc=73.17% | F1=0.845 | AUC=0.652\n",
      "[Arousal (Fold 6)] Ep 04/70 | Loss=0.3176 | Val Acc=73.17% | F1=0.845 | AUC=0.658\n",
      "[Arousal (Fold 6)] Ep 05/70 | Loss=0.3091 | Val Acc=73.17% | F1=0.845 | AUC=0.648\n",
      "[Arousal (Fold 6)] Ep 06/70 | Loss=0.3060 | Val Acc=73.17% | F1=0.845 | AUC=0.655\n",
      "[Arousal (Fold 6)] Ep 07/70 | Loss=0.3029 | Val Acc=73.17% | F1=0.845 | AUC=0.655\n",
      "[Arousal (Fold 6)] Ep 08/70 | Loss=0.3009 | Val Acc=73.17% | F1=0.845 | AUC=0.648\n",
      "[Arousal (Fold 6)] Ep 09/70 | Loss=0.2984 | Val Acc=73.17% | F1=0.845 | AUC=0.630\n",
      "[Arousal (Fold 6)] Ep 10/70 | Loss=0.2963 | Val Acc=73.17% | F1=0.845 | AUC=0.636\n",
      "[Arousal (Fold 6)] Ep 11/70 | Loss=0.2941 | Val Acc=73.17% | F1=0.845 | AUC=0.630\n",
      "[Arousal (Fold 6)] Ep 12/70 | Loss=0.2927 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "[Arousal (Fold 6)] Ep 13/70 | Loss=0.2919 | Val Acc=73.17% | F1=0.845 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 14/70 | Loss=0.2913 | Val Acc=73.17% | F1=0.845 | AUC=0.603\n",
      "[Arousal (Fold 6)] Ep 15/70 | Loss=0.2907 | Val Acc=73.17% | F1=0.845 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 16/70 | Loss=0.2903 | Val Acc=73.17% | F1=0.845 | AUC=0.609\n",
      "[Arousal (Fold 6)] Ep 17/70 | Loss=0.2900 | Val Acc=73.17% | F1=0.845 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 18/70 | Loss=0.2897 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "[Arousal (Fold 6)] Ep 19/70 | Loss=0.2895 | Val Acc=73.17% | F1=0.845 | AUC=0.612\n",
      "[Arousal (Fold 6)] Ep 20/70 | Loss=0.2895 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "[Arousal (Fold 6)] Ep 21/70 | Loss=0.3031 | Val Acc=73.17% | F1=0.845 | AUC=0.630\n",
      "[Arousal (Fold 6)] Ep 22/70 | Loss=0.3077 | Val Acc=73.17% | F1=0.845 | AUC=0.667\n",
      "[Arousal (Fold 6)] Ep 23/70 | Loss=0.3024 | Val Acc=73.17% | F1=0.845 | AUC=0.645\n",
      "[Arousal (Fold 6)] Ep 24/70 | Loss=0.2990 | Val Acc=73.17% | F1=0.845 | AUC=0.633\n",
      "[Arousal (Fold 6)] Ep 25/70 | Loss=0.2967 | Val Acc=73.17% | F1=0.845 | AUC=0.633\n",
      "[Arousal (Fold 6)] Ep 26/70 | Loss=0.2960 | Val Acc=73.17% | F1=0.845 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 27/70 | Loss=0.2951 | Val Acc=73.17% | F1=0.845 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 28/70 | Loss=0.2939 | Val Acc=73.17% | F1=0.845 | AUC=0.624\n",
      "[Arousal (Fold 6)] Ep 29/70 | Loss=0.2911 | Val Acc=73.17% | F1=0.845 | AUC=0.618\n",
      "[Arousal (Fold 6)] Ep 30/70 | Loss=0.2896 | Val Acc=73.17% | F1=0.845 | AUC=0.609\n",
      "[Arousal (Fold 6)] Ep 31/70 | Loss=0.2893 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "â¹ [Arousal (Fold 6)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 6)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.564\n",
      "\n",
      "Fold 6 Results: Acc=0.7317, F1=0.8451, AUC=0.5636\n",
      "\n",
      "----- Arousal Fold 7/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 7)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 7)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 7) | epochs=70\n",
      "[Arousal (Fold 7)] Ep 01/70 | Loss=0.3825 | Val Acc=73.17% | F1=0.845 | AUC=0.573\n",
      "[Arousal (Fold 7)] Ep 02/70 | Loss=0.3486 | Val Acc=73.17% | F1=0.845 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 03/70 | Loss=0.3298 | Val Acc=73.17% | F1=0.845 | AUC=0.603\n",
      "[Arousal (Fold 7)] Ep 04/70 | Loss=0.3197 | Val Acc=73.17% | F1=0.845 | AUC=0.642\n",
      "[Arousal (Fold 7)] Ep 05/70 | Loss=0.3121 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "[Arousal (Fold 7)] Ep 06/70 | Loss=0.3082 | Val Acc=73.17% | F1=0.845 | AUC=0.642\n",
      "[Arousal (Fold 7)] Ep 07/70 | Loss=0.3026 | Val Acc=73.17% | F1=0.845 | AUC=0.676\n",
      "[Arousal (Fold 7)] Ep 08/70 | Loss=0.3005 | Val Acc=73.17% | F1=0.845 | AUC=0.664\n",
      "[Arousal (Fold 7)] Ep 09/70 | Loss=0.2976 | Val Acc=73.17% | F1=0.845 | AUC=0.670\n",
      "[Arousal (Fold 7)] Ep 10/70 | Loss=0.2966 | Val Acc=73.17% | F1=0.845 | AUC=0.682\n",
      "[Arousal (Fold 7)] Ep 11/70 | Loss=0.2953 | Val Acc=73.17% | F1=0.845 | AUC=0.676\n",
      "[Arousal (Fold 7)] Ep 12/70 | Loss=0.2936 | Val Acc=73.17% | F1=0.845 | AUC=0.691\n",
      "[Arousal (Fold 7)] Ep 13/70 | Loss=0.2929 | Val Acc=73.17% | F1=0.845 | AUC=0.682\n",
      "[Arousal (Fold 7)] Ep 14/70 | Loss=0.2920 | Val Acc=73.17% | F1=0.845 | AUC=0.685\n",
      "[Arousal (Fold 7)] Ep 15/70 | Loss=0.2914 | Val Acc=73.17% | F1=0.845 | AUC=0.682\n",
      "[Arousal (Fold 7)] Ep 16/70 | Loss=0.2909 | Val Acc=73.17% | F1=0.845 | AUC=0.682\n",
      "[Arousal (Fold 7)] Ep 17/70 | Loss=0.2905 | Val Acc=73.17% | F1=0.845 | AUC=0.676\n",
      "[Arousal (Fold 7)] Ep 18/70 | Loss=0.2902 | Val Acc=73.17% | F1=0.845 | AUC=0.682\n",
      "[Arousal (Fold 7)] Ep 19/70 | Loss=0.2901 | Val Acc=73.17% | F1=0.845 | AUC=0.682\n",
      "[Arousal (Fold 7)] Ep 20/70 | Loss=0.2900 | Val Acc=73.17% | F1=0.845 | AUC=0.682\n",
      "[Arousal (Fold 7)] Ep 21/70 | Loss=0.3019 | Val Acc=73.17% | F1=0.845 | AUC=0.685\n",
      "[Arousal (Fold 7)] Ep 22/70 | Loss=0.3087 | Val Acc=73.17% | F1=0.845 | AUC=0.664\n",
      "[Arousal (Fold 7)] Ep 23/70 | Loss=0.3024 | Val Acc=73.17% | F1=0.845 | AUC=0.648\n",
      "[Arousal (Fold 7)] Ep 24/70 | Loss=0.3003 | Val Acc=73.17% | F1=0.845 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 25/70 | Loss=0.2992 | Val Acc=73.17% | F1=0.845 | AUC=0.652\n",
      "[Arousal (Fold 7)] Ep 26/70 | Loss=0.2958 | Val Acc=73.17% | F1=0.845 | AUC=0.664\n",
      "[Arousal (Fold 7)] Ep 27/70 | Loss=0.2975 | Val Acc=73.17% | F1=0.845 | AUC=0.694\n",
      "[Arousal (Fold 7)] Ep 28/70 | Loss=0.2948 | Val Acc=73.17% | F1=0.845 | AUC=0.697\n",
      "[Arousal (Fold 7)] Ep 29/70 | Loss=0.2921 | Val Acc=73.17% | F1=0.845 | AUC=0.700\n",
      "[Arousal (Fold 7)] Ep 30/70 | Loss=0.2900 | Val Acc=73.17% | F1=0.845 | AUC=0.697\n",
      "[Arousal (Fold 7)] Ep 31/70 | Loss=0.2903 | Val Acc=73.17% | F1=0.845 | AUC=0.727\n",
      "â¹ [Arousal (Fold 7)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 7)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.652\n",
      "\n",
      "Fold 7 Results: Acc=0.7317, F1=0.8451, AUC=0.6515\n",
      "\n",
      "----- Arousal Fold 8/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 8)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 8)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 8) | epochs=70\n",
      "[Arousal (Fold 8)] Ep 01/70 | Loss=0.3826 | Val Acc=73.17% | F1=0.845 | AUC=0.442\n",
      "[Arousal (Fold 8)] Ep 02/70 | Loss=0.3439 | Val Acc=73.17% | F1=0.845 | AUC=0.591\n",
      "[Arousal (Fold 8)] Ep 03/70 | Loss=0.3254 | Val Acc=73.17% | F1=0.845 | AUC=0.518\n",
      "[Arousal (Fold 8)] Ep 04/70 | Loss=0.3165 | Val Acc=73.17% | F1=0.845 | AUC=0.533\n",
      "[Arousal (Fold 8)] Ep 05/70 | Loss=0.3112 | Val Acc=73.17% | F1=0.845 | AUC=0.555\n",
      "[Arousal (Fold 8)] Ep 06/70 | Loss=0.3053 | Val Acc=73.17% | F1=0.845 | AUC=0.570\n",
      "[Arousal (Fold 8)] Ep 07/70 | Loss=0.3029 | Val Acc=73.17% | F1=0.845 | AUC=0.558\n",
      "[Arousal (Fold 8)] Ep 08/70 | Loss=0.2996 | Val Acc=73.17% | F1=0.845 | AUC=0.542\n",
      "[Arousal (Fold 8)] Ep 09/70 | Loss=0.2975 | Val Acc=73.17% | F1=0.845 | AUC=0.576\n",
      "[Arousal (Fold 8)] Ep 10/70 | Loss=0.2950 | Val Acc=73.17% | F1=0.845 | AUC=0.582\n",
      "[Arousal (Fold 8)] Ep 11/70 | Loss=0.2939 | Val Acc=73.17% | F1=0.845 | AUC=0.564\n",
      "[Arousal (Fold 8)] Ep 12/70 | Loss=0.2928 | Val Acc=73.17% | F1=0.845 | AUC=0.555\n",
      "[Arousal (Fold 8)] Ep 13/70 | Loss=0.2917 | Val Acc=73.17% | F1=0.845 | AUC=0.555\n",
      "[Arousal (Fold 8)] Ep 14/70 | Loss=0.2910 | Val Acc=73.17% | F1=0.845 | AUC=0.558\n",
      "[Arousal (Fold 8)] Ep 15/70 | Loss=0.2907 | Val Acc=73.17% | F1=0.845 | AUC=0.552\n",
      "[Arousal (Fold 8)] Ep 16/70 | Loss=0.2902 | Val Acc=73.17% | F1=0.845 | AUC=0.552\n",
      "[Arousal (Fold 8)] Ep 17/70 | Loss=0.2899 | Val Acc=73.17% | F1=0.845 | AUC=0.548\n",
      "[Arousal (Fold 8)] Ep 18/70 | Loss=0.2896 | Val Acc=73.17% | F1=0.845 | AUC=0.555\n",
      "[Arousal (Fold 8)] Ep 19/70 | Loss=0.2896 | Val Acc=73.17% | F1=0.845 | AUC=0.555\n",
      "[Arousal (Fold 8)] Ep 20/70 | Loss=0.2894 | Val Acc=73.17% | F1=0.845 | AUC=0.552\n",
      "[Arousal (Fold 8)] Ep 21/70 | Loss=0.3011 | Val Acc=73.17% | F1=0.845 | AUC=0.545\n",
      "[Arousal (Fold 8)] Ep 22/70 | Loss=0.3090 | Val Acc=73.17% | F1=0.845 | AUC=0.567\n",
      "[Arousal (Fold 8)] Ep 23/70 | Loss=0.3010 | Val Acc=73.17% | F1=0.845 | AUC=0.585\n",
      "[Arousal (Fold 8)] Ep 24/70 | Loss=0.3010 | Val Acc=73.17% | F1=0.845 | AUC=0.579\n",
      "[Arousal (Fold 8)] Ep 25/70 | Loss=0.2983 | Val Acc=73.17% | F1=0.845 | AUC=0.588\n",
      "[Arousal (Fold 8)] Ep 26/70 | Loss=0.2964 | Val Acc=73.17% | F1=0.845 | AUC=0.573\n",
      "[Arousal (Fold 8)] Ep 27/70 | Loss=0.2947 | Val Acc=73.17% | F1=0.845 | AUC=0.591\n",
      "[Arousal (Fold 8)] Ep 28/70 | Loss=0.2920 | Val Acc=73.17% | F1=0.845 | AUC=0.612\n",
      "[Arousal (Fold 8)] Ep 29/70 | Loss=0.2908 | Val Acc=73.17% | F1=0.845 | AUC=0.573\n",
      "[Arousal (Fold 8)] Ep 30/70 | Loss=0.2901 | Val Acc=73.17% | F1=0.845 | AUC=0.570\n",
      "[Arousal (Fold 8)] Ep 31/70 | Loss=0.2897 | Val Acc=73.17% | F1=0.845 | AUC=0.573\n",
      "â¹ [Arousal (Fold 8)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 8)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.561\n",
      "\n",
      "Fold 8 Results: Acc=0.7317, F1=0.8451, AUC=0.5606\n",
      "\n",
      "----- Arousal Fold 9/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 9)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 9)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 9) | epochs=70\n",
      "[Arousal (Fold 9)] Ep 01/70 | Loss=0.3896 | Val Acc=73.17% | F1=0.845 | AUC=0.630\n",
      "[Arousal (Fold 9)] Ep 02/70 | Loss=0.3581 | Val Acc=73.17% | F1=0.845 | AUC=0.639\n",
      "[Arousal (Fold 9)] Ep 03/70 | Loss=0.3335 | Val Acc=73.17% | F1=0.845 | AUC=0.618\n",
      "[Arousal (Fold 9)] Ep 04/70 | Loss=0.3177 | Val Acc=73.17% | F1=0.845 | AUC=0.585\n",
      "[Arousal (Fold 9)] Ep 05/70 | Loss=0.3118 | Val Acc=73.17% | F1=0.845 | AUC=0.612\n",
      "[Arousal (Fold 9)] Ep 06/70 | Loss=0.3082 | Val Acc=73.17% | F1=0.845 | AUC=0.585\n",
      "[Arousal (Fold 9)] Ep 07/70 | Loss=0.3054 | Val Acc=73.17% | F1=0.845 | AUC=0.630\n",
      "[Arousal (Fold 9)] Ep 08/70 | Loss=0.3033 | Val Acc=73.17% | F1=0.845 | AUC=0.591\n",
      "[Arousal (Fold 9)] Ep 09/70 | Loss=0.2984 | Val Acc=73.17% | F1=0.845 | AUC=0.594\n",
      "[Arousal (Fold 9)] Ep 10/70 | Loss=0.2955 | Val Acc=73.17% | F1=0.845 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 11/70 | Loss=0.2945 | Val Acc=73.17% | F1=0.845 | AUC=0.588\n",
      "[Arousal (Fold 9)] Ep 12/70 | Loss=0.2930 | Val Acc=73.17% | F1=0.845 | AUC=0.594\n",
      "[Arousal (Fold 9)] Ep 13/70 | Loss=0.2915 | Val Acc=73.17% | F1=0.845 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 14/70 | Loss=0.2912 | Val Acc=73.17% | F1=0.845 | AUC=0.609\n",
      "[Arousal (Fold 9)] Ep 15/70 | Loss=0.2905 | Val Acc=73.17% | F1=0.845 | AUC=0.600\n",
      "[Arousal (Fold 9)] Ep 16/70 | Loss=0.2902 | Val Acc=73.17% | F1=0.845 | AUC=0.606\n",
      "[Arousal (Fold 9)] Ep 17/70 | Loss=0.2898 | Val Acc=73.17% | F1=0.845 | AUC=0.591\n",
      "[Arousal (Fold 9)] Ep 18/70 | Loss=0.2895 | Val Acc=73.17% | F1=0.845 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 19/70 | Loss=0.2895 | Val Acc=73.17% | F1=0.845 | AUC=0.600\n",
      "[Arousal (Fold 9)] Ep 20/70 | Loss=0.2895 | Val Acc=73.17% | F1=0.845 | AUC=0.618\n",
      "[Arousal (Fold 9)] Ep 21/70 | Loss=0.3002 | Val Acc=73.17% | F1=0.845 | AUC=0.582\n",
      "[Arousal (Fold 9)] Ep 22/70 | Loss=0.3084 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "[Arousal (Fold 9)] Ep 23/70 | Loss=0.3008 | Val Acc=73.17% | F1=0.845 | AUC=0.594\n",
      "[Arousal (Fold 9)] Ep 24/70 | Loss=0.3006 | Val Acc=73.17% | F1=0.845 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 25/70 | Loss=0.2994 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "[Arousal (Fold 9)] Ep 26/70 | Loss=0.2950 | Val Acc=73.17% | F1=0.845 | AUC=0.609\n",
      "[Arousal (Fold 9)] Ep 27/70 | Loss=0.2935 | Val Acc=73.17% | F1=0.845 | AUC=0.591\n",
      "[Arousal (Fold 9)] Ep 28/70 | Loss=0.2928 | Val Acc=73.17% | F1=0.845 | AUC=0.597\n",
      "[Arousal (Fold 9)] Ep 29/70 | Loss=0.2916 | Val Acc=73.17% | F1=0.845 | AUC=0.600\n",
      "[Arousal (Fold 9)] Ep 30/70 | Loss=0.2902 | Val Acc=73.17% | F1=0.845 | AUC=0.600\n",
      "[Arousal (Fold 9)] Ep 31/70 | Loss=0.2888 | Val Acc=73.17% | F1=0.845 | AUC=0.594\n",
      "â¹ [Arousal (Fold 9)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 9)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.388\n",
      "\n",
      "Fold 9 Results: Acc=0.7317, F1=0.8451, AUC=0.3879\n",
      "\n",
      "----- Arousal Fold 10/10 -----\n",
      "Train size=332, Val size=41, Test size=41\n",
      "\n",
      "[Arousal (Fold 10)] TRAIN median threshold = 3.0000\n",
      "  Train class counts: [ 92 240]\n",
      "  Val   class counts: [11 30]\n",
      "  Test  class counts: [11 30]\n",
      "[Arousal (Fold 10)] pos_weight for BCEWithLogitsLoss = 0.383\n",
      "\n",
      "ðŸš€ Training Arousal (Fold 10) | epochs=70\n",
      "[Arousal (Fold 10)] Ep 01/70 | Loss=0.3851 | Val Acc=73.17% | F1=0.845 | AUC=0.648\n",
      "[Arousal (Fold 10)] Ep 02/70 | Loss=0.3501 | Val Acc=73.17% | F1=0.845 | AUC=0.570\n",
      "[Arousal (Fold 10)] Ep 03/70 | Loss=0.3294 | Val Acc=73.17% | F1=0.845 | AUC=0.621\n",
      "[Arousal (Fold 10)] Ep 04/70 | Loss=0.3184 | Val Acc=73.17% | F1=0.845 | AUC=0.603\n",
      "[Arousal (Fold 10)] Ep 05/70 | Loss=0.3122 | Val Acc=73.17% | F1=0.845 | AUC=0.615\n",
      "[Arousal (Fold 10)] Ep 06/70 | Loss=0.3085 | Val Acc=73.17% | F1=0.845 | AUC=0.606\n",
      "[Arousal (Fold 10)] Ep 07/70 | Loss=0.3033 | Val Acc=73.17% | F1=0.845 | AUC=0.594\n",
      "[Arousal (Fold 10)] Ep 08/70 | Loss=0.3000 | Val Acc=73.17% | F1=0.845 | AUC=0.570\n",
      "[Arousal (Fold 10)] Ep 09/70 | Loss=0.2983 | Val Acc=73.17% | F1=0.845 | AUC=0.567\n",
      "[Arousal (Fold 10)] Ep 10/70 | Loss=0.2967 | Val Acc=73.17% | F1=0.845 | AUC=0.576\n",
      "[Arousal (Fold 10)] Ep 11/70 | Loss=0.2956 | Val Acc=73.17% | F1=0.845 | AUC=0.564\n",
      "[Arousal (Fold 10)] Ep 12/70 | Loss=0.2927 | Val Acc=73.17% | F1=0.845 | AUC=0.570\n",
      "[Arousal (Fold 10)] Ep 13/70 | Loss=0.2918 | Val Acc=73.17% | F1=0.845 | AUC=0.564\n",
      "[Arousal (Fold 10)] Ep 14/70 | Loss=0.2913 | Val Acc=73.17% | F1=0.845 | AUC=0.561\n",
      "[Arousal (Fold 10)] Ep 15/70 | Loss=0.2912 | Val Acc=73.17% | F1=0.845 | AUC=0.555\n",
      "[Arousal (Fold 10)] Ep 16/70 | Loss=0.2904 | Val Acc=73.17% | F1=0.845 | AUC=0.555\n",
      "[Arousal (Fold 10)] Ep 17/70 | Loss=0.2900 | Val Acc=73.17% | F1=0.845 | AUC=0.545\n",
      "[Arousal (Fold 10)] Ep 18/70 | Loss=0.2898 | Val Acc=73.17% | F1=0.845 | AUC=0.548\n",
      "[Arousal (Fold 10)] Ep 19/70 | Loss=0.2898 | Val Acc=73.17% | F1=0.845 | AUC=0.545\n",
      "[Arousal (Fold 10)] Ep 20/70 | Loss=0.2897 | Val Acc=73.17% | F1=0.845 | AUC=0.552\n",
      "[Arousal (Fold 10)] Ep 21/70 | Loss=0.2999 | Val Acc=73.17% | F1=0.845 | AUC=0.573\n",
      "[Arousal (Fold 10)] Ep 22/70 | Loss=0.3110 | Val Acc=73.17% | F1=0.845 | AUC=0.570\n",
      "[Arousal (Fold 10)] Ep 23/70 | Loss=0.3008 | Val Acc=73.17% | F1=0.845 | AUC=0.579\n",
      "[Arousal (Fold 10)] Ep 24/70 | Loss=0.3004 | Val Acc=73.17% | F1=0.845 | AUC=0.567\n",
      "[Arousal (Fold 10)] Ep 25/70 | Loss=0.2991 | Val Acc=73.17% | F1=0.845 | AUC=0.585\n",
      "[Arousal (Fold 10)] Ep 26/70 | Loss=0.2954 | Val Acc=73.17% | F1=0.845 | AUC=0.576\n",
      "[Arousal (Fold 10)] Ep 27/70 | Loss=0.2937 | Val Acc=73.17% | F1=0.845 | AUC=0.552\n",
      "[Arousal (Fold 10)] Ep 28/70 | Loss=0.2926 | Val Acc=73.17% | F1=0.845 | AUC=0.561\n",
      "[Arousal (Fold 10)] Ep 29/70 | Loss=0.2910 | Val Acc=73.17% | F1=0.845 | AUC=0.558\n",
      "[Arousal (Fold 10)] Ep 30/70 | Loss=0.2900 | Val Acc=73.17% | F1=0.845 | AUC=0.552\n",
      "[Arousal (Fold 10)] Ep 31/70 | Loss=0.2890 | Val Acc=73.17% | F1=0.845 | AUC=0.552\n",
      "â¹ [Arousal (Fold 10)] Early stopping at epoch 31\n",
      "\n",
      "ðŸ”š [Arousal (Fold 10)] TEST (best thr=0.100) | Acc=73.17% | F1=0.845 | AUC=0.464\n",
      "\n",
      "Fold 10 Results: Acc=0.7317, F1=0.8451, AUC=0.4636\n",
      "\n",
      "===== FINAL Arousal 10-FOLD RESULTS =====\n",
      "Accuracy: 0.7271 Â± 0.0086\n",
      "F1-score: 0.8415 Â± 0.0054\n",
      "AUC:      0.5714 Â± 0.0874\n",
      "\n",
      "===== OVERALL SUMMARY =====\n",
      "Valence Acc: mean=0.6207, std=0.0190\n",
      "Aroual Acc:  mean=0.7271, std=0.0086\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# CapsNet on READY-MADE SPECTROGRAMS\n",
    "# (No spectrogram conversion in this file)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------- DEVICE -------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# ----------------- Dataset wrapper -------------------\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.asarray(X, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).float()\n",
    "\n",
    "# ----------------- Augmentations -------------------\n",
    "def spec_augment(\n",
    "    X,\n",
    "    time_mask_width=4,\n",
    "    n_time_masks=2,\n",
    "    freq_mask_width=6,\n",
    "    n_freq_masks=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    X: [N, C, F, T] spectrograms\n",
    "    Applies random time & frequency masks (SpecAugment style).\n",
    "    \"\"\"\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "\n",
    "    for i in range(N):\n",
    "        # Time masks\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T:\n",
    "                continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "\n",
    "        # Frequency masks\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F:\n",
    "                continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# ----------------- Capsule helpers -------------------\n",
    "def squash(s, dim=-1, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Squash activation from Sabour et al.\n",
    "    s: tensor of shape (..., dim)\n",
    "    \"\"\"\n",
    "    squared_norm = (s ** 2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1.0 + squared_norm)\n",
    "    v = scale * s / torch.sqrt(squared_norm + epsilon)\n",
    "    return v\n",
    "\n",
    "class PrimaryCapsules(nn.Module):\n",
    "    \"\"\"\n",
    "    Primary capsules implemented with Conv2d and reshape.\n",
    "    Produces N_primary_capsules per sample with capsule_dim each.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_capsules=8, capsule_dim=16, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        # out_channels = num_capsules * capsule_dim\n",
    "        self.conv = nn.Conv2d(in_channels, num_capsules * capsule_dim, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.capsule_dim = capsule_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, F, T]\n",
    "        out = self.conv(x)  # [B, num_capsules*capsule_dim, F', T']\n",
    "        B, _, Fp, Tp = out.shape\n",
    "        out = out.view(B, self.num_capsules, self.capsule_dim, Fp * Tp)\n",
    "        out = out.permute(0, 3, 1, 2).contiguous()  # [B, positions, num_capsules, capsule_dim]\n",
    "        # flatten positions and capsule channels -> primary capsules\n",
    "        B, P, NC, CD = out.shape\n",
    "        out = out.view(B, P * NC, CD)  # [B, num_primary_caps, capsule_dim]\n",
    "        out = squash(out, dim=-1)\n",
    "        return out  # [B, num_primary_caps, capsule_dim]\n",
    "\n",
    "class DigitCapsules(nn.Module):\n",
    "    \"\"\"\n",
    "    DigitCaps with dynamic routing.\n",
    "    Inputs:\n",
    "      - num_input_caps: number of input capsules per sample (e.g., primary capsules)\n",
    "      - in_dim: dimension of each input capsule\n",
    "      - num_output_caps: classes (e.g., 2)\n",
    "      - out_dim: dimension of each output capsule\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input_caps, in_dim, num_output_caps=2, out_dim=16, routing_iters=3):\n",
    "        super().__init__()\n",
    "        self.num_input_caps = num_input_caps\n",
    "        self.in_dim = in_dim\n",
    "        self.num_output_caps = num_output_caps\n",
    "        self.out_dim = out_dim\n",
    "        self.routing_iters = routing_iters\n",
    "\n",
    "        # Weight matrix to map input capsules to each output capsule\n",
    "        # shape: [1, num_input_caps, num_output_caps, out_dim, in_dim]\n",
    "        self.W = nn.Parameter(\n",
    "            0.01 * torch.randn(1, num_input_caps, num_output_caps, out_dim, in_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, num_input_caps, in_dim]\n",
    "        B = x.size(0)\n",
    "        # expand x to [B, num_input_caps, num_output_caps, in_dim, 1]\n",
    "        x = x.unsqueeze(2).unsqueeze(-1)  # [B, num_input_caps, 1, in_dim, 1]\n",
    "        W = self.W.repeat(B, 1, 1, 1, 1)   # [B, num_input_caps, num_output_caps, out_dim, in_dim]\n",
    "        # u_hat: [B, num_input_caps, num_output_caps, out_dim, 1]\n",
    "        u_hat = torch.matmul(W, x)  # linear transform\n",
    "        u_hat = u_hat.squeeze(-1)   # [B, num_input_caps, num_output_caps, out_dim]\n",
    "\n",
    "        # routing logits\n",
    "        b = torch.zeros(B, self.num_input_caps, self.num_output_caps, device=u_hat.device)\n",
    "\n",
    "        for r in range(self.routing_iters):\n",
    "            # coupling coefficients via softmax over output capsules\n",
    "            c = torch.softmax(b, dim=2)  # [B, num_input_caps, num_output_caps]\n",
    "            c = c.unsqueeze(-1)  # [B, num_input_caps, num_output_caps, 1]\n",
    "            # s = sum_i c_i * u_hat_i  -> [B, num_output_caps, out_dim]\n",
    "            s = (c * u_hat).sum(dim=1)  # sum over input capsules\n",
    "            v = squash(s, dim=-1)       # [B, num_output_caps, out_dim]\n",
    "            if r < self.routing_iters - 1:\n",
    "                # update b: add agreement\n",
    "                # compute agreement a_{ij} = u_hat_{ij} dot v_j\n",
    "                v_expanded = v.unsqueeze(1)  # [B,1,num_output_caps,out_dim]\n",
    "                a = (u_hat * v_expanded).sum(dim=-1)  # [B, num_input_caps, num_output_caps]\n",
    "                b = b + a\n",
    "        return v  # [B, num_output_caps, out_dim]\n",
    "\n",
    "\n",
    "# ----------------- CapsNet model -------------------\n",
    "class EEG_CapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Small CapsNet for spectrogram inputs [B, C, F, T].\n",
    "    Uses a conv backbone -> PrimaryCaps -> DigitCaps -> class logits (lengths).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=14, n_classes=2, routing_iters=3, primary_caps_num=8, primary_caps_dim=16, digit_caps_dim=16):\n",
    "        super().__init__()\n",
    "        # conv backbone\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),  # reduce spatial dims\n",
    "        )\n",
    "\n",
    "        # after conv, produce primary capsules\n",
    "        # we will use a primary conv that outputs (primary_caps_num * primary_caps_dim) channels\n",
    "        self.primary = PrimaryCapsules(in_channels=128,\n",
    "                                       num_capsules=primary_caps_num,\n",
    "                                       capsule_dim=primary_caps_dim,\n",
    "                                       kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # num_primary_caps depends on spatial positions after primary conv\n",
    "        # We'll compute this lazily in forward by inspecting output shape\n",
    "        self.digit_caps_module_configured = False\n",
    "        self.n_classes = n_classes\n",
    "        self.routing_iters = routing_iters\n",
    "        self.digit_caps_dim = digit_caps_dim\n",
    "\n",
    "        # small fc head (optional) to map capsule outputs to a single logit per class (we will use lengths)\n",
    "        # but we won't add extra FC layers here to keep capsule spirit.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, F, T]\n",
    "        B = x.size(0)\n",
    "        out = self.conv_block(x)  # [B, 128, F', T']\n",
    "        primary = self.primary(out)  # [B, num_primary_caps, primary_caps_dim]\n",
    "        num_primary_caps = primary.size(1)\n",
    "        in_dim = primary.size(2)\n",
    "\n",
    "        # configure DigitCaps lazily (so we can compute num_primary_caps)\n",
    "        if not self.digit_caps_module_configured:\n",
    "            self.digitcaps = DigitCapsules(num_input_caps=num_primary_caps,\n",
    "                                           in_dim=in_dim,\n",
    "                                           num_output_caps=self.n_classes,\n",
    "                                           out_dim=self.digit_caps_dim,\n",
    "                                           routing_iters=self.routing_iters).to(x.device)\n",
    "            self.digit_caps_module_configured = True\n",
    "\n",
    "        digit_caps = self.digitcaps(primary)  # [B, n_classes, digit_caps_dim]\n",
    "\n",
    "        # lengths (L2 norm) per class\n",
    "        lengths = torch.norm(digit_caps, dim=-1)  # [B, n_classes]\n",
    "\n",
    "        # For binary tasks we will treat the second capsule (index 1) as \"positive class\" logit.\n",
    "        # We will return the lengths (un-sigmoided) so they can be used with BCEWithLogitsLoss.\n",
    "        return lengths  # [B, n_classes]\n",
    "\n",
    "# ----------------- Train ONE fold for one emotion -------------------\n",
    "def train_one_fold_emotion(\n",
    "    X,\n",
    "    y_cont,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    emotion_name=\"Valence\",\n",
    "    epochs=70,\n",
    "    base_seed=42,\n",
    "    batch_size=16,\n",
    "):\n",
    "    # 1) Binarize labels w.r.t. TRAIN median\n",
    "    y_train_cont = y_cont[train_idx]\n",
    "    thr = np.median(y_train_cont)\n",
    "\n",
    "    y_train_bin = (y_train_cont >= thr).astype(float)\n",
    "    y_val_bin   = (y_cont[val_idx]  >= thr).astype(float)\n",
    "    y_test_bin  = (y_cont[test_idx] >= thr).astype(float)\n",
    "\n",
    "    print(f\"\\n[{emotion_name}] TRAIN median threshold = {thr:.4f}\")\n",
    "    print(\"  Train class counts:\", np.bincount(y_train_bin.astype(int)))\n",
    "    print(\"  Val   class counts:\", np.bincount(y_val_bin.astype(int)))\n",
    "    print(\"  Test  class counts:\", np.bincount(y_test_bin.astype(int)))\n",
    "\n",
    "    # 2) Standardize features using TRAIN only\n",
    "    X = np.nan_to_num(X)\n",
    "    X_train = X[train_idx]\n",
    "    X_val   = X[val_idx]\n",
    "    X_test  = X[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(len(train_idx), -1)\n",
    "    scaler.fit(X_train_flat)\n",
    "\n",
    "    def transform_block(X_block):\n",
    "        flat = X_block.reshape(X_block.shape[0], -1)\n",
    "        flat_scaled = scaler.transform(flat)\n",
    "        return flat_scaled.reshape(X_block.shape)\n",
    "\n",
    "    X_train_scaled = transform_block(X_train)\n",
    "    X_val_scaled   = transform_block(X_val)\n",
    "    X_test_scaled  = transform_block(X_test)\n",
    "\n",
    "    # 3) Strong augmentation: SpecAugment + noise\n",
    "    X_train_sa    = spec_augment(X_train_scaled)\n",
    "    X_train_noise = add_noise(X_train_scaled, std=0.03)\n",
    "\n",
    "    X_train_aug = np.concatenate(\n",
    "        [X_train_scaled, X_train_sa, X_train_noise],\n",
    "        axis=0,\n",
    "    )\n",
    "    y_train_aug = np.concatenate(\n",
    "        [y_train_bin, y_train_bin, y_train_bin],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # 4) DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        EEGDataset(X_train_aug, y_train_aug),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        EEGDataset(X_val_scaled, y_val_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        EEGDataset(X_test_scaled, y_test_bin),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # 5) Model + optimizer + CLASS-WEIGHTED loss\n",
    "    torch.manual_seed(base_seed)\n",
    "    random.seed(base_seed)\n",
    "    np.random.seed(base_seed)\n",
    "\n",
    "    n_channels = X.shape[1]\n",
    "    model = EEG_CapsNet(in_channels=n_channels, n_classes=2, routing_iters=3,\n",
    "                        primary_caps_num=8, primary_caps_dim=16, digit_caps_dim=16).to(DEVICE)\n",
    "\n",
    "    # pos_weight for BCEWithLogitsLoss: weight for positive class\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts) > 0 else 1\n",
    "    pos = class_counts[1] if len(class_counts) > 1 else 1\n",
    "    pos_weight_val = float(neg) / float(pos) if pos > 0 else 1.0\n",
    "    # BCEWithLogitsLoss for multi-label expects pos_weight per class: here size [1] (only positive class)\n",
    "    # We'll compute loss on the second capsule (class index 1). To keep API simple, compute BCEWithLogitsLoss on the positive-class logits only.\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    print(f\"[{emotion_name}] pos_weight for BCEWithLogitsLoss = {pos_weight_val:.3f}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)  # expects logits\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "    patience = 18\n",
    "    counter = 0\n",
    "\n",
    "    print(f\"\\nðŸš€ Training {emotion_name} | epochs={epochs}\")\n",
    "    for ep in range(1, epochs + 1):\n",
    "        # ---- train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            lengths = model(xb)  # [B, 2]\n",
    "            # positive-class logits: lengths[:,1]\n",
    "            logits_pos = lengths[:, 1].unsqueeze(1)  # [B,1]\n",
    "            loss = criterion(logits_pos, yb.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            n_samples += xb.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss = total_loss / max(1, n_samples)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        y_true_val, y_prob_val = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                lengths = model(xb)  # [B,2]\n",
    "                probs_pos = torch.sigmoid(lengths[:, 1]).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy())\n",
    "                y_prob_val.extend(probs_pos)\n",
    "\n",
    "        y_true_val = np.array(y_true_val)\n",
    "        y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        val_f1  = f1_score(y_true_val, y_pred_val)\n",
    "        try:\n",
    "            val_auc = roc_auc_score(y_true_val, y_prob_val)\n",
    "        except:\n",
    "            val_auc = float(\"nan\")\n",
    "\n",
    "        print(\n",
    "            f\"[{emotion_name}] Ep {ep:02d}/{epochs} | \"\n",
    "            f\"Loss={train_loss:.4f} | Val Acc={val_acc*100:.2f}% | \"\n",
    "            f\"F1={val_f1:.3f} | AUC={val_auc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # early stopping on val accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience:\n",
    "                print(f\"â¹ [{emotion_name}] Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    # ---- load best & evaluate on TEST (with best threshold) ----\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    y_true_test, y_prob_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            lengths = model(xb)\n",
    "            probs_pos = torch.sigmoid(lengths[:, 1]).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy())\n",
    "            y_prob_test.extend(probs_pos)\n",
    "\n",
    "    y_true_test = np.array(y_true_test)\n",
    "    y_prob_test = np.array(y_prob_test)\n",
    "\n",
    "    # search threshold for best TEST accuracy\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for thr_ in np.linspace(0.1, 0.9, 17):\n",
    "        yp = (y_prob_test >= thr_).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc = acc_thr\n",
    "            best_thr = thr_\n",
    "\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1  = f1_score(y_true_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nðŸ”š [{emotion_name}] TEST (best thr={best_thr:.3f}) \"\n",
    "        f\"| Acc={test_acc*100:.2f}% | F1={test_f1:.3f} | AUC={test_auc:.3f}\\n\"\n",
    "    )\n",
    "\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# ----------------- 10-fold CV driver -------------------\n",
    "def run_10fold_cv_emotion(X, y_cont, emotion_name=\"Valence\", epochs=70, base_seed=42):\n",
    "    \"\"\"\n",
    "    10-fold stratified CV:\n",
    "      - Outer fold chooses TEST.\n",
    "      - Remaining data is split into TRAIN / VAL\n",
    "        so that VAL size â‰ˆ TEST size.\n",
    "    \"\"\"\n",
    "    print(f\"\\n########## {emotion_name}: 10-fold STRATIFIED CV ##########\")\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "\n",
    "    # For stratification: global median threshold\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    print(f\"{emotion_name} global median (for stratification) = {global_thr:.4f}\")\n",
    "    print(\"Class counts:\", np.bincount(y_bin_global.astype(int)))\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=10,\n",
    "        shuffle=True,\n",
    "        random_state=base_seed,\n",
    "    )\n",
    "\n",
    "    fold_accs, fold_f1s, fold_aucs = [], [], []\n",
    "\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(\n",
    "        skf.split(np.arange(len(y_cont)), y_bin_global),\n",
    "        start=1\n",
    "    ):\n",
    "        # Split trainval into train & val with val size ~= test size\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "\n",
    "        tv_train_idx, tv_val_idx = train_test_split(\n",
    "            trainval_idx,\n",
    "            test_size=val_frac,\n",
    "            random_state=base_seed + fold,\n",
    "            shuffle=True,\n",
    "            stratify=y_trainval_bin,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n----- {emotion_name} Fold {fold}/10 -----\")\n",
    "        print(\n",
    "            f\"Train size={len(tv_train_idx)}, \"\n",
    "            f\"Val size={len(tv_val_idx)}, \"\n",
    "            f\"Test size={len(test_idx)}\"\n",
    "        )\n",
    "\n",
    "        acc, f1, auc = train_one_fold_emotion(\n",
    "            X,\n",
    "            y_cont,\n",
    "            tv_train_idx,\n",
    "            tv_val_idx,\n",
    "            test_idx,\n",
    "            emotion_name=f\"{emotion_name} (Fold {fold})\",\n",
    "            epochs=epochs,\n",
    "            base_seed=base_seed + fold,\n",
    "            batch_size=16,\n",
    "        )\n",
    "\n",
    "        fold_accs.append(acc)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_aucs.append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold} Results: \"\n",
    "            f\"Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\"\n",
    "        )\n",
    "\n",
    "    fold_accs = np.array(fold_accs)\n",
    "    fold_f1s  = np.array(fold_f1s)\n",
    "    fold_aucs = np.array(fold_aucs)\n",
    "\n",
    "    print(f\"\\n===== FINAL {emotion_name} 10-FOLD RESULTS =====\")\n",
    "    print(f\"Accuracy: {fold_accs.mean():.4f} Â± {fold_accs.std():.4f}\")\n",
    "    print(f\"F1-score: {fold_f1s.mean():.4f} Â± {fold_f1s.std():.4f}\")\n",
    "    print(f\"AUC:      {fold_aucs.mean():.4f} Â± {fold_aucs.std():.4f}\")\n",
    "\n",
    "    return fold_accs, fold_f1s, fold_aucs\n",
    "\n",
    "# ----------------- MAIN (assumes X, y_val, y_aro already created) -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure you've already run the cell that does:\n",
    "    # X, y_val, y_aro, groups, channels = load_dreamer_and_build_spectrograms(...)\n",
    "    print(\"\\nâœ… Spectrogram dataset:\", X.shape)\n",
    "\n",
    "    # Valence\n",
    "    val_accs, val_f1s, val_aucs = run_10fold_cv_emotion(\n",
    "        X, y_val, emotion_name=\"Valence\", epochs=70, base_seed=42\n",
    "    )\n",
    "\n",
    "    # Arousal\n",
    "    aro_accs, aro_f1s, aro_aucs = run_10fold_cv_emotion(\n",
    "        X, y_aro, emotion_name=\"Arousal\", epochs=70, base_seed=142\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== OVERALL SUMMARY =====\")\n",
    "    print(f\"Valence Acc: mean={val_accs.mean():.4f}, std={val_accs.std():.4f}\")\n",
    "    print(f\"Aroual Acc:  mean={aro_accs.mean():.4f}, std={aro_accs.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1408e948-e47c-4213-82bb-c55f1e2cce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL MODELS COMBINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c0c7c14-06e6-4dd2-acaf-dd96815af130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "==================================================\n",
      "Model: cnn_bilstm\n",
      "\n",
      "Running 10-fold CV for model: cnn_bilstm\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6429, F1=0.7170, AUC=0.6106\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.5714, F1=0.6667, AUC=0.5144\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.6190, F1=0.6800, AUC=0.5745\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5476, F1=0.6545, AUC=0.4941\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6341, F1=0.6939, AUC=0.5900\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7805, F1=0.8085, AUC=0.7625\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.5610, F1=0.6250, AUC=0.4975\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6341, F1=0.7541, AUC=0.5400\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.5854, F1=0.7018, AUC=0.4900\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.6098, F1=0.7037, AUC=0.5800\n",
      "\n",
      "Running 10-fold CV for model: cnn_bilstm\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6905, F1=0.8116, AUC=0.5528\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.7143, F1=0.8235, AUC=0.5694\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7143, F1=0.8286, AUC=0.5667\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.7143, F1=0.8235, AUC=0.5250\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.7805, F1=0.8696, AUC=0.6182\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7073, F1=0.8235, AUC=0.5394\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.6098, F1=0.7576, AUC=0.3182\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.7073, F1=0.8182, AUC=0.6394\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.7073, F1=0.8182, AUC=0.5758\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8254, AUC=0.7273\n",
      "\n",
      "==================================================\n",
      "Model: cnn_only\n",
      "\n",
      "Running 10-fold CV for model: cnn_only\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6429, F1=0.7273, AUC=0.6635\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6429, F1=0.7059, AUC=0.6274\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.4524, F1=0.5106, AUC=0.4014\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5238, F1=0.6000, AUC=0.5447\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.7317, F1=0.7755, AUC=0.6800\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7317, F1=0.8070, AUC=0.7100\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.5122, F1=0.5652, AUC=0.5025\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6098, F1=0.7037, AUC=0.5375\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.5610, F1=0.6786, AUC=0.4650\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.6341, F1=0.7170, AUC=0.6375\n",
      "\n",
      "Running 10-fold CV for model: cnn_only\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6667, F1=0.7742, AUC=0.5000\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.5714, F1=0.6897, AUC=0.4361\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7381, F1=0.8358, AUC=0.5556\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.6190, F1=0.7241, AUC=0.5417\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6098, F1=0.7419, AUC=0.5818\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7561, F1=0.8276, AUC=0.6455\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.5854, F1=0.7213, AUC=0.5030\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.7317, F1=0.8254, AUC=0.6515\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6341, F1=0.7619, AUC=0.5364\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8254, AUC=0.5636\n",
      "\n",
      "==================================================\n",
      "Model: cnn_gru\n",
      "\n",
      "Running 10-fold CV for model: cnn_gru\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6429, F1=0.7541, AUC=0.6683\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.5238, F1=0.6429, AUC=0.5529\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.5714, F1=0.6250, AUC=0.5337\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5952, F1=0.6909, AUC=0.5694\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6098, F1=0.7143, AUC=0.6625\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.6585, F1=0.7500, AUC=0.6000\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.5610, F1=0.6400, AUC=0.4550\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6829, F1=0.7797, AUC=0.6175\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.5122, F1=0.6552, AUC=0.4350\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.5610, F1=0.6786, AUC=0.5475\n",
      "\n",
      "Running 10-fold CV for model: cnn_gru\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.5952, F1=0.7385, AUC=0.5222\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.7143, F1=0.8000, AUC=0.5750\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7381, F1=0.8406, AUC=0.6250\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.6190, F1=0.7500, AUC=0.5611\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6585, F1=0.7879, AUC=0.4697\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7073, F1=0.8182, AUC=0.6121\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.7073, F1=0.8125, AUC=0.5212\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6829, F1=0.8000, AUC=0.5182\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6829, F1=0.8000, AUC=0.3242\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8358, AUC=0.5061\n",
      "\n",
      "==================================================\n",
      "Model: mlp\n",
      "\n",
      "Running 10-fold CV for model: mlp\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.5952, F1=0.6909, AUC=0.5793\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.5476, F1=0.6415, AUC=0.5505\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.5238, F1=0.5455, AUC=0.5361\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5000, F1=0.5882, AUC=0.4094\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6098, F1=0.6800, AUC=0.5725\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7561, F1=0.8000, AUC=0.7550\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.4634, F1=0.5600, AUC=0.3450\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.5610, F1=0.6400, AUC=0.5150\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.5366, F1=0.6780, AUC=0.4475\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7073, F1=0.7600, AUC=0.7075\n",
      "\n",
      "Running 10-fold CV for model: mlp\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6905, F1=0.7869, AUC=0.5472\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6905, F1=0.7937, AUC=0.5611\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7381, F1=0.8254, AUC=0.6611\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5714, F1=0.6786, AUC=0.5500\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6829, F1=0.7937, AUC=0.5697\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7561, F1=0.8387, AUC=0.6606\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.6829, F1=0.7937, AUC=0.6212\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6829, F1=0.7869, AUC=0.6455\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6341, F1=0.7541, AUC=0.5030\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7073, F1=0.7931, AUC=0.6939\n",
      "\n",
      "==================================================\n",
      "Model: capsnet\n",
      "\n",
      "Running 10-fold CV for model: capsnet\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6190, F1=0.7647, AUC=0.5986\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6190, F1=0.7647, AUC=0.4255\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.6190, F1=0.7647, AUC=0.4207\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5952, F1=0.7463, AUC=0.4094\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6829, F1=0.7719, AUC=0.6125\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.6098, F1=0.7576, AUC=0.6250\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.6585, F1=0.7667, AUC=0.5725\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6341, F1=0.7619, AUC=0.4925\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6098, F1=0.7576, AUC=0.3750\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.6585, F1=0.7742, AUC=0.6450\n",
      "\n",
      "Running 10-fold CV for model: capsnet\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.7143, F1=0.8333, AUC=0.5944\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.7143, F1=0.8333, AUC=0.6056\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7143, F1=0.8333, AUC=0.6528\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.7381, F1=0.8451, AUC=0.6639\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.7317, F1=0.8451, AUC=0.5788\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7317, F1=0.8451, AUC=0.6152\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.7317, F1=0.8451, AUC=0.4576\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.7317, F1=0.8451, AUC=0.6212\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.7317, F1=0.8451, AUC=0.4788\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8451, AUC=0.3939\n",
      "\n",
      "==================================================\n",
      "Model: svm\n",
      "\n",
      "Running 10-fold CV for model: svm\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.7143, F1=0.8125, AUC=0.6274\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6190, F1=0.7647, AUC=0.4688\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.5952, F1=0.6383, AUC=0.5505\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5714, F1=0.7188, AUC=0.5012\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6341, F1=0.7619, AUC=0.5200\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7317, F1=0.7755, AUC=0.7550\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.6098, F1=0.7576, AUC=0.4425\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6341, F1=0.7692, AUC=0.5575\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6341, F1=0.7692, AUC=0.4175\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8000, AUC=0.6575\n",
      "\n",
      "Running 10-fold CV for model: svm\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.7381, F1=0.8451, AUC=0.4583\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6905, F1=0.8169, AUC=0.5806\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7381, F1=0.8451, AUC=0.7194\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.7381, F1=0.8451, AUC=0.5806\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.7561, F1=0.8571, AUC=0.5515\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7805, F1=0.8696, AUC=0.6364\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.7317, F1=0.8451, AUC=0.6121\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.7317, F1=0.8451, AUC=0.6152\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.7317, F1=0.8451, AUC=0.5303\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8451, AUC=0.6121\n",
      "\n",
      "==================================================\n",
      "Model: cascade\n",
      "\n",
      "Running 10-fold CV for model: cascade\n",
      "\n",
      "Fold 1\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5697\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5012\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5325\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 1 -> Acc=0.6429, F1=0.7692, AUC=0.5817\n",
      "\n",
      "Fold 2\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5541\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5925\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5325\n",
      "Fold 2 -> Acc=0.6429, F1=0.7692, AUC=0.4375\n",
      "\n",
      "Fold 3\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5589\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4603\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5385\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 3 -> Acc=0.6190, F1=0.7647, AUC=0.5433\n",
      "\n",
      "Fold 4\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6502\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5445\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5373\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 4 -> Acc=0.5952, F1=0.7463, AUC=0.4024\n",
      "\n",
      "Fold 5\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6987\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6900\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.7237\n",
      "Fold 5 -> Acc=0.6341, F1=0.7619, AUC=0.5837\n",
      "\n",
      "Fold 6\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4838\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4925\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4813\n",
      "Fold 6 -> Acc=0.7561, F1=0.8077, AUC=0.7100\n",
      "\n",
      "Fold 7\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4637\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4400\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5588\n",
      "Fold 7 -> Acc=0.6585, F1=0.7812, AUC=0.5112\n",
      "\n",
      "Fold 8\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5188\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4563\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4325\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 8 -> Acc=0.6341, F1=0.7619, AUC=0.5275\n",
      "\n",
      "Fold 9\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6350\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6550\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5987\n",
      "Fold 9 -> Acc=0.6341, F1=0.7692, AUC=0.5212\n",
      "\n",
      "Fold 10\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4838\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.3962\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4188\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 10 -> Acc=0.6098, F1=0.7576, AUC=0.5312\n",
      "\n",
      "Running 10-fold CV for model: cascade\n",
      "\n",
      "Fold 1\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6528\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6431\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6167\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 1 -> Acc=0.7143, F1=0.8333, AUC=0.5097\n",
      "\n",
      "Fold 2\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5222\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5250\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4417\n",
      "Fold 2 -> Acc=0.7381, F1=0.8451, AUC=0.5833\n",
      "\n",
      "Fold 3\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4986\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5278\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4694\n",
      "Fold 3 -> Acc=0.7857, F1=0.8696, AUC=0.6014\n",
      "\n",
      "Fold 4\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5056\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4347\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4944\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 4 -> Acc=0.7143, F1=0.8333, AUC=0.4861\n",
      "\n",
      "Fold 5\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5348\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4894\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4576\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 5 -> Acc=0.7317, F1=0.8451, AUC=0.6439\n",
      "\n",
      "Fold 6\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4773\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5121\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5106\n",
      "Fold 6 -> Acc=0.7317, F1=0.8451, AUC=0.7000\n",
      "\n",
      "Fold 7\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6288\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6197\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6773\n",
      "Fold 7 -> Acc=0.7317, F1=0.8451, AUC=0.4879\n",
      "\n",
      "Fold 8\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.7136\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6939\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6318\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 8 -> Acc=0.8049, F1=0.8750, AUC=0.6045\n",
      "\n",
      "Fold 9\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5985\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5500\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6303\n",
      "Fold 9 -> Acc=0.7317, F1=0.8451, AUC=0.5000\n",
      "\n",
      "Fold 10\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4379\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.3924\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4273\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 10 -> Acc=0.7317, F1=0.8451, AUC=0.6030\n",
      "\n",
      "Saved results: benchmark_results\\results_table.csv\n",
      "        model  valence_acc_mean  valence_acc_std  valence_f1_mean  \\\n",
      "0  cnn_bilstm          0.618583         0.062487         0.700514   \n",
      "1    cnn_only          0.604239         0.087337         0.679079   \n",
      "2     cnn_gru          0.591870         0.053827         0.693056   \n",
      "3         mlp          0.580081         0.086477         0.658407   \n",
      "4     capsnet          0.630604         0.026137         0.763023   \n",
      "5         svm          0.647561         0.054708         0.756771   \n",
      "6     cascade          0.642683         0.041505         0.768899   \n",
      "\n",
      "   valence_f1_std  valence_auc_mean  valence_auc_std  arousal_acc_mean  \\\n",
      "0        0.049238          0.565364         0.077725          0.707724   \n",
      "1        0.088511          0.576951         0.096691          0.664402   \n",
      "2        0.051513          0.564172         0.073892          0.683740   \n",
      "3        0.077631          0.541778         0.118843          0.683682   \n",
      "4        0.007531          0.517662         0.098433          0.727120   \n",
      "5        0.046052          0.549781         0.099311          0.736818   \n",
      "6        0.015536          0.534985         0.079485          0.741580   \n",
      "\n",
      "   arousal_acc_std  arousal_f1_mean  arousal_f1_std  arousal_auc_mean  \\\n",
      "0         0.039868         0.819966        0.025633          0.563207   \n",
      "1         0.066146         0.772734        0.050509          0.551515   \n",
      "2         0.044670         0.798342        0.031342          0.523485   \n",
      "3         0.048900         0.784460        0.041344          0.601338   \n",
      "4         0.008607         0.841549        0.005379          0.566212   \n",
      "5         0.021297         0.845910        0.012378          0.589646   \n",
      "6         0.028185         0.848165        0.012944          0.571995   \n",
      "\n",
      "   arousal_auc_std  \n",
      "0         0.099058  \n",
      "1         0.062093  \n",
      "2         0.080722  \n",
      "3         0.059875  \n",
      "4         0.086070  \n",
      "5         0.065631  \n",
      "6         0.069346  \n",
      "Saved benchmark_results\\valence_acc_plot.png\n",
      "Saved benchmark_results\\arousal_acc_plot.png\n",
      "Done. results saved to: benchmark_results\\results_table.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Combined benchmark script (fixed MLP pipeline).\n",
    "Requires: numpy, pandas, matplotlib, sklearn, torch\n",
    "Make sure X, y_val, y_aro are defined before running.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------- DEVICE ----------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# ---------- Shared dataset + augmentations ----------\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.asarray(X, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).float()\n",
    "\n",
    "class EEGDatasetMLP(Dataset):\n",
    "    def __init__(self, X_flat, y):\n",
    "        self.X = np.asarray(X_flat, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).float()\n",
    "\n",
    "def spec_augment(X, time_mask_width=4, n_time_masks=2, freq_mask_width=6, n_freq_masks=2):\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "    for i in range(N):\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T:\n",
    "                continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F:\n",
    "                continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "    return X_aug\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# ---------- Models ----------\n",
    "# CNN + BiLSTM\n",
    "class EEG_CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, n_channels=14, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.25),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=2,\n",
    "                            batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(2*hidden_size),\n",
    "            nn.Linear(2*hidden_size, 64), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.mean(dim=2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.fc(h_cat).squeeze(-1)\n",
    "\n",
    "# CNN-only\n",
    "class EEG_CNN_only(nn.Module):\n",
    "    def __init__(self, n_channels=14, hidden_feat=128):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, hidden_feat, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_feat), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1)), nn.Dropout(0.3),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_feat),\n",
    "            nn.Linear(hidden_feat, hidden_feat//2), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_feat//2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x).squeeze(-1)\n",
    "\n",
    "# CNN + GRU\n",
    "class EEG_CNN_GRU(nn.Module):\n",
    "    def __init__(self, n_channels=14, feat=128, rnn_hidden=64, rnn_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.25),\n",
    "            nn.Conv2d(128, feat, kernel_size=3, padding=1), nn.BatchNorm2d(feat), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.gru = nn.GRU(input_size=feat, hidden_size=rnn_hidden, num_layers=rnn_layers,\n",
    "                          batch_first=True, bidirectional=False, dropout=dropout if rnn_layers>1 else 0.0)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(rnn_hidden),\n",
    "            nn.Linear(rnn_hidden, rnn_hidden//2), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_hidden//2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.mean(dim=2)\n",
    "        x = x.permute(0,2,1)\n",
    "        _, h_n = self.gru(x)\n",
    "        h_last = h_n[-1]\n",
    "        return self.fc(h_last).squeeze(-1)\n",
    "\n",
    "# MLP\n",
    "class EEG_MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[1024,512,256], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers=[]\n",
    "        prev=input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.LayerNorm(h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev=h\n",
    "        layers.append(nn.Linear(prev,1))\n",
    "        self.net=nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# CapsNet (compact)\n",
    "def squash(s, dim=-1, epsilon=1e-9):\n",
    "    squared_norm = (s**2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1.0 + squared_norm)\n",
    "    v = scale * s / torch.sqrt(squared_norm + epsilon)\n",
    "    return v\n",
    "\n",
    "class PrimaryCapsules(nn.Module):\n",
    "    def __init__(self, in_channels, num_capsules=8, capsule_dim=16, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, num_capsules*capsule_dim, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.capsule_dim = capsule_dim\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        B, _, Fp, Tp = out.shape\n",
    "        out = out.view(B, self.num_capsules, self.capsule_dim, Fp*Tp)\n",
    "        out = out.permute(0,3,1,2).contiguous()\n",
    "        B,P,NC,CD = out.shape\n",
    "        out = out.view(B, P*NC, CD)\n",
    "        out = squash(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "class DigitCapsules(nn.Module):\n",
    "    def __init__(self, num_input_caps, in_dim, num_output_caps=2, out_dim=16, routing_iters=3):\n",
    "        super().__init__()\n",
    "        self.num_input_caps = num_input_caps\n",
    "        self.in_dim = in_dim\n",
    "        self.num_output_caps = num_output_caps\n",
    "        self.out_dim = out_dim\n",
    "        self.routing_iters = routing_iters\n",
    "        self.W = nn.Parameter(0.01*torch.randn(1, num_input_caps, num_output_caps, out_dim, in_dim))\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = x.unsqueeze(2).unsqueeze(-1)\n",
    "        W = self.W.repeat(B,1,1,1,1)\n",
    "        u_hat = torch.matmul(W, x).squeeze(-1)\n",
    "        b = torch.zeros(B, self.num_input_caps, self.num_output_caps, device=u_hat.device)\n",
    "        for r in range(self.routing_iters):\n",
    "            c = torch.softmax(b, dim=2).unsqueeze(-1)\n",
    "            s = (c * u_hat).sum(dim=1)\n",
    "            v = squash(s, dim=-1)\n",
    "            if r < self.routing_iters-1:\n",
    "                v_expanded = v.unsqueeze(1)\n",
    "                a = (u_hat * v_expanded).sum(dim=-1)\n",
    "                b = b + a\n",
    "        return v\n",
    "\n",
    "class EEG_CapsNet(nn.Module):\n",
    "    def __init__(self, in_channels=14, n_classes=2, routing_iters=3, primary_caps_num=8, primary_caps_dim=16, digit_caps_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,64,kernel_size=3,padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.primary = PrimaryCapsules(in_channels=128, num_capsules=primary_caps_num, capsule_dim=primary_caps_dim)\n",
    "        self.digit_caps_module_configured=False\n",
    "        self.n_classes=n_classes\n",
    "        self.digit_caps_dim=digit_caps_dim\n",
    "        self.routing_iters=routing_iters\n",
    "    def forward(self,x):\n",
    "        out = self.conv_block(x)\n",
    "        primary = self.primary(out)\n",
    "        num_primary_caps = primary.size(1)\n",
    "        in_dim = primary.size(2)\n",
    "        if not self.digit_caps_module_configured:\n",
    "            self.digitcaps = DigitCapsules(num_input_caps=num_primary_caps, in_dim=in_dim, num_output_caps=self.n_classes, out_dim=self.digit_caps_dim, routing_iters=self.routing_iters).to(x.device)\n",
    "            self.digit_caps_module_configured=True\n",
    "        digit_caps = self.digitcaps(primary)\n",
    "        lengths = torch.norm(digit_caps, dim=-1)\n",
    "        return lengths\n",
    "\n",
    "# ---------- Cascade helpers ----------\n",
    "def fit_layer_oof_preds(X, y, estimators, n_splits=5, random_state=0):\n",
    "    n_samples = X.shape[0]\n",
    "    oof_preds=[]\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for est in estimators:\n",
    "        oof = np.zeros(n_samples, dtype=np.float32)\n",
    "        for tr, te in skf.split(np.arange(n_samples), y):\n",
    "            est_clone = deepcopy(est)\n",
    "            try:\n",
    "                est_clone.random_state = random_state\n",
    "            except Exception:\n",
    "                pass\n",
    "            est_clone.fit(X[tr], y[tr])\n",
    "            if hasattr(est_clone, \"predict_proba\"):\n",
    "                p = est_clone.predict_proba(X[te])[:,1]\n",
    "            else:\n",
    "                if hasattr(est_clone, \"decision_function\"):\n",
    "                    scores = est_clone.decision_function(X[te])\n",
    "                    p = 1.0/(1.0+np.exp(-scores))\n",
    "                else:\n",
    "                    p = est_clone.predict(X[te])\n",
    "            oof[te]=p\n",
    "        oof_preds.append(oof.reshape(-1,1))\n",
    "    meta_train = np.hstack(oof_preds)\n",
    "    fitted=[]\n",
    "    for est in estimators:\n",
    "        est_clone = deepcopy(est)\n",
    "        try:\n",
    "            est_clone.random_state = random_state\n",
    "        except Exception:\n",
    "            pass\n",
    "        est_clone.fit(X,y)\n",
    "        fitted.append(est_clone)\n",
    "    return meta_train, fitted\n",
    "\n",
    "def transform_with_layer(X, fitted_estimators):\n",
    "    preds=[]\n",
    "    for est in fitted_estimators:\n",
    "        if hasattr(est, \"predict_proba\"):\n",
    "            p = est.predict_proba(X)[:,1].reshape(-1,1)\n",
    "        else:\n",
    "            if hasattr(est, \"decision_function\"):\n",
    "                scores = est.decision_function(X)\n",
    "                p = (1.0/(1.0+np.exp(-scores))).reshape(-1,1)\n",
    "            else:\n",
    "                p = est.predict(X).reshape(-1,1)\n",
    "        preds.append(p)\n",
    "    return np.hstack(preds)\n",
    "\n",
    "def train_cascade(X_train, y_train, X_val, y_val, base_estimators=None, max_layers=3, n_splits=5, random_state=0, early_stopping_rounds=2):\n",
    "    if base_estimators is None:\n",
    "        base_estimators = [RandomForestClassifier(n_estimators=100, n_jobs=-1), ExtraTreesClassifier(n_estimators=100, n_jobs=-1)]\n",
    "    layers=[]\n",
    "    Xtr = X_train.copy(); Xv = X_val.copy()\n",
    "    best_val_auc = -np.inf; no_improve=0\n",
    "    for layer_idx in range(1, max_layers+1):\n",
    "        print(f\"--- Cascade Layer {layer_idx} ---\")\n",
    "        meta_tr, fitted = fit_layer_oof_preds(Xtr, y_train, base_estimators, n_splits=n_splits, random_state=random_state+layer_idx)\n",
    "        meta_val = transform_with_layer(Xv, fitted)\n",
    "        Xtr = np.hstack([Xtr, meta_tr]); Xv = np.hstack([Xv, meta_val])\n",
    "        layers.append({'estimators': fitted})\n",
    "        val_clf = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=random_state+layer_idx)\n",
    "        val_clf.fit(Xtr, y_train)\n",
    "        try:\n",
    "            y_val_prob = val_clf.predict_proba(Xv)[:,1]\n",
    "        except:\n",
    "            if hasattr(val_clf, \"decision_function\"):\n",
    "                y_val_prob = 1.0/(1.0+np.exp(-val_clf.decision_function(Xv)))\n",
    "            else:\n",
    "                y_val_prob = val_clf.predict(Xv)\n",
    "        val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "        print(f\"Layer {layer_idx} validation AUC: {val_auc:.4f}\")\n",
    "        if val_auc > best_val_auc + 1e-4:\n",
    "            best_val_auc = val_auc; no_improve=0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stopping_rounds:\n",
    "                print(\"No improvement â€” stopping cascade.\")\n",
    "                break\n",
    "    return layers, Xtr, Xv\n",
    "\n",
    "def predict_cascade(layers, X_raw):\n",
    "    Xcur = X_raw.copy()\n",
    "    for layer in layers:\n",
    "        fitted = layer['estimators']\n",
    "        meta = transform_with_layer(Xcur, fitted)\n",
    "        Xcur = np.hstack([Xcur, meta])\n",
    "    return Xcur\n",
    "\n",
    "# ---------- Training routines ----------\n",
    "def train_nn_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=70, base_seed=42, batch_size=16):\n",
    "    # Data augmentation on shaped scaled arrays\n",
    "    X_train_sa = spec_augment(X_train_scaled)\n",
    "    X_train_noise = add_noise(X_train_scaled)\n",
    "    X_train_aug = np.concatenate([X_train_scaled, X_train_sa, X_train_noise], axis=0)\n",
    "    y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "\n",
    "    train_loader = DataLoader(EEGDataset(X_train_aug, y_train_aug), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(EEGDataset(X_val_scaled, y_val_bin), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(EEGDataset(X_test_scaled, y_test_bin), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(base_seed); random.seed(base_seed); np.random.seed(base_seed)\n",
    "    model = model.to(DEVICE)\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts)>0 else 1\n",
    "    pos = class_counts[1] if len(class_counts)>1 else 1\n",
    "    pos_weight_val = float(neg)/float(pos) if pos>0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc=0.0; best_state=None; patience=18; counter=0\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss=0.0; n_samples=0\n",
    "        for xb,yb in train_loader:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item()*xb.size(0); n_samples += xb.size(0)\n",
    "        scheduler.step()\n",
    "        # validation\n",
    "        model.eval()\n",
    "        y_true_val=[]; y_prob_val=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy()); y_prob_val.extend(probs)\n",
    "        y_true_val = np.array(y_true_val); y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc; best_state = model.state_dict(); counter=0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    y_true_test=[]; y_prob_test=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy()); y_prob_test.extend(probs)\n",
    "    y_true_test = np.array(y_true_test); y_prob_test = np.array(y_prob_test)\n",
    "\n",
    "    # threshold search\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "def train_capsnet_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=70, base_seed=42, batch_size=16):\n",
    "    X_train_sa = spec_augment(X_train_scaled)\n",
    "    X_train_noise = add_noise(X_train_scaled)\n",
    "    X_train_aug = np.concatenate([X_train_scaled, X_train_sa, X_train_noise], axis=0)\n",
    "    y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "\n",
    "    train_loader = DataLoader(EEGDataset(X_train_aug, y_train_aug), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(EEGDataset(X_val_scaled, y_val_bin), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(EEGDataset(X_test_scaled, y_test_bin), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(base_seed); random.seed(base_seed); np.random.seed(base_seed)\n",
    "    model = model.to(DEVICE)\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts)>0 else 1\n",
    "    pos = class_counts[1] if len(class_counts)>1 else 1\n",
    "    pos_weight_val = float(neg)/float(pos) if pos>0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc=0.0; best_state=None; patience=18; counter=0\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb,yb in train_loader:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            lengths = model(xb)            # [B, n_classes]\n",
    "            logits_pos = lengths[:,1].unsqueeze(1)\n",
    "            loss = criterion(logits_pos, yb.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "        scheduler.step()\n",
    "        # val\n",
    "        model.eval()\n",
    "        y_true_val=[]; y_prob_val=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                lengths = model(xb)\n",
    "                probs_pos = torch.sigmoid(lengths[:,1]).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy()); y_prob_val.extend(probs_pos)\n",
    "        y_true_val = np.array(y_true_val); y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc; best_state = model.state_dict(); counter=0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    y_true_test=[]; y_prob_test=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            lengths = model(xb)\n",
    "            probs_pos = torch.sigmoid(lengths[:,1]).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy()); y_prob_test.extend(probs_pos)\n",
    "    y_true_test = np.array(y_true_test); y_prob_test = np.array(y_prob_test)\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "def train_mlp_one_fold(X_train_scaled_flat, y_train_bin, X_val_scaled_flat, y_val_bin, X_test_scaled_flat, y_test_bin, input_dim, epochs=70, base_seed=42, batch_size=32):\n",
    "    # X_*_flat are [N, D]\n",
    "    # Augment: reshape flat -> shaped for spec_augment\n",
    "    # We need to know original shaped dims: infer by dividing by channels*F*T => user must keep original shape available. To avoid complications,\n",
    "    # we assume X_train_scaled_flat, X_val_scaled_flat, X_test_scaled_flat come from reshaping shaped arrays and we also receive shaped array dims separately.\n",
    "    # Simpler approach: we've been using shaped arrays earlier; so here the caller will pass flattened arrays along with shaped arrays (we will call this function accordingly).\n",
    "    # But to keep signature simple, we expect caller already prepared augmented flat arrays.\n",
    "    train_loader = DataLoader(EEGDatasetMLP(X_train_scaled_flat, y_train_bin), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(EEGDatasetMLP(X_val_scaled_flat, y_val_bin), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(EEGDatasetMLP(X_test_scaled_flat, y_test_bin), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(base_seed); random.seed(base_seed); np.random.seed(base_seed)\n",
    "    model = EEG_MLP(input_dim=input_dim, hidden_dims=[1024,512,256], dropout=0.3).to(DEVICE)\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts)>0 else 1\n",
    "    pos = class_counts[1] if len(class_counts)>1 else 1\n",
    "    pos_weight_val = float(neg)/float(pos) if pos>0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc=0.0; best_state=None; patience=18; counter=0\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb,yb in train_loader:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        y_true_val=[]; y_prob_val=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy()); y_prob_val.extend(probs)\n",
    "        y_true_val=np.array(y_true_val); y_prob_val=np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc; best_state = model.state_dict(); counter=0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    y_true_test=[]; y_prob_test=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy()); y_prob_test.extend(probs)\n",
    "    y_true_test=np.array(y_true_test); y_prob_test=np.array(y_prob_test)\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# SVM helper\n",
    "def train_svm_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, base_seed=42):\n",
    "    classes = np.unique(y_train_aug)\n",
    "    cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_aug)\n",
    "    class_weight_dict = {int(c): float(w) for c,w in zip(classes,cw)}\n",
    "    clf = SVC(kernel='rbf', C=1.0, gamma='scale', class_weight=class_weight_dict, probability=True, random_state=base_seed)\n",
    "    clf.fit(X_train_aug_flat, y_train_aug)\n",
    "    y_prob_test = clf.predict_proba(X_test_flat)[:,1]\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_test_bin, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_test_bin, y_pred_best)\n",
    "    test_f1 = f1_score(y_test_bin, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_test_bin, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# Cascade helper wrapper\n",
    "def train_cascade_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, base_seed=42, max_layers=3):\n",
    "    base_estimators = [RandomForestClassifier(n_estimators=100, n_jobs=-1), ExtraTreesClassifier(n_estimators=100, n_jobs=-1)]\n",
    "    layers, Xtr_trans, Xval_trans = train_cascade(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, base_estimators=base_estimators, max_layers=max_layers, n_splits=5, random_state=base_seed, early_stopping_rounds=2)\n",
    "    final_clf = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=base_seed)\n",
    "    final_clf.fit(Xtr_trans, y_train_aug)\n",
    "    X_test_trans = predict_cascade(layers, X_test_flat)\n",
    "    if hasattr(final_clf, \"predict_proba\"):\n",
    "        y_prob_test = final_clf.predict_proba(X_test_trans)[:,1]\n",
    "    else:\n",
    "        if hasattr(final_clf, \"decision_function\"):\n",
    "            y_prob_test = 1.0/(1.0+np.exp(-final_clf.decision_function(X_test_trans)))\n",
    "        else:\n",
    "            y_prob_test = final_clf.predict(X_test_trans)\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_test_bin, yp)\n",
    "        if acc_thr > best_acc:\n",
    "            best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_test_bin, y_pred_best)\n",
    "    test_f1 = f1_score(y_test_bin, y_pred_best)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_test_bin, y_prob_test)\n",
    "    except:\n",
    "        test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# ---------- CV runner ----------\n",
    "def run_10fold_for_model(X, y_cont, model_type=\"cnn_bilstm\", epochs=70, base_seed=42, batch_size=16, max_layers=3):\n",
    "    print(f\"\\nRunning 10-fold CV for model: {model_type}\")\n",
    "    X = np.nan_to_num(X)\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=base_seed)\n",
    "\n",
    "    fold_accs=[]; fold_f1s=[]; fold_aucs=[]\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(skf.split(np.arange(len(y_cont)), y_bin_global), start=1):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "        tv_train_idx, tv_val_idx = train_test_split(trainval_idx, test_size=val_frac, random_state=base_seed+fold, shuffle=True, stratify=y_trainval_bin)\n",
    "\n",
    "        X_train = X[tv_train_idx]; X_val = X[tv_val_idx]; X_test = X[test_idx]\n",
    "        y_train_cont = y_cont[tv_train_idx]; y_val_cont = y_cont[tv_val_idx]; y_test_cont = y_cont[test_idx]\n",
    "\n",
    "        thr = np.median(y_train_cont)\n",
    "        y_train_bin = (y_train_cont >= thr).astype(int)\n",
    "        y_val_bin = (y_val_cont >= thr).astype(int)\n",
    "        y_test_bin = (y_test_cont >= thr).astype(int)\n",
    "\n",
    "        # standardize using TRAIN only\n",
    "        scaler = StandardScaler()\n",
    "        X_train_flat = X_train.reshape(len(tv_train_idx), -1)\n",
    "        scaler.fit(X_train_flat)\n",
    "        def transform_block(X_block):\n",
    "            flat = X_block.reshape(X_block.shape[0], -1)\n",
    "            flat_scaled = scaler.transform(flat)\n",
    "            return flat_scaled.reshape(X_block.shape)\n",
    "        X_train_scaled = transform_block(X_train)\n",
    "        X_val_scaled = transform_block(X_val)\n",
    "        X_test_scaled = transform_block(X_test)\n",
    "\n",
    "        # Branch by model type\n",
    "        if model_type == \"cnn_bilstm\":\n",
    "            model = EEG_CNN_BiLSTM(n_channels=X.shape[1], hidden_size=64)\n",
    "            acc,f1,auc = train_nn_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=epochs, base_seed=base_seed+fold, batch_size=batch_size)\n",
    "        elif model_type == \"cnn_only\":\n",
    "            model = EEG_CNN_only(n_channels=X.shape[1], hidden_feat=128)\n",
    "            acc,f1,auc = train_nn_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=epochs, base_seed=base_seed+fold, batch_size=batch_size)\n",
    "        elif model_type == \"cnn_gru\":\n",
    "            model = EEG_CNN_GRU(n_channels=X.shape[1], feat=128, rnn_hidden=64, rnn_layers=2)\n",
    "            acc,f1,auc = train_nn_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=epochs, base_seed=base_seed+fold, batch_size=batch_size)\n",
    "        elif model_type == \"mlp\":\n",
    "            # Prepare flattened arrays and augmented flattened training\n",
    "            X_train_flat = X_train_scaled.reshape(len(tv_train_idx), -1)\n",
    "            X_val_flat = X_val_scaled.reshape(len(tv_val_idx), -1)\n",
    "            X_test_flat = X_test_scaled.reshape(len(test_idx), -1)\n",
    "            # Augment shaped scaled -> flatten\n",
    "            X_train_shaped_scaled = X_train_scaled\n",
    "            X_train_sa = spec_augment(X_train_shaped_scaled)\n",
    "            X_train_noise = add_noise(X_train_shaped_scaled)\n",
    "            X_train_sa_flat = X_train_sa.reshape(X_train_sa.shape[0], -1)\n",
    "            X_train_noise_flat = X_train_noise.reshape(X_train_noise.shape[0], -1)\n",
    "            X_train_aug_flat = np.concatenate([X_train_flat, X_train_sa_flat, X_train_noise_flat], axis=0)\n",
    "            y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "            input_dim = X_train_aug_flat.shape[1]\n",
    "            acc,f1,auc = train_mlp_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, input_dim=input_dim, epochs=epochs, base_seed=base_seed+fold, batch_size=32)\n",
    "        elif model_type == \"capsnet\":\n",
    "            model = EEG_CapsNet(in_channels=X.shape[1], n_classes=2, routing_iters=3, primary_caps_num=8, primary_caps_dim=16, digit_caps_dim=16)\n",
    "            acc,f1,auc = train_capsnet_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=epochs, base_seed=base_seed+fold, batch_size=batch_size)\n",
    "        elif model_type == \"svm\":\n",
    "            X_train_flat = X_train_scaled.reshape(len(tv_train_idx), -1)\n",
    "            X_val_flat = X_val_scaled.reshape(len(tv_val_idx), -1)\n",
    "            X_test_flat = X_test_scaled.reshape(len(test_idx), -1)\n",
    "            X_train_sa = spec_augment(X_train_scaled)\n",
    "            X_train_noise = add_noise(X_train_scaled)\n",
    "            X_train_sa_flat = X_train_sa.reshape(X_train_sa.shape[0], -1)\n",
    "            X_train_noise_flat = X_train_noise.reshape(X_train_noise.shape[0], -1)\n",
    "            X_train_aug_flat = np.concatenate([X_train_flat, X_train_sa_flat, X_train_noise_flat], axis=0)\n",
    "            y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "            acc,f1,auc = train_svm_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, base_seed=base_seed+fold)\n",
    "        elif model_type == \"cascade\":\n",
    "            X_train_flat = X_train_scaled.reshape(len(tv_train_idx), -1)\n",
    "            X_val_flat = X_val_scaled.reshape(len(tv_val_idx), -1)\n",
    "            X_test_flat = X_test_scaled.reshape(len(test_idx), -1)\n",
    "            X_train_sa = spec_augment(X_train_scaled)\n",
    "            X_train_noise = add_noise(X_train_scaled)\n",
    "            X_train_sa_flat = X_train_sa.reshape(X_train_sa.shape[0], -1)\n",
    "            X_train_noise_flat = X_train_noise.reshape(X_train_noise.shape[0], -1)\n",
    "            X_train_aug_flat = np.concatenate([X_train_flat, X_train_sa_flat, X_train_noise_flat], axis=0)\n",
    "            y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "            acc,f1,auc = train_cascade_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, base_seed=base_seed+fold, max_layers=max_layers)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type\")\n",
    "\n",
    "        fold_accs.append(acc); fold_f1s.append(f1); fold_aucs.append(auc)\n",
    "        print(f\"Fold {fold} -> Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "    return np.array(fold_accs), np.array(fold_f1s), np.array(fold_aucs)\n",
    "\n",
    "# ---------- Master benchmark ----------\n",
    "def benchmark_all_models(X, y_val, y_aro, models_to_run=None, epochs=40, base_seed=42, outdir=\"benchmark_results\"):\n",
    "    if models_to_run is None:\n",
    "        models_to_run = [\"cnn_bilstm\",\"cnn_only\",\"cnn_gru\",\"mlp\",\"capsnet\",\"svm\",\"cascade\"]\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    results=[]\n",
    "    per_model_fold_accs={}\n",
    "    for model_name in models_to_run:\n",
    "        print(\"\\n\"+\"=\"*50+\"\\nModel:\", model_name)\n",
    "        val_accs, val_f1s, val_aucs = run_10fold_for_model(X, y_val, model_type=model_name, epochs=epochs, base_seed=base_seed, batch_size=16)\n",
    "        aro_accs, aro_f1s, aro_aucs = run_10fold_for_model(X, y_aro, model_type=model_name, epochs=epochs, base_seed=base_seed+100, batch_size=16)\n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"valence_acc_mean\": val_accs.mean(), \"valence_acc_std\": val_accs.std(),\n",
    "            \"valence_f1_mean\": val_f1s.mean(), \"valence_f1_std\": val_f1s.std(),\n",
    "            \"valence_auc_mean\": np.nanmean(val_aucs), \"valence_auc_std\": np.nanstd(val_aucs),\n",
    "            \"arousal_acc_mean\": aro_accs.mean(), \"arousal_acc_std\": aro_accs.std(),\n",
    "            \"arousal_f1_mean\": aro_f1s.mean(), \"arousal_f1_std\": aro_f1s.std(),\n",
    "            \"arousal_auc_mean\": np.nanmean(aro_aucs), \"arousal_auc_std\": np.nanstd(aro_aucs),\n",
    "        })\n",
    "        per_model_fold_accs[model_name] = {\"valence\": val_accs, \"arousal\": aro_accs}\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(outdir, \"results_table.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"\\nSaved results:\", csv_path)\n",
    "    print(df)\n",
    "\n",
    "    # plots\n",
    "    models = list(per_model_fold_accs.keys()); folds=np.arange(1,11)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for m in models:\n",
    "        plt.plot(folds, per_model_fold_accs[m][\"valence\"]*100, marker='o', label=m)\n",
    "    plt.xticks(folds); plt.xlabel(\"Fold\"); plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Valence per-fold accuracy\"); plt.legend(fontsize='small'); plt.grid(True); plt.tight_layout()\n",
    "    vpng = os.path.join(outdir, \"valence_acc_plot.png\"); plt.savefig(vpng); plt.close()\n",
    "    print(\"Saved\", vpng)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for m in models:\n",
    "        plt.plot(folds, per_model_fold_accs[m][\"arousal\"]*100, marker='o', label=m)\n",
    "    plt.xticks(folds); plt.xlabel(\"Fold\"); plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Arousal per-fold accuracy\"); plt.legend(fontsize='small'); plt.grid(True); plt.tight_layout()\n",
    "    apng = os.path.join(outdir, \"arousal_acc_plot.png\"); plt.savefig(apng); plt.close()\n",
    "    print(\"Saved\", apng)\n",
    "\n",
    "    return df, per_model_fold_accs, csv_path, vpng, apng\n",
    "\n",
    "# ---------- Run (user must have X,y_val,y_aro loaded) ----------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        X; y_val; y_aro\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"Please prepare variables X, y_val, y_aro before running this script.\")\n",
    "\n",
    "    df, per_fold, csv_path, vplot, aplot = benchmark_all_models(X, y_val, y_aro, models_to_run=[\"cnn_bilstm\",\"cnn_only\",\"cnn_gru\",\"mlp\",\"capsnet\",\"svm\",\"cascade\"], epochs=20, base_seed=42)\n",
    "    print(\"Done. results saved to:\", csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82bc0519-0444-4d1c-8084-4d34ebf6b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: C:\\Users\\akash\\benchmark_results\\results_table.csv\n",
      "Valence plot: C:\\Users\\akash\\benchmark_results\\valence_acc_plot.png\n",
      "Arousal plot: C:\\Users\\akash\\benchmark_results\\arousal_acc_plot.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "outdir = \"benchmark_results\"\n",
    "print(\"CSV:\", os.path.abspath(os.path.join(outdir, \"results_table.csv\")))\n",
    "print(\"Valence plot:\", os.path.abspath(os.path.join(outdir, \"valence_acc_plot.png\")))\n",
    "print(\"Arousal plot:\", os.path.abspath(os.path.join(outdir, \"arousal_acc_plot.png\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9885cd0f-f8f2-4b7d-924b-d41a634591e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread(\"benchmark_results/valence_acc_plot.png\")\n",
    "plt.imshow(img); plt.axis('off'); plt.show()\n",
    "\n",
    "img = mpimg.imread(\"benchmark_results/arousal_acc_plot.png\")\n",
    "plt.imshow(img); plt.axis('off'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3acdd64-97ee-457e-af0d-f7b4395e36cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "==================================================\n",
      "Running model: SVM (svm)\n",
      "\n",
      "Running 10-fold CV for model: svm\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6905, F1=0.7937, AUC=0.6130\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6190, F1=0.7500, AUC=0.4952\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.6667, F1=0.7742, AUC=0.5505\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5476, F1=0.7077, AUC=0.4447\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6098, F1=0.7419, AUC=0.5950\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7317, F1=0.8070, AUC=0.7675\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.6098, F1=0.7576, AUC=0.4025\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6341, F1=0.7692, AUC=0.5850\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6098, F1=0.7576, AUC=0.4175\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7561, F1=0.8214, AUC=0.6900\n",
      "\n",
      "Running 10-fold CV for model: svm\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.7143, F1=0.8333, AUC=0.4861\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6905, F1=0.8169, AUC=0.5889\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7619, F1=0.8387, AUC=0.8056\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.7381, F1=0.8451, AUC=0.5833\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.7561, F1=0.8571, AUC=0.6000\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7805, F1=0.8657, AUC=0.5970\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.7317, F1=0.8451, AUC=0.6303\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.7317, F1=0.8451, AUC=0.6015\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.7317, F1=0.8451, AUC=0.5212\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8451, AUC=0.6242\n",
      "\n",
      "==================================================\n",
      "Running model: CNN (cnn_only)\n",
      "\n",
      "Running 10-fold CV for model: cnn_only\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6667, F1=0.7407, AUC=0.6562\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6429, F1=0.7059, AUC=0.6274\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.4524, F1=0.5106, AUC=0.4014\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5238, F1=0.6000, AUC=0.5447\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.7317, F1=0.7755, AUC=0.6800\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7317, F1=0.8070, AUC=0.7100\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.5122, F1=0.5652, AUC=0.5025\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6098, F1=0.7037, AUC=0.5375\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.5610, F1=0.6786, AUC=0.4650\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.6341, F1=0.7170, AUC=0.6375\n",
      "\n",
      "Running 10-fold CV for model: cnn_only\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6667, F1=0.7742, AUC=0.5000\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.5714, F1=0.6897, AUC=0.4361\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7381, F1=0.8358, AUC=0.5556\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.6190, F1=0.7241, AUC=0.5417\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6098, F1=0.7419, AUC=0.5818\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7561, F1=0.8276, AUC=0.6455\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.5854, F1=0.7213, AUC=0.5030\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.7317, F1=0.8254, AUC=0.6515\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6341, F1=0.7619, AUC=0.5364\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8254, AUC=0.5636\n",
      "\n",
      "==================================================\n",
      "Running model: CNN-BiLSTM (cnn_bilstm)\n",
      "\n",
      "Running 10-fold CV for model: cnn_bilstm\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6190, F1=0.7333, AUC=0.6202\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.5714, F1=0.6667, AUC=0.5144\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.6190, F1=0.6800, AUC=0.5745\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5476, F1=0.6545, AUC=0.4941\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6341, F1=0.6939, AUC=0.5900\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7805, F1=0.8085, AUC=0.7625\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.5610, F1=0.6250, AUC=0.4975\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6341, F1=0.7541, AUC=0.5400\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.5854, F1=0.7018, AUC=0.4900\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.6098, F1=0.7037, AUC=0.5800\n",
      "\n",
      "Running 10-fold CV for model: cnn_bilstm\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6905, F1=0.8116, AUC=0.5528\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.7143, F1=0.8235, AUC=0.5694\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7143, F1=0.8286, AUC=0.5667\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.7143, F1=0.8235, AUC=0.5250\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.7805, F1=0.8696, AUC=0.6182\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7073, F1=0.8235, AUC=0.5394\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.6098, F1=0.7576, AUC=0.3182\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.7073, F1=0.8182, AUC=0.6394\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.7073, F1=0.8182, AUC=0.5758\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8254, AUC=0.7273\n",
      "\n",
      "==================================================\n",
      "Running model: CNN-RNN (cnn_gru)\n",
      "\n",
      "Running 10-fold CV for model: cnn_gru\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6429, F1=0.7541, AUC=0.6683\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.5238, F1=0.6429, AUC=0.5529\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.5714, F1=0.6250, AUC=0.5337\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5952, F1=0.6909, AUC=0.5694\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6098, F1=0.7143, AUC=0.6625\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.6585, F1=0.7500, AUC=0.6000\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.5610, F1=0.6400, AUC=0.4550\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6829, F1=0.7797, AUC=0.6175\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.5122, F1=0.6552, AUC=0.4350\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.5610, F1=0.6786, AUC=0.5475\n",
      "\n",
      "Running 10-fold CV for model: cnn_gru\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.5952, F1=0.7385, AUC=0.5222\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.7143, F1=0.8000, AUC=0.5750\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7381, F1=0.8406, AUC=0.6250\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.6190, F1=0.7500, AUC=0.5611\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6585, F1=0.7879, AUC=0.4697\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7073, F1=0.8182, AUC=0.6121\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.7073, F1=0.8125, AUC=0.5212\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6829, F1=0.8000, AUC=0.5182\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6829, F1=0.8000, AUC=0.3242\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8358, AUC=0.5061\n",
      "\n",
      "==================================================\n",
      "Running model: MLP (mlp)\n",
      "\n",
      "Running 10-fold CV for model: mlp\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.5952, F1=0.6909, AUC=0.5793\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.5476, F1=0.6415, AUC=0.5505\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.5238, F1=0.5455, AUC=0.5361\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5000, F1=0.5882, AUC=0.4094\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6098, F1=0.6800, AUC=0.5725\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7561, F1=0.8000, AUC=0.7550\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.4634, F1=0.5600, AUC=0.3450\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.5610, F1=0.6400, AUC=0.5150\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.5366, F1=0.6780, AUC=0.4475\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7073, F1=0.7600, AUC=0.7075\n",
      "\n",
      "Running 10-fold CV for model: mlp\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6905, F1=0.7869, AUC=0.5472\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6905, F1=0.7937, AUC=0.5611\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7381, F1=0.8254, AUC=0.6611\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5714, F1=0.6786, AUC=0.5500\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6829, F1=0.7937, AUC=0.5697\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7561, F1=0.8387, AUC=0.6606\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.6829, F1=0.7937, AUC=0.6212\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6829, F1=0.7869, AUC=0.6455\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6341, F1=0.7541, AUC=0.5030\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7073, F1=0.7931, AUC=0.6939\n",
      "\n",
      "==================================================\n",
      "Running model: CapsNet (capsnet)\n",
      "\n",
      "Running 10-fold CV for model: capsnet\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.6190, F1=0.7647, AUC=0.5986\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.6190, F1=0.7647, AUC=0.4255\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.6190, F1=0.7647, AUC=0.4207\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.5952, F1=0.7463, AUC=0.4094\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.6829, F1=0.7719, AUC=0.6125\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.6098, F1=0.7576, AUC=0.6250\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.6585, F1=0.7667, AUC=0.5725\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.6341, F1=0.7619, AUC=0.4925\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.6098, F1=0.7576, AUC=0.3750\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.6585, F1=0.7742, AUC=0.6450\n",
      "\n",
      "Running 10-fold CV for model: capsnet\n",
      "\n",
      "Fold 1\n",
      "Fold 1 -> Acc=0.7143, F1=0.8333, AUC=0.5944\n",
      "\n",
      "Fold 2\n",
      "Fold 2 -> Acc=0.7143, F1=0.8333, AUC=0.6056\n",
      "\n",
      "Fold 3\n",
      "Fold 3 -> Acc=0.7143, F1=0.8333, AUC=0.6528\n",
      "\n",
      "Fold 4\n",
      "Fold 4 -> Acc=0.7381, F1=0.8451, AUC=0.6639\n",
      "\n",
      "Fold 5\n",
      "Fold 5 -> Acc=0.7317, F1=0.8451, AUC=0.5788\n",
      "\n",
      "Fold 6\n",
      "Fold 6 -> Acc=0.7317, F1=0.8451, AUC=0.6152\n",
      "\n",
      "Fold 7\n",
      "Fold 7 -> Acc=0.7317, F1=0.8451, AUC=0.4576\n",
      "\n",
      "Fold 8\n",
      "Fold 8 -> Acc=0.7317, F1=0.8451, AUC=0.6212\n",
      "\n",
      "Fold 9\n",
      "Fold 9 -> Acc=0.7317, F1=0.8451, AUC=0.4788\n",
      "\n",
      "Fold 10\n",
      "Fold 10 -> Acc=0.7317, F1=0.8451, AUC=0.3939\n",
      "\n",
      "==================================================\n",
      "Running model: Cascade Forest (cascade)\n",
      "\n",
      "Running 10-fold CV for model: cascade\n",
      "\n",
      "Fold 1\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4988\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5901\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6322\n",
      "Fold 1 -> Acc=0.6429, F1=0.7761, AUC=0.4531\n",
      "\n",
      "Fold 2\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5613\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5433\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5805\n",
      "Fold 2 -> Acc=0.6429, F1=0.7761, AUC=0.5048\n",
      "\n",
      "Fold 3\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5925\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5264\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5421\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 3 -> Acc=0.6190, F1=0.7647, AUC=0.5385\n",
      "\n",
      "Fold 4\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5276\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5505\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5577\n",
      "Fold 4 -> Acc=0.5952, F1=0.7463, AUC=0.4024\n",
      "\n",
      "Fold 5\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6562\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6787\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6687\n",
      "Fold 5 -> Acc=0.6341, F1=0.7619, AUC=0.6388\n",
      "\n",
      "Fold 6\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4838\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4325\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4613\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 6 -> Acc=0.7073, F1=0.7778, AUC=0.6538\n",
      "\n",
      "Fold 7\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5562\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5625\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4825\n",
      "Fold 7 -> Acc=0.6585, F1=0.7812, AUC=0.4975\n",
      "\n",
      "Fold 8\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4337\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5100\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4788\n",
      "Fold 8 -> Acc=0.6098, F1=0.7576, AUC=0.6062\n",
      "\n",
      "Fold 9\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6225\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5262\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5813\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 9 -> Acc=0.6098, F1=0.7576, AUC=0.4988\n",
      "\n",
      "Fold 10\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4688\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4675\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4113\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 10 -> Acc=0.6585, F1=0.7667, AUC=0.5975\n",
      "\n",
      "Running 10-fold CV for model: cascade\n",
      "\n",
      "Fold 1\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6972\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6347\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6514\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 1 -> Acc=0.7143, F1=0.8333, AUC=0.5389\n",
      "\n",
      "Fold 2\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5403\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5333\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4819\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 2 -> Acc=0.7619, F1=0.8571, AUC=0.5083\n",
      "\n",
      "Fold 3\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4667\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4681\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4861\n",
      "Fold 3 -> Acc=0.8095, F1=0.8788, AUC=0.6139\n",
      "\n",
      "Fold 4\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5069\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5333\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5042\n",
      "Fold 4 -> Acc=0.7143, F1=0.8333, AUC=0.4917\n",
      "\n",
      "Fold 5\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4742\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.4273\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.4470\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 5 -> Acc=0.7317, F1=0.8451, AUC=0.6318\n",
      "\n",
      "Fold 6\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.6227\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.7636\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5379\n",
      "Fold 6 -> Acc=0.7317, F1=0.8451, AUC=0.6955\n",
      "\n",
      "Fold 7\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.7424\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.7091\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.7545\n",
      "Fold 7 -> Acc=0.7317, F1=0.8451, AUC=0.5045\n",
      "\n",
      "Fold 8\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.4955\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5667\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6409\n",
      "Fold 8 -> Acc=0.7561, F1=0.8571, AUC=0.5833\n",
      "\n",
      "Fold 9\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.7439\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.6121\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.6136\n",
      "No improvement â€” stopping cascade.\n",
      "Fold 9 -> Acc=0.7317, F1=0.8451, AUC=0.4439\n",
      "\n",
      "Fold 10\n",
      "--- Cascade Layer 1 ---\n",
      "Layer 1 validation AUC: 0.5030\n",
      "--- Cascade Layer 2 ---\n",
      "Layer 2 validation AUC: 0.5000\n",
      "--- Cascade Layer 3 ---\n",
      "Layer 3 validation AUC: 0.5045\n",
      "Fold 10 -> Acc=0.7317, F1=0.8451, AUC=0.5273\n",
      "\n",
      "Saved ordered results table to: benchmark_results\\results_table_ordered.csv\n",
      "\n",
      "===== Summary table  =====\n",
      "            Model Valence Acc Â± std Arousal Acc Â± std\n",
      "0             SVM    64.75% Â± 6.03%    73.68% Â± 2.38%\n",
      "1             CNN    60.66% Â± 8.87%    66.44% Â± 6.61%\n",
      "2      CNN-BiLSTM    61.62% Â± 6.20%    70.77% Â± 3.99%\n",
      "3         CNN-RNN    59.19% Â± 5.38%    68.37% Â± 4.47%\n",
      "4             MLP    58.01% Â± 8.65%    68.37% Â± 4.89%\n",
      "5         CapsNet    63.06% Â± 2.61%    72.71% Â± 0.86%\n",
      "6  Cascade Forest    63.78% Â± 3.08%    74.15% Â± 2.69%\n",
      "Saved benchmark_results\\valence_acc_plot.png\n",
      "Saved benchmark_results\\arousal_acc_plot.png\n",
      "\n",
      "All done. Ordered results saved to: benchmark_results\\results_table_ordered.csv\n",
      "Valence plot path: benchmark_results\\valence_acc_plot.png\n",
      "Arousal plot path: benchmark_results\\arousal_acc_plot.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Combined benchmark script â€” ordered table output.\n",
    "Models (order): SVM, CNN, CNN-BiLSTM, CNN-RNN, MLP, CapsNet, Cascade Forest\n",
    "Outputs:\n",
    "  - benchmark_results/results_table_ordered.csv\n",
    "  - benchmark_results/valence_acc_plot.png\n",
    "  - benchmark_results/arousal_acc_plot.png\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------- DEVICE ----------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# ---------- Shared dataset + augmentations ----------\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.asarray(X, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).float()\n",
    "\n",
    "class EEGDatasetMLP(Dataset):\n",
    "    def __init__(self, X_flat, y):\n",
    "        self.X = np.asarray(X_flat, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).float()\n",
    "\n",
    "def spec_augment(X, time_mask_width=4, n_time_masks=2, freq_mask_width=6, n_freq_masks=2):\n",
    "    X_aug = X.copy()\n",
    "    N, C, F, T = X_aug.shape\n",
    "    for i in range(N):\n",
    "        for _ in range(n_time_masks):\n",
    "            w = np.random.randint(1, time_mask_width + 1)\n",
    "            if w >= T: continue\n",
    "            t0 = np.random.randint(0, T - w + 1)\n",
    "            X_aug[i, :, :, t0:t0 + w] = 0.0\n",
    "        for _ in range(n_freq_masks):\n",
    "            w = np.random.randint(1, freq_mask_width + 1)\n",
    "            if w >= F: continue\n",
    "            f0 = np.random.randint(0, F - w + 1)\n",
    "            X_aug[i, :, f0:f0 + w, :] = 0.0\n",
    "    return X_aug\n",
    "\n",
    "def add_noise(X, std=0.03):\n",
    "    noise = np.random.normal(0, std, X.shape).astype(np.float32)\n",
    "    return np.clip(X + noise, -3.0, 3.0)\n",
    "\n",
    "# ---------- Models ----------\n",
    "class EEG_CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, n_channels=14, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.25),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=2,\n",
    "                            batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(2*hidden_size),\n",
    "            nn.Linear(2*hidden_size, 64), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.mean(dim=2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.fc(h_cat).squeeze(-1)\n",
    "\n",
    "class EEG_CNN_only(nn.Module):\n",
    "    def __init__(self, n_channels=14, hidden_feat=128):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, hidden_feat, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_feat), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1)), nn.Dropout(0.3),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_feat),\n",
    "            nn.Linear(hidden_feat, hidden_feat//2), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_feat//2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x).squeeze(-1)\n",
    "\n",
    "class EEG_CNN_GRU(nn.Module):\n",
    "    def __init__(self, n_channels=14, feat=128, rnn_hidden=64, rnn_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.25),\n",
    "            nn.Conv2d(128, feat, kernel_size=3, padding=1), nn.BatchNorm2d(feat), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.gru = nn.GRU(input_size=feat, hidden_size=rnn_hidden, num_layers=rnn_layers,\n",
    "                          batch_first=True, bidirectional=False, dropout=dropout if rnn_layers>1 else 0.0)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(rnn_hidden),\n",
    "            nn.Linear(rnn_hidden, rnn_hidden//2), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_hidden//2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.mean(dim=2)\n",
    "        x = x.permute(0,2,1)\n",
    "        _, h_n = self.gru(x)\n",
    "        h_last = h_n[-1]\n",
    "        return self.fc(h_last).squeeze(-1)\n",
    "\n",
    "class EEG_MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[1024,512,256], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers=[]\n",
    "        prev=input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev,h))\n",
    "            layers.append(nn.LayerNorm(h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev=h\n",
    "        layers.append(nn.Linear(prev,1))\n",
    "        self.net=nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# CapsNet (compact)\n",
    "def squash(s, dim=-1, epsilon=1e-9):\n",
    "    squared_norm = (s**2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1.0 + squared_norm)\n",
    "    v = scale * s / torch.sqrt(squared_norm + epsilon)\n",
    "    return v\n",
    "\n",
    "class PrimaryCapsules(nn.Module):\n",
    "    def __init__(self, in_channels, num_capsules=8, capsule_dim=16, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, num_capsules*capsule_dim, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.capsule_dim = capsule_dim\n",
    "    def forward(self,x):\n",
    "        out = self.conv(x)\n",
    "        B,_,Fp,Tp = out.shape\n",
    "        out = out.view(B, self.num_capsules, self.capsule_dim, Fp*Tp)\n",
    "        out = out.permute(0,3,1,2).contiguous()\n",
    "        B,P,NC,CD = out.shape\n",
    "        out = out.view(B, P*NC, CD)\n",
    "        out = squash(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "class DigitCapsules(nn.Module):\n",
    "    def __init__(self, num_input_caps, in_dim, num_output_caps=2, out_dim=16, routing_iters=3):\n",
    "        super().__init__()\n",
    "        self.num_input_caps = num_input_caps\n",
    "        self.in_dim = in_dim\n",
    "        self.num_output_caps = num_output_caps\n",
    "        self.out_dim = out_dim\n",
    "        self.routing_iters = routing_iters\n",
    "        self.W = nn.Parameter(0.01*torch.randn(1, num_input_caps, num_output_caps, out_dim, in_dim))\n",
    "    def forward(self,x):\n",
    "        B = x.size(0)\n",
    "        x = x.unsqueeze(2).unsqueeze(-1)\n",
    "        W = self.W.repeat(B,1,1,1,1)\n",
    "        u_hat = torch.matmul(W, x).squeeze(-1)\n",
    "        b = torch.zeros(B, self.num_input_caps, self.num_output_caps, device=u_hat.device)\n",
    "        for r in range(self.routing_iters):\n",
    "            c = torch.softmax(b, dim=2).unsqueeze(-1)\n",
    "            s = (c * u_hat).sum(dim=1)\n",
    "            v = squash(s, dim=-1)\n",
    "            if r < self.routing_iters-1:\n",
    "                v_expanded = v.unsqueeze(1)\n",
    "                a = (u_hat * v_expanded).sum(dim=-1)\n",
    "                b = b + a\n",
    "        return v\n",
    "\n",
    "class EEG_CapsNet(nn.Module):\n",
    "    def __init__(self, in_channels=14, n_classes=2, routing_iters=3, primary_caps_num=8, primary_caps_dim=16, digit_caps_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,64,kernel_size=3,padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.primary = PrimaryCapsules(in_channels=128, num_capsules=primary_caps_num, capsule_dim=primary_caps_dim)\n",
    "        self.digit_caps_module_configured=False\n",
    "        self.n_classes=n_classes\n",
    "        self.digit_caps_dim=digit_caps_dim\n",
    "        self.routing_iters=routing_iters\n",
    "    def forward(self,x):\n",
    "        out = self.conv_block(x)\n",
    "        primary = self.primary(out)\n",
    "        num_primary_caps = primary.size(1)\n",
    "        in_dim = primary.size(2)\n",
    "        if not self.digit_caps_module_configured:\n",
    "            self.digitcaps = DigitCapsules(num_input_caps=num_primary_caps, in_dim=in_dim, num_output_caps=self.n_classes, out_dim=self.digit_caps_dim, routing_iters=self.routing_iters).to(x.device)\n",
    "            self.digit_caps_module_configured=True\n",
    "        digit_caps = self.digitcaps(primary)\n",
    "        lengths = torch.norm(digit_caps, dim=-1)\n",
    "        return lengths\n",
    "\n",
    "# ---------- Cascade helpers (same as before) ----------\n",
    "def fit_layer_oof_preds(X, y, estimators, n_splits=5, random_state=0):\n",
    "    n_samples = X.shape[0]\n",
    "    oof_preds=[]\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for est in estimators:\n",
    "        oof = np.zeros(n_samples, dtype=np.float32)\n",
    "        for tr, te in skf.split(np.arange(n_samples), y):\n",
    "            est_clone = deepcopy(est)\n",
    "            try: est_clone.random_state = random_state\n",
    "            except: pass\n",
    "            est_clone.fit(X[tr], y[tr])\n",
    "            if hasattr(est_clone, \"predict_proba\"):\n",
    "                p = est_clone.predict_proba(X[te])[:,1]\n",
    "            else:\n",
    "                if hasattr(est_clone, \"decision_function\"):\n",
    "                    scores = est_clone.decision_function(X[te]); p = 1.0/(1.0+np.exp(-scores))\n",
    "                else:\n",
    "                    p = est_clone.predict(X[te])\n",
    "            oof[te]=p\n",
    "        oof_preds.append(oof.reshape(-1,1))\n",
    "    meta_train = np.hstack(oof_preds)\n",
    "    fitted=[]\n",
    "    for est in estimators:\n",
    "        est_clone = deepcopy(est)\n",
    "        try: est_clone.random_state = random_state\n",
    "        except: pass\n",
    "        est_clone.fit(X,y)\n",
    "        fitted.append(est_clone)\n",
    "    return meta_train, fitted\n",
    "\n",
    "def transform_with_layer(X, fitted_estimators):\n",
    "    preds=[]\n",
    "    for est in fitted_estimators:\n",
    "        if hasattr(est, \"predict_proba\"):\n",
    "            p = est.predict_proba(X)[:,1].reshape(-1,1)\n",
    "        else:\n",
    "            if hasattr(est, \"decision_function\"):\n",
    "                scores = est.decision_function(X); p = (1.0/(1.0+np.exp(-scores))).reshape(-1,1)\n",
    "            else:\n",
    "                p = est.predict(X).reshape(-1,1)\n",
    "        preds.append(p)\n",
    "    return np.hstack(preds)\n",
    "\n",
    "def train_cascade(X_train, y_train, X_val, y_val, base_estimators=None, max_layers=3, n_splits=5, random_state=0, early_stopping_rounds=2):\n",
    "    if base_estimators is None:\n",
    "        base_estimators = [RandomForestClassifier(n_estimators=100, n_jobs=-1), ExtraTreesClassifier(n_estimators=100, n_jobs=-1)]\n",
    "    layers=[]; Xtr=X_train.copy(); Xv=X_val.copy()\n",
    "    best_val_auc = -np.inf; no_improve=0\n",
    "    for layer_idx in range(1, max_layers+1):\n",
    "        print(f\"--- Cascade Layer {layer_idx} ---\")\n",
    "        meta_tr, fitted = fit_layer_oof_preds(Xtr, y_train, base_estimators, n_splits=n_splits, random_state=random_state+layer_idx)\n",
    "        meta_val = transform_with_layer(Xv, fitted)\n",
    "        Xtr = np.hstack([Xtr, meta_tr]); Xv = np.hstack([Xv, meta_val])\n",
    "        layers.append({'estimators': fitted})\n",
    "        val_clf = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=random_state+layer_idx)\n",
    "        val_clf.fit(Xtr, y_train)\n",
    "        try:\n",
    "            y_val_prob = val_clf.predict_proba(Xv)[:,1]\n",
    "        except:\n",
    "            if hasattr(val_clf, \"decision_function\"):\n",
    "                y_val_prob = 1.0/(1.0+np.exp(-val_clf.decision_function(Xv)))\n",
    "            else:\n",
    "                y_val_prob = val_clf.predict(Xv)\n",
    "        val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "        print(f\"Layer {layer_idx} validation AUC: {val_auc:.4f}\")\n",
    "        if val_auc > best_val_auc + 1e-4:\n",
    "            best_val_auc = val_auc; no_improve=0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stopping_rounds:\n",
    "                print(\"No improvement â€” stopping cascade.\")\n",
    "                break\n",
    "    return layers, Xtr, Xv\n",
    "\n",
    "def predict_cascade(layers, X_raw):\n",
    "    Xcur = X_raw.copy()\n",
    "    for layer in layers:\n",
    "        fitted = layer['estimators']\n",
    "        meta = transform_with_layer(Xcur, fitted)\n",
    "        Xcur = np.hstack([Xcur, meta])\n",
    "    return Xcur\n",
    "\n",
    "# ---------- Training functions ----------\n",
    "def train_nn_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=70, base_seed=42, batch_size=16):\n",
    "    X_train_sa = spec_augment(X_train_scaled)\n",
    "    X_train_noise = add_noise(X_train_scaled)\n",
    "    X_train_aug = np.concatenate([X_train_scaled, X_train_sa, X_train_noise], axis=0)\n",
    "    y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "\n",
    "    train_loader = DataLoader(EEGDataset(X_train_aug, y_train_aug), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(EEGDataset(X_val_scaled, y_val_bin), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(EEGDataset(X_test_scaled, y_test_bin), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(base_seed); random.seed(base_seed); np.random.seed(base_seed)\n",
    "    model = model.to(DEVICE)\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts)>0 else 1\n",
    "    pos = class_counts[1] if len(class_counts)>1 else 1\n",
    "    pos_weight_val = float(neg)/float(pos) if pos>0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc=0.0; best_state=None; patience=18; counter=0\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb,yb in train_loader:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        y_true_val=[]; y_prob_val=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy()); y_prob_val.extend(probs)\n",
    "        y_true_val = np.array(y_true_val); y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc; best_state = model.state_dict(); counter=0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience: break\n",
    "\n",
    "    if best_state is not None: model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    y_true_test=[]; y_prob_test=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy()); y_prob_test.extend(probs)\n",
    "    y_true_test = np.array(y_true_test); y_prob_test = np.array(y_prob_test)\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc: best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_best)\n",
    "    try: test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except: test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "def train_capsnet_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=70, base_seed=42, batch_size=16):\n",
    "    X_train_sa = spec_augment(X_train_scaled)\n",
    "    X_train_noise = add_noise(X_train_scaled)\n",
    "    X_train_aug = np.concatenate([X_train_scaled, X_train_sa, X_train_noise], axis=0)\n",
    "    y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "\n",
    "    train_loader = DataLoader(EEGDataset(X_train_aug, y_train_aug), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(EEGDataset(X_val_scaled, y_val_bin), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(EEGDataset(X_test_scaled, y_test_bin), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(base_seed); random.seed(base_seed); np.random.seed(base_seed)\n",
    "    model = model.to(DEVICE)\n",
    "    class_counts = np.bincount(y_train_bin.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts)>0 else 1\n",
    "    pos = class_counts[1] if len(class_counts)>1 else 1\n",
    "    pos_weight_val = float(neg)/float(pos) if pos>0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc=0.0; best_state=None; patience=18; counter=0\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb,yb in train_loader:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            lengths = model(xb)\n",
    "            logits_pos = lengths[:,1].unsqueeze(1)\n",
    "            loss = criterion(logits_pos, yb.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        y_true_val=[]; y_prob_val=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                lengths = model(xb)\n",
    "                probs_pos = torch.sigmoid(lengths[:,1]).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy()); y_prob_val.extend(probs_pos)\n",
    "        y_true_val = np.array(y_true_val); y_prob_val = np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc; best_state = model.state_dict(); counter=0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience: break\n",
    "\n",
    "    if best_state is not None: model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    y_true_test=[]; y_prob_test=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            lengths = model(xb)\n",
    "            probs_pos = torch.sigmoid(lengths[:,1]).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy()); y_prob_test.extend(probs_pos)\n",
    "    y_true_test = np.array(y_true_test); y_prob_test = np.array(y_prob_test)\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc: best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_best)\n",
    "    try: test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except: test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "def train_mlp_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, input_dim, epochs=70, base_seed=42, batch_size=32):\n",
    "    train_loader = DataLoader(EEGDatasetMLP(X_train_aug_flat, y_train_aug), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(EEGDatasetMLP(X_val_flat, y_val_bin), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(EEGDatasetMLP(X_test_flat, y_test_bin), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(base_seed); random.seed(base_seed); np.random.seed(base_seed)\n",
    "    model = EEG_MLP(input_dim=input_dim, hidden_dims=[1024,512,256], dropout=0.3).to(DEVICE)\n",
    "    class_counts = np.bincount(y_train_aug.astype(int))\n",
    "    neg = class_counts[0] if len(class_counts)>0 else 1\n",
    "    pos = class_counts[1] if len(class_counts)>1 else 1\n",
    "    pos_weight_val = float(neg)/float(pos) if pos>0 else 1.0\n",
    "    pos_weight_t = torch.tensor([pos_weight_val], dtype=torch.float32, device=DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    opt = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20)\n",
    "\n",
    "    best_val_acc=0.0; best_state=None; patience=18; counter=0\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb,yb in train_loader:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        y_true_val=[]; y_prob_val=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "                y_true_val.extend(yb.numpy()); y_prob_val.extend(probs)\n",
    "        y_true_val=np.array(y_true_val); y_prob_val=np.array(y_prob_val)\n",
    "        y_pred_val = (y_prob_val >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc; best_state = model.state_dict(); counter=0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if ep > 30 and counter >= patience: break\n",
    "\n",
    "    if best_state is not None: model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    y_true_test=[]; y_prob_test=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            y_true_test.extend(yb.numpy()); y_prob_test.extend(probs)\n",
    "    y_true_test=np.array(y_true_test); y_prob_test=np.array(y_prob_test)\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_true_test, yp)\n",
    "        if acc_thr > best_acc: best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_best)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_best)\n",
    "    try: test_auc = roc_auc_score(y_true_test, y_prob_test)\n",
    "    except: test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "def train_svm_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, base_seed=42):\n",
    "    classes = np.unique(y_train_aug)\n",
    "    cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_aug)\n",
    "    class_weight_dict = {int(c): float(w) for c,w in zip(classes,cw)}\n",
    "    clf = SVC(kernel='rbf', C=1.0, gamma='scale', class_weight=class_weight_dict, probability=True, random_state=base_seed)\n",
    "    clf.fit(X_train_aug_flat, y_train_aug)\n",
    "    y_prob_test = clf.predict_proba(X_test_flat)[:,1]\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_test_bin, yp)\n",
    "        if acc_thr > best_acc: best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_test_bin, y_pred_best)\n",
    "    test_f1 = f1_score(y_test_bin, y_pred_best)\n",
    "    try: test_auc = roc_auc_score(y_test_bin, y_prob_test)\n",
    "    except: test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "def train_cascade_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, base_seed=42, max_layers=3):\n",
    "    base_estimators = [RandomForestClassifier(n_estimators=100, n_jobs=-1), ExtraTreesClassifier(n_estimators=100, n_jobs=-1)]\n",
    "    layers, Xtr_trans, Xval_trans = train_cascade(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, base_estimators=base_estimators, max_layers=max_layers, n_splits=5, random_state=base_seed, early_stopping_rounds=2)\n",
    "    final_clf = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=base_seed)\n",
    "    final_clf.fit(Xtr_trans, y_train_aug)\n",
    "    X_test_trans = predict_cascade(layers, X_test_flat)\n",
    "    if hasattr(final_clf, \"predict_proba\"):\n",
    "        y_prob_test = final_clf.predict_proba(X_test_trans)[:,1]\n",
    "    else:\n",
    "        if hasattr(final_clf, \"decision_function\"):\n",
    "            y_prob_test = 1.0/(1.0+np.exp(-final_clf.decision_function(X_test_trans)))\n",
    "        else:\n",
    "            y_prob_test = final_clf.predict(X_test_trans)\n",
    "    best_thr=0.5; best_acc=0.0\n",
    "    for thr in np.linspace(0.1,0.9,17):\n",
    "        yp = (y_prob_test >= thr).astype(int)\n",
    "        acc_thr = accuracy_score(y_test_bin, yp)\n",
    "        if acc_thr > best_acc: best_acc=acc_thr; best_thr=thr\n",
    "    y_pred_best = (y_prob_test >= best_thr).astype(int)\n",
    "    test_acc = accuracy_score(y_test_bin, y_pred_best)\n",
    "    test_f1 = f1_score(y_test_bin, y_pred_best)\n",
    "    try: test_auc = roc_auc_score(y_test_bin, y_prob_test)\n",
    "    except: test_auc = float(\"nan\")\n",
    "    return test_acc, test_f1, test_auc\n",
    "\n",
    "# ---------- CV runner ----------\n",
    "def run_10fold_for_model(X, y_cont, model_type=\"cnn_bilstm\", epochs=70, base_seed=42, batch_size=16, max_layers=3):\n",
    "    print(f\"\\nRunning 10-fold CV for model: {model_type}\")\n",
    "    X = np.nan_to_num(X)\n",
    "    y_cont = np.asarray(y_cont, dtype=float)\n",
    "    global_thr = np.median(y_cont)\n",
    "    y_bin_global = (y_cont >= global_thr).astype(int)\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=base_seed)\n",
    "\n",
    "    fold_accs=[]; fold_f1s=[]; fold_aucs=[]\n",
    "    for fold, (trainval_idx, test_idx) in enumerate(skf.split(np.arange(len(y_cont)), y_bin_global), start=1):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "        y_trainval = y_cont[trainval_idx]\n",
    "        y_trainval_bin = (y_trainval >= np.median(y_trainval)).astype(int)\n",
    "        test_size = len(test_idx)\n",
    "        val_frac = test_size / len(trainval_idx)\n",
    "        tv_train_idx, tv_val_idx = train_test_split(trainval_idx, test_size=val_frac, random_state=base_seed+fold, shuffle=True, stratify=y_trainval_bin)\n",
    "\n",
    "        X_train = X[tv_train_idx]; X_val = X[tv_val_idx]; X_test = X[test_idx]\n",
    "        y_train_cont = y_cont[tv_train_idx]; y_val_cont = y_cont[tv_val_idx]; y_test_cont = y_cont[test_idx]\n",
    "\n",
    "        thr = np.median(y_train_cont)\n",
    "        y_train_bin = (y_train_cont >= thr).astype(int)\n",
    "        y_val_bin = (y_val_cont >= thr).astype(int)\n",
    "        y_test_bin = (y_test_cont >= thr).astype(int)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_flat = X_train.reshape(len(tv_train_idx), -1)\n",
    "        scaler.fit(X_train_flat)\n",
    "        def transform_block(X_block):\n",
    "            flat = X_block.reshape(X_block.shape[0], -1)\n",
    "            flat_scaled = scaler.transform(flat)\n",
    "            return flat_scaled.reshape(X_block.shape)\n",
    "        X_train_scaled = transform_block(X_train)\n",
    "        X_val_scaled = transform_block(X_val)\n",
    "        X_test_scaled = transform_block(X_test)\n",
    "\n",
    "        if model_type == \"cnn_bilstm\":\n",
    "            model = EEG_CNN_BiLSTM(n_channels=X.shape[1], hidden_size=64)\n",
    "            acc,f1,auc = train_nn_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=epochs, base_seed=base_seed+fold, batch_size=batch_size)\n",
    "        elif model_type == \"cnn_only\":\n",
    "            model = EEG_CNN_only(n_channels=X.shape[1], hidden_feat=128)\n",
    "            acc,f1,auc = train_nn_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=epochs, base_seed=base_seed+fold, batch_size=batch_size)\n",
    "        elif model_type == \"cnn_gru\":\n",
    "            model = EEG_CNN_GRU(n_channels=X.shape[1], feat=128, rnn_hidden=64, rnn_layers=2)\n",
    "            acc,f1,auc = train_nn_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=epochs, base_seed=base_seed+fold, batch_size=batch_size)\n",
    "        elif model_type == \"mlp\":\n",
    "            X_train_flat = X_train_scaled.reshape(len(tv_train_idx), -1)\n",
    "            X_val_flat = X_val_scaled.reshape(len(tv_val_idx), -1)\n",
    "            X_test_flat = X_test_scaled.reshape(len(test_idx), -1)\n",
    "            X_train_sa = spec_augment(X_train_scaled)\n",
    "            X_train_noise = add_noise(X_train_scaled)\n",
    "            X_train_sa_flat = X_train_sa.reshape(X_train_sa.shape[0], -1)\n",
    "            X_train_noise_flat = X_train_noise.reshape(X_train_noise.shape[0], -1)\n",
    "            X_train_aug_flat = np.concatenate([X_train_flat, X_train_sa_flat, X_train_noise_flat], axis=0)\n",
    "            y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "            input_dim = X_train_aug_flat.shape[1]\n",
    "            acc,f1,auc = train_mlp_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, input_dim=input_dim, epochs=epochs, base_seed=base_seed+fold, batch_size=32)\n",
    "        elif model_type == \"capsnet\":\n",
    "            model = EEG_CapsNet(in_channels=X.shape[1], n_classes=2, routing_iters=3, primary_caps_num=8, primary_caps_dim=16, digit_caps_dim=16)\n",
    "            acc,f1,auc = train_capsnet_one_fold(model, X_train_scaled, y_train_bin, X_val_scaled, y_val_bin, X_test_scaled, y_test_bin, epochs=epochs, base_seed=base_seed+fold, batch_size=batch_size)\n",
    "        elif model_type == \"svm\":\n",
    "            X_train_flat = X_train_scaled.reshape(len(tv_train_idx), -1)\n",
    "            X_val_flat = X_val_scaled.reshape(len(tv_val_idx), -1)\n",
    "            X_test_flat = X_test_scaled.reshape(len(test_idx), -1)\n",
    "            X_train_sa = spec_augment(X_train_scaled)\n",
    "            X_train_noise = add_noise(X_train_scaled)\n",
    "            X_train_sa_flat = X_train_sa.reshape(X_train_sa.shape[0], -1)\n",
    "            X_train_noise_flat = X_train_noise.reshape(X_train_noise.shape[0], -1)\n",
    "            X_train_aug_flat = np.concatenate([X_train_flat, X_train_sa_flat, X_train_noise_flat], axis=0)\n",
    "            y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "            acc,f1,auc = train_svm_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, base_seed=base_seed+fold)\n",
    "        elif model_type == \"cascade\":\n",
    "            X_train_flat = X_train_scaled.reshape(len(tv_train_idx), -1)\n",
    "            X_val_flat = X_val_scaled.reshape(len(tv_val_idx), -1)\n",
    "            X_test_flat = X_test_scaled.reshape(len(test_idx), -1)\n",
    "            X_train_sa = spec_augment(X_train_scaled)\n",
    "            X_train_noise = add_noise(X_train_scaled)\n",
    "            X_train_sa_flat = X_train_sa.reshape(X_train_sa.shape[0], -1)\n",
    "            X_train_noise_flat = X_train_noise.reshape(X_train_noise.shape[0], -1)\n",
    "            X_train_aug_flat = np.concatenate([X_train_flat, X_train_sa_flat, X_train_noise_flat], axis=0)\n",
    "            y_train_aug = np.concatenate([y_train_bin, y_train_bin, y_train_bin], axis=0)\n",
    "            acc,f1,auc = train_cascade_one_fold(X_train_aug_flat, y_train_aug, X_val_flat, y_val_bin, X_test_flat, y_test_bin, base_seed=base_seed+fold, max_layers=max_layers)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type\")\n",
    "        fold_accs.append(acc); fold_f1s.append(f1); fold_aucs.append(auc)\n",
    "        print(f\"Fold {fold} -> Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "    return np.array(fold_accs), np.array(fold_f1s), np.array(fold_aucs)\n",
    "\n",
    "# ---------- Master benchmark (ordered output) ----------\n",
    "def benchmark_all_models_ordered(X, y_val, y_aro, epochs=20, base_seed=42, outdir=\"benchmark_results\"):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Desired order & mapping to internal model_type keys\n",
    "    ordered_models = [\n",
    "        (\"SVM\", \"svm\"),\n",
    "        (\"CNN\", \"cnn_only\"),\n",
    "        (\"CNN-BiLSTM\", \"cnn_bilstm\"),\n",
    "        (\"CNN-RNN\", \"cnn_gru\"),\n",
    "        (\"MLP\", \"mlp\"),\n",
    "        (\"CapsNet\", \"capsnet\"),\n",
    "        (\"Cascade Forest\", \"cascade\"),\n",
    "    ]\n",
    "\n",
    "    results_rows = []\n",
    "    per_model_fold_accs = {}\n",
    "\n",
    "    for display_name, key in ordered_models:\n",
    "        print(\"\\n\" + \"=\"*50 + f\"\\nRunning model: {display_name} ({key})\")\n",
    "        val_accs, val_f1s, val_aucs = run_10fold_for_model(X, y_val, model_type=key, epochs=epochs, base_seed=base_seed, batch_size=16)\n",
    "        aro_accs, aro_f1s, aro_aucs = run_10fold_for_model(X, y_aro, model_type=key, epochs=epochs, base_seed=base_seed+100, batch_size=16)\n",
    "        # Format \"mean Â± std\" in percent with 2 decimals\n",
    "        val_mean = val_accs.mean()*100; val_std = val_accs.std()*100\n",
    "        aro_mean = aro_accs.mean()*100; aro_std = aro_accs.std()*100\n",
    "        val_formatted = f\"{val_mean:.2f}% Â± {val_std:.2f}%\"\n",
    "        aro_formatted = f\"{aro_mean:.2f}% Â± {aro_std:.2f}%\"\n",
    "        results_rows.append({\n",
    "            \"Model\": display_name,\n",
    "            \"Valence Acc Â± std\": val_formatted,\n",
    "            \"Arousal Acc Â± std\": aro_formatted,\n",
    "            \"valence_acc_mean\": val_accs.mean(), \"valence_acc_std\": val_accs.std(),\n",
    "            \"arousal_acc_mean\": aro_accs.mean(), \"arousal_acc_std\": aro_accs.std(),\n",
    "        })\n",
    "        per_model_fold_accs[display_name] = {\"valence\": val_accs, \"arousal\": aro_accs}\n",
    "\n",
    "    # Build DataFrame with requested two columns first\n",
    "    df_ordered = pd.DataFrame(results_rows)[[\"Model\", \"Valence Acc Â± std\", \"Arousal Acc Â± std\",\n",
    "                                             \"valence_acc_mean\",\"valence_acc_std\",\"arousal_acc_mean\",\"arousal_acc_std\"]]\n",
    "    csv_path = os.path.join(outdir, \"results_table_ordered.csv\")\n",
    "    df_ordered.to_csv(csv_path, index=False)\n",
    "    print(\"\\nSaved ordered results table to:\", csv_path)\n",
    "    # Print just the concise requested table (first two columns)\n",
    "    print(\"\\n===== Summary table  =====\")\n",
    "    print(df_ordered[[\"Model\",\"Valence Acc Â± std\",\"Arousal Acc Â± std\"]])\n",
    "\n",
    "    # Plots (per-fold accuracy)\n",
    "    folds = np.arange(1, 11)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for name in per_model_fold_accs:\n",
    "        plt.plot(folds, per_model_fold_accs[name][\"valence\"]*100, marker='o', label=name)\n",
    "    plt.xticks(folds); plt.xlabel(\"Fold\"); plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Valence 10 Fold Cross Validation\"); plt.legend(fontsize='small'); plt.grid(True); plt.tight_layout()\n",
    "    vpng = os.path.join(outdir, \"valence_acc_plot.png\"); plt.savefig(vpng); plt.close()\n",
    "    print(\"Saved\", vpng)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for name in per_model_fold_accs:\n",
    "        plt.plot(folds, per_model_fold_accs[name][\"arousal\"]*100, marker='o', label=name)\n",
    "    plt.xticks(folds); plt.xlabel(\"Fold\"); plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Arousal 10 Fold Cross Validation\"); plt.legend(fontsize='small'); plt.grid(True); plt.tight_layout()\n",
    "    apng = os.path.join(outdir, \"arousal_acc_plot.png\"); plt.savefig(apng); plt.close()\n",
    "    print(\"Saved\", apng)\n",
    "\n",
    "    return df_ordered, per_model_fold_accs, csv_path, vpng, apng\n",
    "\n",
    "# ---------- Run (requires X, y_val, y_aro loaded) ----------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        X; y_val; y_aro\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"Please prepare variables X, y_val, y_aro before running this script.\")\n",
    "    # run benchmark (epochs default 20 for speed; increase if needed)\n",
    "    df_ordered, per_fold, csv_path, vplot, aplot = benchmark_all_models_ordered(X, y_val, y_aro, epochs=20, base_seed=42, outdir=\"benchmark_results\")\n",
    "    print(\"\\nAll done. Ordered results saved to:\", csv_path)\n",
    "    print(\"Valence plot path:\", vplot)\n",
    "    print(\"Arousal plot path:\", aplot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e242a92-76b1-41cd-98cd-d3fd58092f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
